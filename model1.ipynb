{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.current_device())  # Should return the current device index\n",
    "    print(torch.cuda.get_device_name(0))  # Should return the name of the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from tkinter import Tk, filedialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 39,  40,  52],\n",
       "        [ 39,  40,  52],\n",
       "        [ 39,  40,  54],\n",
       "        ...,\n",
       "        [188, 207, 217],\n",
       "        [188, 207, 217],\n",
       "        [189, 208, 218]],\n",
       "\n",
       "       [[ 39,  40,  52],\n",
       "        [ 39,  40,  52],\n",
       "        [ 39,  40,  54],\n",
       "        ...,\n",
       "        [188, 207, 217],\n",
       "        [188, 207, 217],\n",
       "        [189, 208, 218]],\n",
       "\n",
       "       [[ 39,  40,  52],\n",
       "        [ 39,  40,  52],\n",
       "        [ 39,  40,  54],\n",
       "        ...,\n",
       "        [188, 207, 217],\n",
       "        [188, 207, 217],\n",
       "        [189, 208, 218]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[175, 139, 108],\n",
       "        [175, 139, 108],\n",
       "        [176, 140, 109],\n",
       "        ...,\n",
       "        [161, 176, 181],\n",
       "        [162, 177, 182],\n",
       "        [162, 177, 182]],\n",
       "\n",
       "       [[171, 138, 106],\n",
       "        [171, 138, 106],\n",
       "        [172, 139, 107],\n",
       "        ...,\n",
       "        [161, 176, 181],\n",
       "        [161, 176, 181],\n",
       "        [161, 176, 181]],\n",
       "\n",
       "       [[170, 137, 105],\n",
       "        [170, 137, 105],\n",
       "        [171, 138, 106],\n",
       "        ...,\n",
       "        [161, 176, 181],\n",
       "        [161, 176, 181],\n",
       "        [161, 176, 181]]], dtype=uint8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "def preprocess_frame(frame, target_size=(640, 640)):\n",
    "    h, w, _ = frame.shape\n",
    "    scale = min(target_size[0] / h, target_size[1] / w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized_frame = cv2.resize(frame, (new_w, new_h))\n",
    "    pad_w = (target_size[1] - new_w) // 2\n",
    "    pad_h = (target_size[0] - new_h) // 2\n",
    "    padded_frame = cv2.copyMakeBorder(resized_frame, pad_h, pad_h, pad_w, pad_w, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "    rgb_frame = cv2.cvtColor(padded_frame, cv2.COLOR_BGR2RGB)\n",
    "    normalized_frame = rgb_frame / 255.0\n",
    "    return normalized_frame\n",
    "\n",
    "def extract_features_from_video(video_path, detection_model_path, segmentation_model_path, pose_model_path, output_yaml, label):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Device:\", torch.cuda.current_device())\n",
    "        print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "    detection_model = YOLO(detection_model_path).to(device)\n",
    "    segmentation_model = YOLO(segmentation_model_path).to(device)\n",
    "    pose_model = YOLO(pose_model_path).to(device)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    video_data = {\n",
    "        'video_metadata': {\n",
    "            'path': video_path,\n",
    "            'fps': cap.get(cv2.CAP_PROP_FPS),\n",
    "            'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    frame_skip = 2\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_skip != 0:\n",
    "            frame_count += 1\n",
    "            continue\n",
    "\n",
    "        preprocessed_frame = preprocess_frame(frame)\n",
    "        preprocessed_frame_tensor = torch.tensor(preprocessed_frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "        detection_results = detection_model(preprocessed_frame_tensor)\n",
    "        segmentation_results = segmentation_model(preprocessed_frame_tensor)\n",
    "        pose_results = pose_model(preprocessed_frame_tensor)\n",
    "\n",
    "        frame_data = {\n",
    "            'frame_number': frame_count,\n",
    "            'timestamp': frame_count / video_data['video_metadata']['fps'],\n",
    "            'features': {\n",
    "                'num_humans': 0,\n",
    "                'num_weapons': 0,\n",
    "                'avg_confidence': 0,\n",
    "                'bounding_boxes': [],\n",
    "                'keypoints': [],\n",
    "                'segmented_areas': [],\n",
    "                'segmentation_confidence': []\n",
    "            },\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "        total_confidence = 0\n",
    "        for result in detection_results:\n",
    "            if hasattr(result, 'names'):\n",
    "                for box in result.boxes:\n",
    "                    try:\n",
    "                        cls = result.names[int(box.cls[0])]\n",
    "                        conf = box.conf[0]\n",
    "                        x1, y1, x2, y2 = box.xyxy[0]\n",
    "\n",
    "                        if cls == 'person':\n",
    "                            frame_data['features']['num_humans'] += 1\n",
    "                        elif cls == 'weapon':  \n",
    "                            frame_data['features']['num_weapons'] += 1\n",
    "\n",
    "                        frame_data['features']['bounding_boxes'].append([float(x1), float(y1), float(x2), float(y2)])\n",
    "                        total_confidence += float(conf)\n",
    "\n",
    "                        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "                        cv2.putText(frame, f'{cls} {conf:.2f}', (int(x1), int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "                    except KeyError as e:\n",
    "                        print(f\"KeyError: {e} - Class index {box.cls[0]} not found in result.names\")\n",
    "            else:\n",
    "                print(\"Result does not have 'names' attribute.\")\n",
    "\n",
    "        if len(detection_results) > 0:\n",
    "            frame_data['features']['avg_confidence'] = total_confidence / len(detection_results)\n",
    "\n",
    "        for result in segmentation_results:\n",
    "            if result.masks is not None:\n",
    "                for mask in result.masks:\n",
    "                    # Ensure mask is a NumPy array\n",
    "                    if isinstance(mask, torch.Tensor):\n",
    "                        mask = mask.cpu().numpy()\n",
    "\n",
    "                    # Check if mask is a single-channel image\n",
    "                    if len(mask.shape) == 3 and mask.shape[2] == 1:\n",
    "                        mask = mask[:, :, 0]\n",
    "\n",
    "                    # Debugging information\n",
    "                    print(\"Mask type:\", type(mask))\n",
    "                    print(\"Mask shape:\", mask.shape)\n",
    "\n",
    "                    try:\n",
    "                        area = cv2.countNonZero(mask)\n",
    "                        frame_data['features']['segmented_areas'].append(area)\n",
    "                        frame_data['features']['segmentation_confidence'].append(mask.confidence)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing mask: {e}\")\n",
    "            else:\n",
    "                print(\"No masks found for this frame.\")\n",
    "\n",
    "        for result in pose_results:\n",
    "            if result.keypoints is not None and len(result.keypoints.xy) > 0:\n",
    "                for keypoint in result.keypoints.xy:\n",
    "                    if len(keypoint) >= 3:\n",
    "                        x, y = keypoint[0], keypoint[1]\n",
    "                        confidence = keypoint[2]\n",
    "                    else:\n",
    "                        x, y, confidence = None, None, None\n",
    "                    frame_data['features']['keypoints'].append([x, y, confidence])\n",
    "\n",
    "        video_data['frames'].append(frame_data)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    with open(output_yaml, 'w') as file:\n",
    "        yaml.dump(video_data, file, default_flow_style=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    video_path = filedialog.askopenfilename(title=\"Select Video File\", filetypes=[(\"MP4 Files\", \"*.mp4\"), (\"AVI Files\", \"*.avi\")])\n",
    "\n",
    "    detection_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m.pt'\n",
    "    segmentation_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-seg.pt'\n",
    "    pose_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-pose.pt'\n",
    "    output_yaml = r'C:\\Users\\harme\\Desktop\\video-detect-gpu\\video_features.yaml'\n",
    "    label = 1\n",
    "\n",
    "    if video_path:\n",
    "        extract_features_from_video(video_path, detection_model_path, segmentation_model_path, pose_model_path, output_yaml, label)\n",
    "    else:\n",
    "        print(\"No video file selected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device: 0\n",
      "GPU Name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "def preprocess_frame(frame, target_size=(640, 640)):\n",
    "    \"\"\"\n",
    "    Preprocesses the input frame by resizing, padding, and normalizing.\n",
    "    \"\"\"\n",
    "    h, w, _ = frame.shape\n",
    "    scale = min(target_size[0] / h, target_size[1] / w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized_frame = cv2.resize(frame, (new_w, new_h))\n",
    "    pad_w = (target_size[1] - new_w) // 2\n",
    "    pad_h = (target_size[0] - new_h) // 2\n",
    "    padded_frame = cv2.copyMakeBorder(\n",
    "        resized_frame, pad_h, pad_h, pad_w, pad_w, cv2.BORDER_CONSTANT, value=(0, 0, 0)\n",
    "    )\n",
    "    rgb_frame = cv2.cvtColor(padded_frame, cv2.COLOR_BGR2RGB)\n",
    "    normalized_frame = rgb_frame / 255.0\n",
    "    return normalized_frame\n",
    "\n",
    "def extract_features_from_video(video_path, detection_model_path, segmentation_model_path, pose_model_path, output_yaml, label):\n",
    "    \"\"\"\n",
    "    Extracts features from a video using YOLO models for detection, segmentation, and pose estimation.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Device:\", torch.cuda.current_device())\n",
    "        print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "    # Load YOLO models\n",
    "    detection_model = YOLO(detection_model_path).to(device)\n",
    "    segmentation_model = YOLO(segmentation_model_path).to(device)\n",
    "    pose_model = YOLO(pose_model_path).to(device)\n",
    "\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    video_data = {\n",
    "        'video_metadata': {\n",
    "            'path': video_path,\n",
    "            'fps': cap.get(cv2.CAP_PROP_FPS),\n",
    "            'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    frame_skip = 2  # Process every 2nd frame\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_skip != 0:\n",
    "            frame_count += 1\n",
    "            continue\n",
    "\n",
    "        preprocessed_frame = preprocess_frame(frame)\n",
    "        preprocessed_frame_tensor = torch.tensor(preprocessed_frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "        # Perform inference with YOLO models\n",
    "        detection_results = detection_model(preprocessed_frame_tensor, verbose=False)\n",
    "        segmentation_results = segmentation_model(preprocessed_frame_tensor, verbose=False)\n",
    "        pose_results = pose_model(preprocessed_frame_tensor, verbose=False)\n",
    "\n",
    "        frame_data = {\n",
    "            'frame_number': frame_count,\n",
    "            'timestamp': frame_count / video_data['video_metadata']['fps'],\n",
    "            'features': {\n",
    "                'num_humans': 0,\n",
    "                'num_weapons': 0,\n",
    "                'avg_confidence': 0,\n",
    "                'bounding_boxes': [],\n",
    "                'keypoints': [],\n",
    "                'segmented_areas': [],\n",
    "                'shape_descriptors': [],\n",
    "                'interaction_scores': [],\n",
    "                'color_histograms': [],\n",
    "                'movement_vectors': [],\n",
    "                'segmentation_confidence': []\n",
    "            },\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "        total_confidence = 0\n",
    "        detection_confidences = []\n",
    "\n",
    "        # Process Detection Results\n",
    "        for result in detection_results:\n",
    "            if hasattr(result, 'names'):\n",
    "                for box in result.boxes:\n",
    "                    try:\n",
    "                        cls = result.names[int(box.cls[0])]\n",
    "                        conf = float(box.conf[0])\n",
    "                        x1, y1, x2, y2 = map(float, box.xyxy[0])\n",
    "\n",
    "                        if cls == 'person':\n",
    "                            frame_data['features']['num_humans'] += 1\n",
    "                        elif cls == 'weapon':\n",
    "                            frame_data['features']['num_weapons'] += 1\n",
    "\n",
    "                        frame_data['features']['bounding_boxes'].append([x1, y1, x2, y2])\n",
    "                        total_confidence += conf\n",
    "                        detection_confidences.append(conf)\n",
    "\n",
    "                        # Draw bounding box\n",
    "                        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "                        cv2.putText(frame, f'{cls} {conf:.2f}', (int(x1), int(y1) - 5),\n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "                    except KeyError as e:\n",
    "                        print(f\"KeyError: {e} - Class index {box.cls[0]} not found in result.names\")\n",
    "\n",
    "        if detection_confidences:\n",
    "            frame_data['features']['avg_confidence'] = sum(detection_confidences) / len(detection_confidences)\n",
    "\n",
    "        # Process Segmentation Results\n",
    "        for result in segmentation_results:\n",
    "            if result.masks is not None and hasattr(result.masks, 'data') and result.masks.data is not None:\n",
    "                masks = result.masks.data\n",
    "                for idx, mask_tensor in enumerate(masks):\n",
    "                    mask_np = mask_tensor.cpu().numpy()\n",
    "                    if mask_np.ndim == 3 and mask_np.shape[0] == 1:\n",
    "                        mask_np = mask_np[0]\n",
    "                    mask_binary = (mask_np > 0.5).astype(np.uint8)\n",
    "                    try:\n",
    "                        area = cv2.countNonZero(mask_binary)\n",
    "                        frame_data['features']['segmented_areas'].append(int(area))\n",
    "                        if idx < len(detection_confidences):\n",
    "                            frame_data['features']['segmentation_confidence'].append(detection_confidences[idx])\n",
    "                        else:\n",
    "                            frame_data['features']['segmentation_confidence'].append(None)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing segmentation: {e}\")\n",
    "                        frame_data['features']['segmented_areas'].append(0)\n",
    "                        frame_data['features']['segmentation_confidence'].append(None)\n",
    "\n",
    "        # Process Pose Results\n",
    "        for result in pose_results:\n",
    "            if result.keypoints is not None and hasattr(result.keypoints, 'xy') and len(result.keypoints.xy) > 0:\n",
    "                for keypoint_set in result.keypoints.xy:\n",
    "                    keypoints_list = []\n",
    "                    for i in range(len(keypoint_set)):\n",
    "                        try:\n",
    "                            keypoint = keypoint_set[i]\n",
    "                            if torch.is_tensor(keypoint):\n",
    "                                x = keypoint[0].item() if keypoint.shape[0] > 0 else None\n",
    "                                y = keypoint[1].item() if keypoint.shape[0] > 1 else None\n",
    "                                conf = keypoint[2].item() if keypoint.shape[0] > 2 else None\n",
    "                            else:\n",
    "                                x = float(keypoint[0]) if len(keypoint) > 0 else None\n",
    "                                y = float(keypoint[1]) if len(keypoint) > 1 else None\n",
    "                                conf = float(keypoint[2]) if len(keypoint) > 2 else None\n",
    "                            keypoints_list.append([x, y, conf])\n",
    "                        except (IndexError, AttributeError) as e:\n",
    "                            keypoints_list.append([None, None, None])\n",
    "                    frame_data['features']['keypoints'].append(keypoints_list)\n",
    "\n",
    "        # Append frame data and display\n",
    "        video_data['frames'].append(frame_data)\n",
    "        cv2.imshow('Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save features to YAML\n",
    "    with open(output_yaml, 'w') as file:\n",
    "        yaml.dump(video_data, file, default_flow_style=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Tkinter and hide root window\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Select video file\n",
    "    video_path = filedialog.askopenfilename(\n",
    "        title=\"Select Video File\",\n",
    "        filetypes=[(\"MP4 Files\", \"*.mp4\"), (\"AVI Files\", \"*.avi\")]\n",
    "    )\n",
    "\n",
    "    # Define paths\n",
    "    detection_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m.pt'\n",
    "    segmentation_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-seg.pt'\n",
    "    pose_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-pose.pt'\n",
    "    output_yaml = r'C:\\Users\\harme\\Desktop\\video-detect-gpu\\video_features.yaml'\n",
    "    label = 1\n",
    "\n",
    "    if video_path:\n",
    "        extract_features_from_video(\n",
    "            video_path,\n",
    "            detection_model_path,\n",
    "            segmentation_model_path,\n",
    "            pose_model_path,\n",
    "            output_yaml,\n",
    "            label\n",
    "        )\n",
    "    else:\n",
    "        print(\"No video file selected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Processing frame 0\n",
      "Processed 0 frames\n",
      "Processing frame 2\n",
      "Processing frame 4\n",
      "Processing frame 6\n",
      "Processing frame 8\n",
      "Processing frame 10\n",
      "Processed 10 frames\n",
      "Processing frame 12\n",
      "Processing frame 14\n",
      "Processing frame 16\n",
      "Processing frame 18\n",
      "Processing frame 20\n",
      "Processed 20 frames\n",
      "Processing frame 22\n",
      "Processing frame 24\n",
      "Processing frame 26\n",
      "Processing frame 28\n",
      "Processing frame 30\n",
      "Processed 30 frames\n",
      "Processing frame 32\n",
      "Processing frame 34\n",
      "Processing frame 36\n",
      "Processing frame 38\n",
      "Processing frame 40\n",
      "Processed 40 frames\n",
      "Processing frame 42\n",
      "Processing frame 44\n",
      "Processing frame 46\n",
      "Processing frame 48\n",
      "Processing frame 50\n",
      "Processed 50 frames\n",
      "Processing frame 52\n",
      "Processing frame 54\n",
      "Processing frame 56\n",
      "Processing frame 58\n",
      "Processing frame 60\n",
      "Processed 60 frames\n",
      "Processing frame 62\n",
      "Processing frame 64\n",
      "Saving data to C:\\Users\\harme\\Desktop\\video-detect-gpu\\video_features.yaml\n",
      "Data saved successfully\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "# [Previous preprocess_frame function remains the same]\n",
    "def preprocess_frame(frame, target_size=(640, 640)):\n",
    "    \"\"\"\n",
    "    Preprocesses the input frame by resizing, padding, and normalizing.\n",
    "    \"\"\"\n",
    "    h, w, _ = frame.shape\n",
    "    scale = min(target_size[0] / h, target_size[1] / w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized_frame = cv2.resize(frame, (new_w, new_h))\n",
    "    pad_w = (target_size[1] - new_w) // 2\n",
    "    pad_h = (target_size[0] - new_h) // 2\n",
    "    padded_frame = cv2.copyMakeBorder(\n",
    "        resized_frame, pad_h, pad_h, pad_w, pad_w, cv2.BORDER_CONSTANT, value=(0, 0, 0)\n",
    "    )\n",
    "    rgb_frame = cv2.cvtColor(padded_frame, cv2.COLOR_BGR2RGB)\n",
    "    normalized_frame = rgb_frame / 255.0\n",
    "    return normalized_frame\n",
    "\n",
    "def convert_tensor_to_serializable(obj):\n",
    "    \"\"\"\n",
    "    Converts tensor objects to serializable Python types.\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(obj):\n",
    "        return obj.cpu().numpy().tolist()\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_tensor_to_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_tensor_to_serializable(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "def extract_features_from_video(video_path, detection_model_path, segmentation_model_path, pose_model_path, output_yaml, label):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "    \n",
    "    # Load YOLO models\n",
    "    detection_model = YOLO(detection_model_path).to(device)\n",
    "    segmentation_model = YOLO(segmentation_model_path).to(device)\n",
    "    pose_model = YOLO(pose_model_path).to(device)\n",
    "\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return\n",
    "\n",
    "    frame_count = 0\n",
    "    video_data = {\n",
    "        'video_metadata': {\n",
    "            'path': str(video_path),\n",
    "            'fps': float(cap.get(cv2.CAP_PROP_FPS)),\n",
    "            'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    frame_skip = 2\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if frame_count % frame_skip != 0:\n",
    "                frame_count += 1\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing frame {frame_count}\")  # Debug print\n",
    "\n",
    "            preprocessed_frame = preprocess_frame(frame)\n",
    "            preprocessed_frame_tensor = torch.tensor(preprocessed_frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "            # Perform inference\n",
    "            detection_results = detection_model(preprocessed_frame_tensor, verbose=False)\n",
    "            segmentation_results = segmentation_model(preprocessed_frame_tensor, verbose=False)\n",
    "            pose_results = pose_model(preprocessed_frame_tensor, verbose=False)\n",
    "\n",
    "            frame_data = {\n",
    "                'frame_number': int(frame_count),\n",
    "                'timestamp': float(frame_count / video_data['video_metadata']['fps']),\n",
    "                'features': {\n",
    "                    'num_humans': 0,\n",
    "                    'num_weapons': 0,\n",
    "                    'avg_confidence': 0.0,\n",
    "                    'bounding_boxes': [],\n",
    "                    'keypoints': [],\n",
    "                    'segmented_areas': [],\n",
    "                    'segmentation_confidence': []\n",
    "                },\n",
    "                'label': int(label)\n",
    "            }\n",
    "\n",
    "            # Process detections\n",
    "            detection_confidences = []\n",
    "            for result in detection_results:\n",
    "                if hasattr(result, 'boxes'):\n",
    "                    for box in result.boxes:\n",
    "                        try:\n",
    "                            cls = int(box.cls[0])\n",
    "                            conf = float(box.conf[0])\n",
    "                            xyxy = box.xyxy[0].cpu().numpy().tolist()\n",
    "\n",
    "                            if cls == 0:  # Assuming 0 is person class\n",
    "                                frame_data['features']['num_humans'] += 1\n",
    "                            \n",
    "                            frame_data['features']['bounding_boxes'].append({\n",
    "                                'class': cls,\n",
    "                                'confidence': conf,\n",
    "                                'coordinates': xyxy\n",
    "                            })\n",
    "                            detection_confidences.append(conf)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing detection: {e}\")\n",
    "\n",
    "            # Calculate average confidence\n",
    "            if detection_confidences:\n",
    "                frame_data['features']['avg_confidence'] = float(sum(detection_confidences) / len(detection_confidences))\n",
    "\n",
    "            # Process segmentation\n",
    "            for result in segmentation_results:\n",
    "                if result.masks is not None:\n",
    "                    for mask in result.masks.data:\n",
    "                        try:\n",
    "                            mask_np = mask.cpu().numpy()\n",
    "                            area = float(np.sum(mask_np > 0.5))\n",
    "                            frame_data['features']['segmented_areas'].append(area)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing segmentation: {e}\")\n",
    "\n",
    "            # Process pose\n",
    "            for result in pose_results:\n",
    "                if result.keypoints is not None:\n",
    "                    try:\n",
    "                        keypoints = result.keypoints.xy\n",
    "                        keypoints_data = convert_tensor_to_serializable(keypoints)\n",
    "                        frame_data['features']['keypoints'].extend(keypoints_data)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing pose: {e}\")\n",
    "\n",
    "            # Append frame data\n",
    "            video_data['frames'].append(frame_data)\n",
    "\n",
    "            # Display progress\n",
    "            if frame_count % 10 == 0:\n",
    "                print(f\"Processed {frame_count} frames\")\n",
    "\n",
    "            # Display frame\n",
    "            cv2.imshow('Detection', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Convert all data to serializable format\n",
    "        video_data = convert_tensor_to_serializable(video_data)\n",
    "\n",
    "        # Save to YAML file\n",
    "        try:\n",
    "            print(f\"Saving data to {output_yaml}\")\n",
    "            with open(output_yaml, 'w') as file:\n",
    "                yaml.dump(video_data, file, default_flow_style=False)\n",
    "            print(\"Data saved successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving YAML file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Get video file\n",
    "    video_path = filedialog.askopenfilename(\n",
    "        title=\"Select Video File\",\n",
    "        filetypes=[(\"Video Files\", \"*.mp4;*.avi\")]\n",
    "    )\n",
    "\n",
    "    if not video_path:\n",
    "        print(\"No video file selected\")\n",
    "        exit()\n",
    "\n",
    "    # Define paths\n",
    "    detection_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m.pt'\n",
    "    segmentation_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-seg.pt'\n",
    "    pose_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-pose.pt'\n",
    "    output_yaml = r'C:\\Users\\harme\\Desktop\\video-detect-gpu\\video_features.yaml'\n",
    "    label = 1\n",
    "\n",
    "    # Run feature extraction\n",
    "    extract_features_from_video(\n",
    "        video_path,\n",
    "        detection_model_path,\n",
    "        segmentation_model_path,\n",
    "        pose_model_path,\n",
    "        output_yaml,\n",
    "        label\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "def preprocess_frame(frame, target_size=(640, 640)):\n",
    "    \"\"\"\n",
    "    Preprocesses the input frame by resizing, padding, and normalizing.\n",
    "    \"\"\"\n",
    "    h, w, _ = frame.shape\n",
    "    scale = min(target_size[0] / h, target_size[1] / w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized_frame = cv2.resize(frame, (new_w, new_h))\n",
    "    pad_w = (target_size[1] - new_w) // 2\n",
    "    pad_h = (target_size[0] - new_h) // 2\n",
    "    padded_frame = cv2.copyMakeBorder(\n",
    "        resized_frame, pad_h, pad_h, pad_w, pad_w, cv2.BORDER_CONSTANT, value=(0, 0, 0)\n",
    "    )\n",
    "    rgb_frame = cv2.cvtColor(padded_frame, cv2.COLOR_BGR2RGB)\n",
    "    normalized_frame = rgb_frame / 255.0\n",
    "    return normalized_frame\n",
    "\n",
    "def draw_detections(frame, detections, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and labels on the frame\n",
    "    \"\"\"\n",
    "    annotated_frame = frame.copy()\n",
    "    for det in detections:\n",
    "        if hasattr(det, 'boxes'):\n",
    "            for box in det.boxes:\n",
    "                # Get box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "                \n",
    "                # Get confidence and class\n",
    "                conf = float(box.conf[0])\n",
    "                cls = int(box.cls[0])\n",
    "                \n",
    "                # Draw rectangle\n",
    "                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                \n",
    "                # Add label\n",
    "                label = f'Class {cls}: {conf:.2f}'\n",
    "                cv2.putText(annotated_frame, label, (x1, y1 - 10),\n",
    "                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    return annotated_frame\n",
    "\n",
    "def draw_poses(frame, poses, color=(255, 0, 0)):\n",
    "    \"\"\"\n",
    "    Draw pose keypoints and connections on the frame\n",
    "    \"\"\"\n",
    "    annotated_frame = frame.copy()\n",
    "    if poses is not None:\n",
    "        for pose in poses:\n",
    "            if pose.keypoints is not None:\n",
    "                keypoints = pose.keypoints[0].cpu().numpy()\n",
    "                for kp in keypoints:\n",
    "                    x, y = map(int, kp[:2])\n",
    "                    cv2.circle(annotated_frame, (x, y), 4, color, -1)\n",
    "                \n",
    "                # Draw connections (you can customize these based on your needs)\n",
    "                # Example: connecting shoulders to hips\n",
    "                if len(keypoints) >= 6:  # Assuming basic body keypoints are present\n",
    "                    # Connect shoulders\n",
    "                    cv2.line(annotated_frame, \n",
    "                            tuple(map(int, keypoints[5][:2])),\n",
    "                            tuple(map(int, keypoints[6][:2])),\n",
    "                            color, 2)\n",
    "    \n",
    "    return annotated_frame\n",
    "\n",
    "def extract_features_from_video(video_path, detection_model_path, segmentation_model_path, pose_model_path, output_yaml, label):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load models\n",
    "    detection_model = YOLO(detection_model_path)\n",
    "    segmentation_model = YOLO(segmentation_model_path)\n",
    "    pose_model = YOLO(pose_model_path)\n",
    "\n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Create video writer\n",
    "    output_video_path = video_path.rsplit('.', 1)[0] + '_annotated.mp4'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    video_data = {\n",
    "        'video_metadata': {\n",
    "            'path': str(video_path),\n",
    "            'fps': float(fps),\n",
    "            'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    frame_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Create a copy for annotation\n",
    "            display_frame = frame.copy()\n",
    "\n",
    "            # Process frame data\n",
    "            frame_data = {\n",
    "                'frame_number': frame_count,\n",
    "                'timestamp': frame_count / fps,\n",
    "                'detections': [],\n",
    "                'poses': [],\n",
    "                'segments': []\n",
    "            }\n",
    "\n",
    "            # Run detections\n",
    "            detections = detection_model(frame)\n",
    "            poses = pose_model(frame)\n",
    "            segments = segmentation_model(frame)\n",
    "\n",
    "            # Process detections\n",
    "            for detection in detections:\n",
    "                if hasattr(detection, 'boxes'):\n",
    "                    for box in detection.boxes:\n",
    "                        det_data = {\n",
    "                            'bbox': box.xyxy[0].cpu().numpy().tolist(),\n",
    "                            'confidence': float(box.conf[0]),\n",
    "                            'class': int(box.cls[0])\n",
    "                        }\n",
    "                        frame_data['detections'].append(det_data)\n",
    "\n",
    "                        # Draw detection\n",
    "                        x1, y1, x2, y2 = map(int, det_data['bbox'])\n",
    "                        cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                        label = f\"Class {det_data['class']}: {det_data['confidence']:.2f}\"\n",
    "                        cv2.putText(display_frame, label, (x1, y1-10), \n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # Process poses\n",
    "            if poses:\n",
    "                for pose in poses:\n",
    "                    if pose.keypoints is not None:\n",
    "                        kpts = pose.keypoints.data[0].cpu().numpy()\n",
    "                        pose_data = {\n",
    "                            'keypoints': kpts.tolist()\n",
    "                        }\n",
    "                        frame_data['poses'].append(pose_data)\n",
    "\n",
    "                        # Draw pose keypoints\n",
    "                        for kpt in kpts:\n",
    "                            x, y = map(int, kpt[:2])\n",
    "                            cv2.circle(display_frame, (x, y), 4, (255, 0, 0), -1)\n",
    "\n",
    "            # Process segmentation\n",
    "            if segments:\n",
    "                for segment in segments:\n",
    "                    if segment.masks is not None:\n",
    "                        masks = segment.masks.data.cpu().numpy()\n",
    "                        for mask in masks:\n",
    "                            mask = (mask > 0.5).astype(np.uint8) * 255\n",
    "                            colored_mask = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
    "                            display_frame = cv2.addWeighted(display_frame, 0.7, colored_mask, 0.3, 0)\n",
    "\n",
    "            # Add frame data to video data\n",
    "            video_data['frames'].append(frame_data)\n",
    "\n",
    "            # Display and save frame\n",
    "            cv2.imshow('Detections', display_frame)\n",
    "            out.write(display_frame)\n",
    "\n",
    "            # Process key events\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('p'):\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "            frame_count += 1\n",
    "            if frame_count % 30 == 0:\n",
    "                print(f\"Processed {frame_count} frames\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        raise  # This will show the full error traceback\n",
    "\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Save features to YAML\n",
    "        try:\n",
    "            with open(output_yaml, 'w') as file:\n",
    "                yaml.dump(video_data, file, default_flow_style=False)\n",
    "            print(f\"Features saved to {output_yaml}\")\n",
    "            print(f\"Annotated video saved to {output_video_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {e}\")\n",
    "            \n",
    "    # Define paths\n",
    "    detection_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m.pt'\n",
    "    segmentation_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-seg.pt'\n",
    "    pose_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-pose.pt'\n",
    "    output_yaml = r'C:\\Users\\harme\\Desktop\\video-detect-gpu\\video_features.yaml'\n",
    "    label = 1\n",
    "\n",
    "    # Run feature extraction\n",
    "    extract_features_from_video(\n",
    "        video_path,\n",
    "        detection_model_path,\n",
    "        segmentation_model_path,\n",
    "        pose_model_path,\n",
    "        output_yaml,\n",
    "        label\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device: 0\n",
      "GPU Name: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "\n",
      "0: 640x640 2 persons, 4 benchs, 3 backpacks, 1 chair, 1 dining table, 27.5ms\n",
      "Speed: 0.0ms preprocess, 27.5ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 4 benchs, 3 backpacks, 1 chair, 1 dining table, 32.2ms\n",
      "Speed: 0.0ms preprocess, 32.2ms inference, 8.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 31.1ms\n",
      "Speed: 0.0ms preprocess, 31.1ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) :-1: error: (-5:Bad argument) in function 'countNonZero'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 142\u001b[0m\n\u001b[0;32m    139\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m video_path:\n\u001b[1;32m--> 142\u001b[0m     \u001b[43mextract_features_from_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetection_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegmentation_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_yaml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo video file selected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 102\u001b[0m, in \u001b[0;36mextract_features_from_video\u001b[1;34m(video_path, detection_model_path, segmentation_model_path, pose_model_path, output_yaml, label)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mmasks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mmasks:\n\u001b[1;32m--> 102\u001b[0m         area \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcountNonZero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m         frame_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegmented_areas\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(area)\n\u001b[0;32m    104\u001b[0m         frame_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegmentation_confidence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(mask\u001b[38;5;241m.\u001b[39mconfidence)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.11.0) :-1: error: (-5:Bad argument) in function 'countNonZero'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "\n",
    "def preprocess_frame(frame, target_size=(640, 640)):\n",
    "    h, w, _ = frame.shape\n",
    "    scale = min(target_size[0] / h, target_size[1] / w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized_frame = cv2.resize(frame, (new_w, new_h))\n",
    "    pad_w = (target_size[1] - new_w) // 2\n",
    "    pad_h = (target_size[0] - new_h) // 2\n",
    "    padded_frame = cv2.copyMakeBorder(resized_frame, pad_h, pad_h, pad_w, pad_w, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "    rgb_frame = cv2.cvtColor(padded_frame, cv2.COLOR_BGR2RGB)\n",
    "    normalized_frame = rgb_frame / 255.0\n",
    "    return normalized_frame\n",
    "\n",
    "\n",
    "def extract_features_from_video(video_path, detection_model_path, segmentation_model_path, pose_model_path, output_yaml, label):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Device:\", torch.cuda.current_device())\n",
    "        print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "    detection_model = YOLO(detection_model_path).to(device)\n",
    "    segmentation_model = YOLO(segmentation_model_path).to(device)\n",
    "    pose_model = YOLO(pose_model_path).to(device)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    video_data = {\n",
    "        'video_metadata': {\n",
    "            'path': video_path,\n",
    "            'fps': cap.get(cv2.CAP_PROP_FPS),\n",
    "            'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    frame_skip = 2\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_skip != 0:\n",
    "            frame_count += 1\n",
    "            continue\n",
    "\n",
    "        preprocessed_frame = preprocess_frame(frame)\n",
    "        preprocessed_frame_tensor = torch.tensor(preprocessed_frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "        detection_results = detection_model(preprocessed_frame_tensor)\n",
    "        segmentation_results = segmentation_model(preprocessed_frame_tensor)\n",
    "        pose_results = pose_model(preprocessed_frame_tensor)\n",
    "\n",
    "        frame_data = {\n",
    "            'frame_number': frame_count,\n",
    "            'timestamp': frame_count / video_data['video_metadata']['fps'],\n",
    "            'features': {\n",
    "                'num_humans': 0,\n",
    "                'num_weapons': 0,\n",
    "                'avg_confidence': 0,\n",
    "                'bounding_boxes': [],\n",
    "                'keypoints': [],\n",
    "                'segmented_areas': [],\n",
    "                'segmentation_confidence': []\n",
    "            },\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "        total_confidence = 0\n",
    "        for result in detection_results:\n",
    "            for box in result.boxes:\n",
    "                cls = result.names.get(int(box.cls[0].item()), \"Unknown\")\n",
    "                conf = box.conf[0]\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "\n",
    "                if cls == 'person':\n",
    "                    frame_data['features']['num_humans'] += 1\n",
    "                elif cls == 'weapon':  \n",
    "                    frame_data['features']['num_weapons'] += 1\n",
    "\n",
    "                frame_data['features']['bounding_boxes'].append([float(x1), float(y1), float(x2), float(y2)])\n",
    "                total_confidence += float(conf)\n",
    "\n",
    "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "                cv2.putText(frame, f'{cls} {conf:.2f}', (int(x1), int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        if len(detection_results) > 0:\n",
    "            frame_data['features']['avg_confidence'] = total_confidence / len(detection_results)\n",
    "\n",
    "        for result in segmentation_results:\n",
    "            if result.masks is not None:\n",
    "                for mask in result.masks:\n",
    "                    area = cv2.countNonZero(mask)\n",
    "                    frame_data['features']['segmented_areas'].append(area)\n",
    "                    frame_data['features']['segmentation_confidence'].append(mask.confidence)\n",
    "            else:\n",
    "                print(\"No masks found for this frame.\")\n",
    "\n",
    "        for result in pose_results:\n",
    "            if result.keypoints is not None and len(result.keypoints.xy) > 0:\n",
    "                for keypoint in result.keypoints.xy:\n",
    "                    if len(keypoint) >= 3:\n",
    "                        x, y = keypoint[0], keypoint[1]\n",
    "                        confidence = keypoint[2]\n",
    "                    else:\n",
    "                        x, y, confidence = None, None, None\n",
    "                    frame_data['features']['keypoints'].append([x, y, confidence])\n",
    "\n",
    "        video_data['frames'].append(frame_data)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    with open(output_yaml, 'w') as file:\n",
    "        yaml.dump(video_data, file, default_flow_style=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    video_path = filedialog.askopenfilename(title=\"Select Video File\", filetypes=[(\"MP4 Files\", \"*.mp4\"), (\"AVI Files\", \"*.avi\")])\n",
    "\n",
    "    detection_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m.pt'\n",
    "    segmentation_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-seg.pt'\n",
    "    pose_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-pose.pt'\n",
    "    output_yaml = r'C:\\Users\\harme\\Desktop\\violence detection\\video_features.yaml'\n",
    "    label = 1\n",
    "\n",
    "    if video_path:\n",
    "        extract_features_from_video(video_path, detection_model_path, segmentation_model_path, pose_model_path, output_yaml, label)\n",
    "    else:\n",
    "        print(\"No video file selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to preprocess video frame\n",
    "def preprocess_frame(frame, target_size=(640, 640)):\n",
    "    h, w, _ = frame.shape\n",
    "    scale = min(target_size[0] / h, target_size[1] / w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized_frame = cv2.resize(frame, (new_w, new_h))\n",
    "\n",
    "    pad_w = (target_size[1] - new_w) // 2\n",
    "    pad_h = (target_size[0] - new_h) // 2\n",
    "    padded_frame = cv2.copyMakeBorder(resized_frame, pad_h, pad_h, pad_w, pad_w, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(padded_frame, cv2.COLOR_BGR2RGB)\n",
    "    normalized_frame = rgb_frame / 255.0\n",
    "\n",
    "    return normalized_frame\n",
    "\n",
    "# Main function for feature extraction\n",
    "def extract_features_from_video(video_path, detection_model_path, segmentation_model_path, pose_model_path, output_yaml, label):\n",
    "    detection_model = YOLO(detection_model_path).to('cuda')\n",
    "    segmentation_model = YOLO(segmentation_model_path).to('cuda')\n",
    "    pose_model = YOLO(pose_model_path).to('cuda')\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    video_data = {\n",
    "        'video_metadata': {\n",
    "            'path': video_path,\n",
    "            'fps': cap.get(cv2.CAP_PROP_FPS),\n",
    "            'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    frame_skip = 2  # Process every 2nd frame\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_skip != 0:\n",
    "            frame_count += 1\n",
    "            continue\n",
    "\n",
    "        preprocessed_frame = preprocess_frame(frame)\n",
    "        preprocessed_frame_tensor = torch.tensor(preprocessed_frame).to('cuda')\n",
    "\n",
    "        detection_results = detection_model(preprocessed_frame_tensor)\n",
    "        segmentation_results = segmentation_model(preprocessed_frame_tensor)\n",
    "        pose_results = pose_model(preprocessed_frame_tensor)\n",
    "\n",
    "        frame_data = {\n",
    "            'frame_number': frame_count,\n",
    "            'timestamp': frame_count / video_data['video_metadata']['fps'],\n",
    "            'features': {\n",
    "                'num_humans': 0,\n",
    "                'num_weapons': 0,\n",
    "                'avg_confidence': 0,\n",
    "                'bounding_boxes': [],\n",
    "                'keypoints': [],\n",
    "                'segmented_areas': [],\n",
    "                'segmentation_confidence': []\n",
    "            },\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "        total_confidence = 0\n",
    "        for result in detection_results:\n",
    "            for box in result.boxes:\n",
    "                cls = result.names[box.cls[0]]\n",
    "                conf = box.conf[0]\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "\n",
    "                if cls == 'person':\n",
    "                    frame_data['features']['num_humans'] += 1\n",
    "                elif cls == 'weapon':\n",
    "                    frame_data['features']['num_weapons'] += 1\n",
    "\n",
    "                frame_data['features']['bounding_boxes'].append([float(x1), float(y1), float(x2), float(y2)])\n",
    "                total_confidence += float(conf)\n",
    "\n",
    "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "                cv2.putText(frame, f'{cls} {conf:.2f}', (int(x1), int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        if len(detection_results) > 0:\n",
    "            frame_data['features']['avg_confidence'] = total_confidence / len(detection_results)\n",
    "\n",
    "        for result in segmentation_results:\n",
    "            if result.masks is not None:\n",
    "                for mask in result.masks:\n",
    "                    area = cv2.countNonZero(mask)\n",
    "                    frame_data['features']['segmented_areas'].append(area)\n",
    "                    frame_data['features']['segmentation_confidence'].append(mask.confidence)\n",
    "\n",
    "        for result in pose_results:\n",
    "            if result.keypoints is not None and len(result.keypoints.xy) > 0:\n",
    "                for keypoint in result.keypoints.xy:\n",
    "                    if len(keypoint) >= 3:\n",
    "                        x, y = keypoint[0], keypoint[1]\n",
    "                        confidence = keypoint[2]\n",
    "                    else:\n",
    "                        x, y, confidence = None, None, None\n",
    "                    frame_data['features']['keypoints'].append([x, y, confidence])\n",
    "\n",
    "        video_data['frames'].append(frame_data)\n",
    "        cv2.imshow('Detections', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    with open(output_yaml, 'w') as file:\n",
    "        yaml.dump(video_data, file, default_flow_style=False)\n",
    "\n",
    "video_path = r'C:\\Users\\harme\\Desktop\\violence detection\\data\\fi21_xvid.avi'\n",
    "detection_model_path = r'C:\\Users\\harme\\Desktop\\video-detect-gpu\\yolo11m.pt'\n",
    "segmentation_model_path = r'C:\\Users\\harme\\Desktop\\vide0-detect-gpu\\yolo11m-seg.pt'\n",
    "pose_model_path = r'C:\\Users\\harme\\Desktop\\violence-detect-gpu\\yolo11m-pose.pt'\n",
    "output_yaml = r'C:\\Users\\harme\\Desktop\\video-detect-gpu\\video_features.yaml'\n",
    "label = 1\n",
    "\n",
    "extract_features_from_video(video_path, detection_model_path, segmentation_model_path, pose_model_path, output_yaml, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "def preprocess_frame(frame, target_size=(640, 640)):\n",
    "    \"\"\"\n",
    "    Preprocesses the input frame by resizing, padding, and normalizing.\n",
    "    \"\"\"\n",
    "    h, w, _ = frame.shape\n",
    "    scale = min(target_size[0] / h, target_size[1] / w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized_frame = cv2.resize(frame, (new_w, new_h))\n",
    "    pad_w = (target_size[1] - new_w) // 2\n",
    "    pad_h = (target_size[0] - new_h) // 2\n",
    "    padded_frame = cv2.copyMakeBorder(\n",
    "        resized_frame, pad_h, pad_h, pad_w, pad_w, cv2.BORDER_CONSTANT, value=(0, 0, 0)\n",
    "    )\n",
    "    rgb_frame = cv2.cvtColor(padded_frame, cv2.COLOR_BGR2RGB)\n",
    "    normalized_frame = rgb_frame / 255.0\n",
    "    return normalized_frame\n",
    "\n",
    "def draw_detections(frame, boxes, scores, class_ids, class_names):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and labels on the frame\n",
    "    \"\"\"\n",
    "    for box, score, class_id in zip(boxes, scores, class_ids):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        class_name = class_names.get(class_id, f\"Class {class_id}\")\n",
    "        \n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw label\n",
    "        label = f'{class_name} {score:.2f}'\n",
    "        (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "        cv2.rectangle(frame, (x1, y1 - label_height - 10), (x1 + label_width, y1), (0, 255, 0), -1)\n",
    "        cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "def draw_keypoints(frame, keypoints, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Draw pose keypoints and connections on the frame\n",
    "    \"\"\"\n",
    "    if keypoints is None:\n",
    "        return frame\n",
    "\n",
    "    # Define keypoint connections (you can customize this based on your needs)\n",
    "    connections = [\n",
    "        (5, 7), (7, 9),    # Right arm\n",
    "        (6, 8), (8, 10),   # Left arm\n",
    "        (11, 13), (13, 15), # Right leg\n",
    "        (12, 14), (14, 16), # Left leg\n",
    "        (5, 6),            # Shoulders\n",
    "        (11, 12),          # Hips\n",
    "        (5, 11), (6, 12)   # Spine\n",
    "    ]\n",
    "\n",
    "    # Draw keypoints\n",
    "    for kpt in keypoints:\n",
    "        for i, (x, y, conf) in enumerate(kpt):\n",
    "            if conf > confidence_threshold:\n",
    "                cv2.circle(frame, (int(x), int(y)), 4, (255, 0, 0), -1)\n",
    "\n",
    "    # Draw connections\n",
    "    for kpt in keypoints:\n",
    "        for connection in connections:\n",
    "            if (kpt[connection[0]][2] > confidence_threshold and \n",
    "                kpt[connection[1]][2] > confidence_threshold):\n",
    "                pt1 = (int(kpt[connection[0]][0]), int(kpt[connection[0]][1]))\n",
    "                pt2 = (int(kpt[connection[1]][0]), int(kpt[connection[1]][1]))\n",
    "                cv2.line(frame, pt1, pt2, (0, 255, 255), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def apply_segmentation_mask(frame, masks, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Apply segmentation masks to the frame\n",
    "    \"\"\"\n",
    "    if masks is None:\n",
    "        return frame\n",
    "\n",
    "    overlay = frame.copy()\n",
    "    \n",
    "    for mask in masks:\n",
    "        color = np.random.randint(0, 255, 3).tolist()\n",
    "        mask_np = mask.cpu().numpy() if torch.is_tensor(mask) else mask\n",
    "        mask_np = (mask_np > 0.5).astype(np.uint8)\n",
    "        \n",
    "        colored_mask = np.zeros_like(frame)\n",
    "        colored_mask[mask_np > 0] = color\n",
    "        \n",
    "        cv2.addWeighted(colored_mask, alpha, overlay, 1 - alpha, 0, overlay)\n",
    "    \n",
    "    return overlay\n",
    "\n",
    "def process_frame(frame, detection_model, segmentation_model, pose_model):\n",
    "    \"\"\"\n",
    "    Process a single frame through all models\n",
    "    \"\"\"\n",
    "    # Run models\n",
    "    det_results = detection_model(frame, verbose=False)\n",
    "    seg_results = segmentation_model(frame, verbose=False)\n",
    "    pose_results = pose_model(frame, verbose=False)\n",
    "\n",
    "    frame_data = {\n",
    "        'detections': [],\n",
    "        'segmentations': [],\n",
    "        'poses': []\n",
    "    }\n",
    "\n",
    "    # Process detections\n",
    "    if det_results:\n",
    "        for result in det_results:\n",
    "            if hasattr(result, 'boxes'):\n",
    "                boxes = result.boxes\n",
    "                for box in boxes:\n",
    "                    det_data = {\n",
    "                        'bbox': box.xyxy[0].cpu().numpy().tolist(),\n",
    "                        'confidence': float(box.conf[0]),\n",
    "                        'class_id': int(box.cls[0])\n",
    "                    }\n",
    "                    frame_data['detections'].append(det_data)\n",
    "\n",
    "    # Process segmentations\n",
    "    if seg_results:\n",
    "        for result in seg_results:\n",
    "            if hasattr(result, 'masks') and result.masks is not None:\n",
    "                for mask in result.masks.data:\n",
    "                    seg_data = {\n",
    "                        'mask': mask.cpu().numpy().tolist()\n",
    "                    }\n",
    "                    frame_data['segmentations'].append(seg_data)\n",
    "\n",
    "    # Process poses\n",
    "    if pose_results:\n",
    "        for result in pose_results:\n",
    "            if hasattr(result, 'keypoints') and result.keypoints is not None:\n",
    "                for keypoint in result.keypoints:\n",
    "                    pose_data = {\n",
    "                        'keypoints': keypoint.data[0].cpu().numpy().tolist()\n",
    "                    }\n",
    "                    frame_data['poses'].append(pose_data)\n",
    "\n",
    "    return frame_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Processing: 0.0% complete\n",
      "Processing: 2.8% complete\n",
      "Processing: 5.6% complete\n",
      "Processing: 8.4% complete\n",
      "Processing: 11.2% complete\n",
      "Processing: 13.9% complete\n",
      "Processing: 16.7% complete\n",
      "Processing: 19.5% complete\n",
      "Processing: 22.3% complete\n",
      "Processing: 25.1% complete\n",
      "Processing: 27.9% complete\n",
      "Processing: 30.7% complete\n",
      "Processing: 33.5% complete\n",
      "Processing: 36.2% complete\n",
      "Processing: 39.0% complete\n",
      "Processing: 41.8% complete\n",
      "Processing: 44.6% complete\n",
      "Processing: 47.4% complete\n",
      "Processing: 50.2% complete\n",
      "Processing: 53.0% complete\n",
      "Processing: 55.8% complete\n",
      "Processing: 58.6% complete\n",
      "Processing: 61.3% complete\n",
      "Processing: 64.1% complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 144\u001b[0m\n\u001b[0;32m    141\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Run feature extraction\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m \u001b[43mextract_features_from_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdetection_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegmentation_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpose_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_yaml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 115\u001b[0m, in \u001b[0;36mextract_features_from_video\u001b[1;34m(video_path, detection_model_path, segmentation_model_path, pose_model_path, output_yaml, label)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_yaml, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m--> 115\u001b[0m         \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_flow_style\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_yaml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnnotated video saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_video_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\__init__.py:253\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(data, stream, Dumper, **kwds)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(data, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, Dumper\u001b[38;5;241m=\u001b[39mDumper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m    249\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m    Serialize a Python object into a YAML stream.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m    If stream is None, return the produced string instead.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dump_all([data], stream, Dumper\u001b[38;5;241m=\u001b[39mDumper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\__init__.py:241\u001b[0m, in \u001b[0;36mdump_all\u001b[1;34m(documents, stream, Dumper, default_style, default_flow_style, canonical, indent, width, allow_unicode, line_break, encoding, explicit_start, explicit_end, version, tags, sort_keys)\u001b[0m\n\u001b[0;32m    239\u001b[0m     dumper\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m--> 241\u001b[0m         \u001b[43mdumper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m     dumper\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:27\u001b[0m, in \u001b[0;36mBaseRepresenter.represent\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepresent\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m---> 27\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserialize(node)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresented_objects \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:48\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     46\u001b[0m data_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__mro__\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_types[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml_representers:\n\u001b[1;32m---> 48\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaml_representers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_types\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_type \u001b[38;5;129;01min\u001b[39;00m data_types:\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:207\u001b[0m, in \u001b[0;36mSafeRepresenter.represent_dict\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepresent_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtag:yaml.org,2002:map\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:118\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_mapping\u001b[1;34m(self, tag, mapping, flow_style)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item_key, item_value \u001b[38;5;129;01min\u001b[39;00m mapping:\n\u001b[0;32m    117\u001b[0m     node_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresent_data(item_key)\n\u001b[1;32m--> 118\u001b[0m     node_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(node_key, ScalarNode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node_key\u001b[38;5;241m.\u001b[39mstyle):\n\u001b[0;32m    120\u001b[0m         best_style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:48\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     46\u001b[0m data_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__mro__\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_types[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml_representers:\n\u001b[1;32m---> 48\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaml_representers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_types\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_type \u001b[38;5;129;01min\u001b[39;00m data_types:\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:199\u001b[0m, in \u001b[0;36mSafeRepresenter.represent_list\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepresent_list\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m#pairs = (len(data) > 0 and isinstance(data, list))\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m#if pairs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m#            break\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m#if not pairs:\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtag:yaml.org,2002:seq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:92\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_sequence\u001b[1;34m(self, tag, sequence, flow_style)\u001b[0m\n\u001b[0;32m     90\u001b[0m best_style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sequence:\n\u001b[1;32m---> 92\u001b[0m     node_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(node_item, ScalarNode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node_item\u001b[38;5;241m.\u001b[39mstyle):\n\u001b[0;32m     94\u001b[0m         best_style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:48\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     46\u001b[0m data_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__mro__\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_types[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml_representers:\n\u001b[1;32m---> 48\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaml_representers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_types\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_type \u001b[38;5;129;01min\u001b[39;00m data_types:\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:207\u001b[0m, in \u001b[0;36mSafeRepresenter.represent_dict\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepresent_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtag:yaml.org,2002:map\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:118\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_mapping\u001b[1;34m(self, tag, mapping, flow_style)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item_key, item_value \u001b[38;5;129;01min\u001b[39;00m mapping:\n\u001b[0;32m    117\u001b[0m     node_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresent_data(item_key)\n\u001b[1;32m--> 118\u001b[0m     node_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(node_key, ScalarNode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node_key\u001b[38;5;241m.\u001b[39mstyle):\n\u001b[0;32m    120\u001b[0m         best_style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:48\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     46\u001b[0m data_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__mro__\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_types[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml_representers:\n\u001b[1;32m---> 48\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaml_representers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_types\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_type \u001b[38;5;129;01min\u001b[39;00m data_types:\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:199\u001b[0m, in \u001b[0;36mSafeRepresenter.represent_list\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepresent_list\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m#pairs = (len(data) > 0 and isinstance(data, list))\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m#if pairs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m#            break\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m#if not pairs:\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtag:yaml.org,2002:seq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:92\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_sequence\u001b[1;34m(self, tag, sequence, flow_style)\u001b[0m\n\u001b[0;32m     90\u001b[0m best_style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sequence:\n\u001b[1;32m---> 92\u001b[0m     node_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(node_item, ScalarNode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node_item\u001b[38;5;241m.\u001b[39mstyle):\n\u001b[0;32m     94\u001b[0m         best_style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:48\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     46\u001b[0m data_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__mro__\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_types[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml_representers:\n\u001b[1;32m---> 48\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaml_representers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_types\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_type \u001b[38;5;129;01min\u001b[39;00m data_types:\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:207\u001b[0m, in \u001b[0;36mSafeRepresenter.represent_dict\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepresent_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtag:yaml.org,2002:map\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:118\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_mapping\u001b[1;34m(self, tag, mapping, flow_style)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item_key, item_value \u001b[38;5;129;01min\u001b[39;00m mapping:\n\u001b[0;32m    117\u001b[0m     node_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresent_data(item_key)\n\u001b[1;32m--> 118\u001b[0m     node_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(node_key, ScalarNode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node_key\u001b[38;5;241m.\u001b[39mstyle):\n\u001b[0;32m    120\u001b[0m         best_style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:48\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     46\u001b[0m data_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__mro__\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_types[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml_representers:\n\u001b[1;32m---> 48\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaml_representers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_types\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_type \u001b[38;5;129;01min\u001b[39;00m data_types:\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:199\u001b[0m, in \u001b[0;36mSafeRepresenter.represent_list\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepresent_list\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m#pairs = (len(data) > 0 and isinstance(data, list))\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m#if pairs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m#            break\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m#if not pairs:\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtag:yaml.org,2002:seq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:92\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_sequence\u001b[1;34m(self, tag, sequence, flow_style)\u001b[0m\n\u001b[0;32m     90\u001b[0m best_style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sequence:\n\u001b[1;32m---> 92\u001b[0m     node_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(node_item, ScalarNode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node_item\u001b[38;5;241m.\u001b[39mstyle):\n\u001b[0;32m     94\u001b[0m         best_style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:48\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     46\u001b[0m data_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__mro__\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_types[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml_representers:\n\u001b[1;32m---> 48\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaml_representers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_types\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_type \u001b[38;5;129;01min\u001b[39;00m data_types:\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:199\u001b[0m, in \u001b[0;36mSafeRepresenter.represent_list\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepresent_list\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m#pairs = (len(data) > 0 and isinstance(data, list))\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m#if pairs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m#            break\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m#if not pairs:\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtag:yaml.org,2002:seq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:92\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_sequence\u001b[1;34m(self, tag, sequence, flow_style)\u001b[0m\n\u001b[0;32m     90\u001b[0m best_style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sequence:\n\u001b[1;32m---> 92\u001b[0m     node_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(node_item, ScalarNode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node_item\u001b[38;5;241m.\u001b[39mstyle):\n\u001b[0;32m     94\u001b[0m         best_style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:48\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     46\u001b[0m data_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__mro__\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_types[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml_representers:\n\u001b[1;32m---> 48\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaml_representers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_types\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_type \u001b[38;5;129;01min\u001b[39;00m data_types:\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:189\u001b[0m, in \u001b[0;36mSafeRepresenter.represent_float\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m value \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m value:\n\u001b[0;32m    188\u001b[0m         value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.0e\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtag:yaml.org,2002:float\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\yaml\\representer.py:79\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_scalar\u001b[1;34m(self, tag, value, style)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepresent_scalar\u001b[39m(\u001b[38;5;28mself\u001b[39m, tag, value, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m style \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m         style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_style\u001b[49m\n\u001b[0;32m     80\u001b[0m     node \u001b[38;5;241m=\u001b[39m ScalarNode(tag, value, style\u001b[38;5;241m=\u001b[39mstyle)\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malias_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_features_from_video(video_path, detection_model_path, segmentation_model_path, pose_model_path, output_yaml, label):\n",
    "    \"\"\"\n",
    "    Main function to extract features from video\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load models\n",
    "    detection_model = YOLO(detection_model_path)\n",
    "    segmentation_model = YOLO(segmentation_model_path)\n",
    "    pose_model = YOLO(pose_model_path)\n",
    "\n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Create video writer\n",
    "    output_video_path = video_path.rsplit('.', 1)[0] + '_annotated.mp4'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize video data structure\n",
    "    video_data = {\n",
    "        'video_metadata': {\n",
    "            'path': str(video_path),\n",
    "            'fps': float(fps),\n",
    "            'frame_count': total_frames,\n",
    "            'width': frame_width,\n",
    "            'height': frame_height\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    frame_count = 0\n",
    "    processing_interval = 2  # Process every nth frame\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if frame_count % processing_interval == 0:\n",
    "                # Create copy for display\n",
    "                display_frame = frame.copy()\n",
    "\n",
    "                # Process frame\n",
    "                frame_data = process_frame(frame, detection_model, segmentation_model, pose_model)\n",
    "                \n",
    "                # Add metadata to frame data\n",
    "                frame_data.update({\n",
    "                    'frame_number': frame_count,\n",
    "                    'timestamp': frame_count / fps,\n",
    "                    'label': label\n",
    "                })\n",
    "\n",
    "                # Draw visualizations\n",
    "                if frame_data['detections']:\n",
    "                    boxes = [det['bbox'] for det in frame_data['detections']]\n",
    "                    scores = [det['confidence'] for det in frame_data['detections']]\n",
    "                    class_ids = [det['class_id'] for det in frame_data['detections']]\n",
    "                    display_frame = draw_detections(display_frame, boxes, scores, class_ids, detection_model.names)\n",
    "\n",
    "                if frame_data['poses']:\n",
    "                    keypoints = [pose['keypoints'] for pose in frame_data['poses']]\n",
    "                    display_frame = draw_keypoints(display_frame, keypoints)\n",
    "\n",
    "                if frame_data['segmentations']:\n",
    "                    masks = [np.array(seg['mask']) for seg in frame_data['segmentations']]\n",
    "                    display_frame = apply_segmentation_mask(display_frame, masks)\n",
    "\n",
    "                # Add frame data to video data\n",
    "                video_data['frames'].append(frame_data)\n",
    "\n",
    "                # Display progress\n",
    "                if frame_count % 30 == 0:\n",
    "                    progress = (frame_count / total_frames) * 100\n",
    "                    print(f\"Processing: {progress:.1f}% complete\")\n",
    "\n",
    "                # Display frame\n",
    "                cv2.imshow('Processing', display_frame)\n",
    "                out.write(display_frame)\n",
    "\n",
    "                # Handle key events\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "                elif key == ord('p'):\n",
    "                    cv2.waitKey(0)\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Save features to YAML\n",
    "        try:\n",
    "            with open(output_yaml, 'w') as file:\n",
    "                yaml.dump(video_data, file, default_flow_style=False)\n",
    "            print(f\"Features saved to {output_yaml}\")\n",
    "            print(f\"Annotated video saved to {output_video_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Get video file\n",
    "    video_path = filedialog.askopenfilename(\n",
    "        title=\"Select Video File\",\n",
    "        filetypes=[(\"Video Files\", \"*.mp4;*.avi\")]\n",
    "    )\n",
    "\n",
    "    if not video_path:\n",
    "        print(\"No video file selected\")\n",
    "        exit()\n",
    "\n",
    "    # Define paths\n",
    "    detection_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m.pt'\n",
    "    segmentation_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-seg.pt'\n",
    "    pose_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-pose.pt'\n",
    "    output_yaml = r'C:\\Users\\harme\\Desktop\\video-detect-gpu\\video_features.yaml'\n",
    "    label = 1\n",
    "\n",
    "    # Run feature extraction\n",
    "    extract_features_from_video(\n",
    "        video_path,\n",
    "        detection_model_path,\n",
    "        segmentation_model_path,\n",
    "        pose_model_path,\n",
    "        output_yaml,\n",
    "        label\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model to yolov8n-weapon-detection.pt...\n",
      "Download completed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_model(url, save_path):\n",
    "    \"\"\"Download model if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(save_path):\n",
    "        print(f\"Downloading model to {save_path}...\")\n",
    "        response = requests.get(url)\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Download completed!\")\n",
    "    return save_path\n",
    "\n",
    "# Model URL and save path\n",
    "model_url = \"https://github.com/keremberke/weapon-detection-yolov8/releases/download/v1.0.0/yolov8n-weapon-detection.pt\"\n",
    "model_path = \"yolov8n-weapon-detection.pt\"\n",
    "\n",
    "# Download the model\n",
    "model_path = download_model(model_url, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "could not find MARK",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 169\u001b[0m\n\u001b[0;32m    166\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Run detection\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m \u001b[43mextract_features_from_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_yaml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 45\u001b[0m, in \u001b[0;36mextract_features_from_video\u001b[1;34m(video_path, model_path, output_yaml, label)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Load weapon detection model\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable classes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mnames)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Open video\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\ultralytics\\models\\yolo\\model.py:23\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\ultralytics\\engine\\model.py:146\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Delete super().training for accessing self.model.training\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\ultralytics\\engine\\model.py:289\u001b[0m, in \u001b[0;36mModel._load\u001b[1;34m(self, weights, task)\u001b[0m\n\u001b[0;32m    286\u001b[0m weights \u001b[38;5;241m=\u001b[39m checks\u001b[38;5;241m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(weights)\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\ultralytics\\nn\\tasks.py:900\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[1;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    899\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 900\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[0;32m    902\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\ultralytics\\nn\\tasks.py:827\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[1;34m(weight, safe_only)\u001b[0m\n\u001b[0;32m    825\u001b[0m                 ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(f, pickle_module\u001b[38;5;241m=\u001b[39msafe_pickle)\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 827\u001b[0m             ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\ultralytics\\utils\\patches.py:86\u001b[0m, in \u001b[0;36mtorch_load\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _torch_load(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\torch\\serialization.py:1495\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1493\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1494\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(\n\u001b[0;32m   1496\u001b[0m     opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args\n\u001b[0;32m   1497\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\torch\\serialization.py:1744\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1739\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1740\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1741\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1742\u001b[0m     )\n\u001b[1;32m-> 1744\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[0;32m   1746\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: could not find MARK"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "import requests\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "def download_model(url, save_path):\n",
    "    \"\"\"Download model if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(save_path):\n",
    "        print(f\"Downloading model to {save_path}...\")\n",
    "        response = requests.get(url)\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Download completed!\")\n",
    "    return save_path\n",
    "\n",
    "def draw_detections(frame, results, conf_threshold=0.3):\n",
    "    \"\"\"Draw weapon detections on frame\"\"\"\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            conf = float(box.conf[0])\n",
    "            if conf < conf_threshold:\n",
    "                continue\n",
    "\n",
    "            cls = int(box.cls[0])\n",
    "            class_name = result.names[cls]\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "\n",
    "            color = (0, 0, 255) if 'weapon' in class_name.lower() or 'gun' in class_name.lower() else (0, 255, 0)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            label = f'{class_name} {conf:.2f}'\n",
    "            cv2.putText(frame, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def extract_features_from_video(video_path, model_path, output_yaml, label):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load weapon detection model\n",
    "    model = YOLO(model_path)\n",
    "    print(\"Available classes:\", model.names)\n",
    "\n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Create video writer\n",
    "    output_video_path = video_path.rsplit('.', 1)[0] + '_weapon_detection.mp4'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    video_data = {\n",
    "        'video_metadata': {\n",
    "            'path': str(video_path),\n",
    "            'fps': float(fps),\n",
    "            'frame_count': total_frames\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    frame_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Run detection\n",
    "            results = model(frame, verbose=False)\n",
    "            \n",
    "            # Process detections\n",
    "            frame_data = {\n",
    "                'frame_number': frame_count,\n",
    "                'timestamp': frame_count / fps,\n",
    "                'detections': []\n",
    "            }\n",
    "\n",
    "            for result in results:\n",
    "                for box in result.boxes:\n",
    "                    detection = {\n",
    "                        'class': result.names[int(box.cls[0])],\n",
    "                        'confidence': float(box.conf[0]),\n",
    "                        'bbox': box.xyxy[0].cpu().numpy().tolist()\n",
    "                    }\n",
    "                    frame_data['detections'].append(detection)\n",
    "\n",
    "            # Draw detections\n",
    "            display_frame = draw_detections(frame.copy(), results)\n",
    "\n",
    "            # Add frame data\n",
    "            video_data['frames'].append(frame_data)\n",
    "\n",
    "            # Display progress\n",
    "            if frame_count % 30 == 0:\n",
    "                progress = (frame_count / total_frames) * 100\n",
    "                print(f\"Processing: {progress:.1f}% complete\")\n",
    "\n",
    "            # Display frame\n",
    "            cv2.imshow('Weapon Detection', display_frame)\n",
    "            out.write(display_frame)\n",
    "\n",
    "            # Handle key events\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('p'):\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Save features\n",
    "        try:\n",
    "            with open(output_yaml, 'w') as file:\n",
    "                yaml.dump(video_data, file, default_flow_style=False)\n",
    "            print(f\"Features saved to {output_yaml}\")\n",
    "            print(f\"Annotated video saved to {output_video_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Download the weapon detection model\n",
    "    model_url = \"https://github.com/keremberke/weapon-detection-yolov8/releases/download/v1.0.0/yolov8n-weapon-detection.pt\"\n",
    "    model_path = \"yolov8n-weapon-detection.pt\"\n",
    "    model_path = download_model(model_url, model_path)\n",
    "\n",
    "    # Get video file\n",
    "    video_path = filedialog.askopenfilename(\n",
    "        title=\"Select Video File\",\n",
    "        filetypes=[(\"Video Files\", \"*.mp4;*.avi\")]\n",
    "    )\n",
    "\n",
    "    if not video_path:\n",
    "        print(\"No video file selected\")\n",
    "        exit()\n",
    "\n",
    "    output_yaml = r'C:\\Users\\harme\\Desktop\\video-detect-gpu\\weapon_detection_features.yaml'\n",
    "    label = 1\n",
    "\n",
    "    # Run detection\n",
    "    extract_features_from_video(\n",
    "        video_path,\n",
    "        model_path,\n",
    "        output_yaml,\n",
    "        label\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 23.7ms\n",
      "Speed: 3.5ms preprocess, 23.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 29.8ms\n",
      "Speed: 2.0ms preprocess, 29.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 19.7ms\n",
      "Speed: 2.5ms preprocess, 19.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 0/66 frames\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.7ms\n",
      "Speed: 6.0ms preprocess, 20.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 22.0ms\n",
      "Speed: 2.0ms preprocess, 22.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 18.1ms\n",
      "Speed: 2.8ms preprocess, 18.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 21.5ms\n",
      "Speed: 2.2ms preprocess, 21.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 19.1ms\n",
      "Speed: 4.0ms preprocess, 19.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 29.0ms\n",
      "Speed: 1.0ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 21.7ms\n",
      "Speed: 2.0ms preprocess, 21.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 18.1ms\n",
      "Speed: 2.4ms preprocess, 18.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.5ms\n",
      "Speed: 2.0ms preprocess, 16.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 21.0ms\n",
      "Speed: 2.0ms preprocess, 21.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 26.5ms\n",
      "Speed: 3.0ms preprocess, 26.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.7ms\n",
      "Speed: 0.0ms preprocess, 20.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 25.7ms\n",
      "Speed: 0.0ms preprocess, 25.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 21.1ms\n",
      "Speed: 1.9ms preprocess, 21.1ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.8ms\n",
      "Speed: 3.0ms preprocess, 16.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 29.4ms\n",
      "Speed: 0.0ms preprocess, 29.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 20.9ms\n",
      "Speed: 3.1ms preprocess, 20.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 18.2ms\n",
      "Speed: 2.0ms preprocess, 18.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 21.8ms\n",
      "Speed: 2.1ms preprocess, 21.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 21.7ms\n",
      "Speed: 3.3ms preprocess, 21.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 29.5ms\n",
      "Speed: 2.0ms preprocess, 29.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.7ms\n",
      "Speed: 8.2ms preprocess, 20.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 21.5ms\n",
      "Speed: 0.0ms preprocess, 21.5ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 21.0ms\n",
      "Speed: 0.0ms preprocess, 21.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 21.0ms\n",
      "Speed: 2.0ms preprocess, 21.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 17.3ms\n",
      "Speed: 1.4ms preprocess, 17.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 17.3ms\n",
      "Speed: 2.0ms preprocess, 17.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 21.0ms\n",
      "Speed: 2.0ms preprocess, 21.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 14.4ms\n",
      "Speed: 2.0ms preprocess, 14.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.2ms\n",
      "Speed: 2.0ms preprocess, 16.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 32.0ms\n",
      "Speed: 0.0ms preprocess, 32.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 16.6ms\n",
      "Speed: 0.0ms preprocess, 16.6ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 30.5ms\n",
      "Speed: 3.0ms preprocess, 30.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 21.0ms\n",
      "Speed: 2.0ms preprocess, 21.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 17.5ms\n",
      "Speed: 2.1ms preprocess, 17.5ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 26.4ms\n",
      "Speed: 2.5ms preprocess, 26.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 28.4ms\n",
      "Speed: 2.5ms preprocess, 28.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 19.2ms\n",
      "Speed: 2.0ms preprocess, 19.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 28.2ms\n",
      "Speed: 2.0ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 18.1ms\n",
      "Speed: 2.0ms preprocess, 18.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 19.7ms\n",
      "Speed: 2.0ms preprocess, 19.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 23.1ms\n",
      "Speed: 2.0ms preprocess, 23.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 26.9ms\n",
      "Speed: 9.8ms preprocess, 26.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 22.8ms\n",
      "Speed: 3.0ms preprocess, 22.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 18.9ms\n",
      "Speed: 2.0ms preprocess, 18.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 16.7ms\n",
      "Speed: 2.0ms preprocess, 16.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.6ms\n",
      "Speed: 2.0ms preprocess, 16.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 33.4ms\n",
      "Speed: 1.7ms preprocess, 33.4ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 17.1ms\n",
      "Speed: 2.0ms preprocess, 17.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 23.8ms\n",
      "Speed: 5.3ms preprocess, 23.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 21.2ms\n",
      "Speed: 1.9ms preprocess, 21.2ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 18.4ms\n",
      "Speed: 3.0ms preprocess, 18.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 18.4ms\n",
      "Speed: 3.0ms preprocess, 18.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 28.7ms\n",
      "Speed: 2.0ms preprocess, 28.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 16.7ms\n",
      "Speed: 2.0ms preprocess, 16.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 18.9ms\n",
      "Speed: 3.0ms preprocess, 18.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 21.4ms\n",
      "Speed: 3.0ms preprocess, 21.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 26.7ms\n",
      "Speed: 0.0ms preprocess, 26.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 22.2ms\n",
      "Speed: 2.3ms preprocess, 22.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 22.0ms\n",
      "Speed: 3.0ms preprocess, 22.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.0ms\n",
      "Speed: 2.1ms preprocess, 20.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 23.9ms\n",
      "Speed: 2.0ms preprocess, 23.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 20.3ms\n",
      "Speed: 2.0ms preprocess, 20.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.7ms\n",
      "Speed: 2.8ms preprocess, 16.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.9ms\n",
      "Speed: 2.0ms preprocess, 20.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 18.4ms\n",
      "Speed: 2.0ms preprocess, 18.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.6ms\n",
      "Speed: 0.0ms preprocess, 20.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 28.4ms\n",
      "Speed: 0.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 21.3ms\n",
      "Speed: 0.0ms preprocess, 21.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 21.3ms\n",
      "Speed: 0.0ms preprocess, 21.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 21.7ms\n",
      "Speed: 2.0ms preprocess, 21.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 22.4ms\n",
      "Speed: 2.0ms preprocess, 22.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.1ms\n",
      "Speed: 2.0ms preprocess, 16.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 23.2ms\n",
      "Speed: 0.0ms preprocess, 23.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 12.6ms\n",
      "Speed: 4.2ms preprocess, 12.6ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.2ms\n",
      "Speed: 1.0ms preprocess, 20.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.9ms\n",
      "Speed: 2.5ms preprocess, 16.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.4ms\n",
      "Speed: 1.0ms preprocess, 20.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 22.8ms\n",
      "Speed: 2.0ms preprocess, 22.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 23.1ms\n",
      "Speed: 0.0ms preprocess, 23.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 22.2ms\n",
      "Speed: 1.2ms preprocess, 22.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 22.7ms\n",
      "Speed: 0.0ms preprocess, 22.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 1 remote, 21.7ms\n",
      "Speed: 0.0ms preprocess, 21.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 1 remote, 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 30.0ms\n",
      "Speed: 3.0ms preprocess, 30.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 30/66 frames\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.9ms\n",
      "Speed: 0.0ms preprocess, 20.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 21.2ms\n",
      "Speed: 2.0ms preprocess, 21.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 18.2ms\n",
      "Speed: 2.0ms preprocess, 18.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.7ms\n",
      "Speed: 2.0ms preprocess, 20.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.2ms\n",
      "Speed: 2.0ms preprocess, 20.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 20.5ms\n",
      "Speed: 2.0ms preprocess, 20.5ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.9ms\n",
      "Speed: 2.0ms preprocess, 20.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.6ms\n",
      "Speed: 1.5ms preprocess, 20.6ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 11.5ms\n",
      "Speed: 2.0ms preprocess, 11.5ms inference, 9.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 14.9ms\n",
      "Speed: 2.0ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.6ms\n",
      "Speed: 2.0ms preprocess, 16.6ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 12.1ms\n",
      "Speed: 0.8ms preprocess, 12.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.1ms\n",
      "Speed: 2.3ms preprocess, 16.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 24.5ms\n",
      "Speed: 0.9ms preprocess, 24.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 33.4ms\n",
      "Speed: 2.0ms preprocess, 33.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 18.5ms\n",
      "Speed: 2.0ms preprocess, 18.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 28.5ms\n",
      "Speed: 0.0ms preprocess, 28.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 21.1ms\n",
      "Speed: 2.0ms preprocess, 21.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 23.2ms\n",
      "Speed: 0.5ms preprocess, 23.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 23.9ms\n",
      "Speed: 0.9ms preprocess, 23.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 24.4ms\n",
      "Speed: 2.3ms preprocess, 24.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.8ms\n",
      "Speed: 2.2ms preprocess, 16.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 24.6ms\n",
      "Speed: 0.0ms preprocess, 24.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 15.8ms\n",
      "Speed: 2.8ms preprocess, 15.8ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 18.3ms\n",
      "Speed: 8.1ms preprocess, 18.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 30.4ms\n",
      "Speed: 2.7ms preprocess, 30.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 18.6ms\n",
      "Speed: 0.0ms preprocess, 18.6ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.3ms\n",
      "Speed: 3.0ms preprocess, 16.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 19.4ms\n",
      "Speed: 0.0ms preprocess, 19.4ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.1ms\n",
      "Speed: 3.0ms preprocess, 16.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 17.8ms\n",
      "Speed: 2.0ms preprocess, 17.8ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 23.1ms\n",
      "Speed: 2.0ms preprocess, 23.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 17.8ms\n",
      "Speed: 1.7ms preprocess, 17.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 27.6ms\n",
      "Speed: 5.6ms preprocess, 27.6ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 0.4ms\n",
      "Speed: 2.0ms preprocess, 0.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.4ms\n",
      "Speed: 2.4ms preprocess, 20.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 25.1ms\n",
      "Speed: 0.0ms preprocess, 25.1ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 24.4ms\n",
      "Speed: 3.6ms preprocess, 24.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 46.7ms\n",
      "Speed: 0.0ms preprocess, 46.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 23.7ms\n",
      "Speed: 0.0ms preprocess, 23.7ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 15.5ms\n",
      "Speed: 5.0ms preprocess, 15.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.1ms\n",
      "Speed: 3.3ms preprocess, 16.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 29.2ms\n",
      "Speed: 0.4ms preprocess, 29.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 22.0ms\n",
      "Speed: 0.0ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 19.3ms\n",
      "Speed: 3.1ms preprocess, 19.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 18.4ms\n",
      "Speed: 2.0ms preprocess, 18.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.2ms\n",
      "Speed: 2.2ms preprocess, 20.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 23.1ms\n",
      "Speed: 1.8ms preprocess, 23.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 19.5ms\n",
      "Speed: 0.0ms preprocess, 19.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 22.9ms\n",
      "Speed: 2.0ms preprocess, 22.9ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 17.5ms\n",
      "Speed: 2.0ms preprocess, 17.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.3ms\n",
      "Speed: 2.2ms preprocess, 16.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.4ms\n",
      "Speed: 2.0ms preprocess, 20.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 21.4ms\n",
      "Speed: 0.0ms preprocess, 21.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.6ms\n",
      "Speed: 2.0ms preprocess, 16.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.6ms\n",
      "Speed: 2.0ms preprocess, 20.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 19.3ms\n",
      "Speed: 2.2ms preprocess, 19.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 19.8ms\n",
      "Speed: 0.0ms preprocess, 19.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 22.7ms\n",
      "Speed: 0.0ms preprocess, 22.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 27.8ms\n",
      "Speed: 3.1ms preprocess, 27.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 19.9ms\n",
      "Speed: 0.0ms preprocess, 19.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 dining table, 20.1ms\n",
      "Speed: 0.0ms preprocess, 20.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 dining table, 23.6ms\n",
      "Speed: 1.9ms preprocess, 23.6ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 21.3ms\n",
      "Speed: 2.9ms preprocess, 21.3ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 27.7ms\n",
      "Speed: 0.0ms preprocess, 27.7ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.9ms\n",
      "Speed: 2.0ms preprocess, 20.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 17.5ms\n",
      "Speed: 1.5ms preprocess, 17.5ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 12.1ms\n",
      "Speed: 2.2ms preprocess, 12.1ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 19.6ms\n",
      "Speed: 2.7ms preprocess, 19.6ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 27.9ms\n",
      "Speed: 2.0ms preprocess, 27.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 24.5ms\n",
      "Speed: 0.0ms preprocess, 24.5ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 25.5ms\n",
      "Speed: 8.4ms preprocess, 25.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 17.8ms\n",
      "Speed: 2.0ms preprocess, 17.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 25.0ms\n",
      "Speed: 3.0ms preprocess, 25.0ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 33.0ms\n",
      "Speed: 2.0ms preprocess, 33.0ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 21.3ms\n",
      "Speed: 2.5ms preprocess, 21.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 23.8ms\n",
      "Speed: 0.0ms preprocess, 23.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 17.8ms\n",
      "Speed: 3.3ms preprocess, 17.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.2ms\n",
      "Speed: 0.1ms preprocess, 20.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 23.5ms\n",
      "Speed: 5.0ms preprocess, 23.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 19.3ms\n",
      "Speed: 3.6ms preprocess, 19.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 19.4ms\n",
      "Speed: 2.0ms preprocess, 19.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 10.8ms\n",
      "Speed: 2.0ms preprocess, 10.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 60/66 frames\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 19.3ms\n",
      "Speed: 3.0ms preprocess, 19.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 34.6ms\n",
      "Speed: 0.0ms preprocess, 34.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 15.7ms\n",
      "Speed: 2.0ms preprocess, 15.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 6.4ms\n",
      "Speed: 2.0ms preprocess, 6.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 14.6ms\n",
      "Speed: 1.7ms preprocess, 14.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 16.6ms\n",
      "Speed: 2.0ms preprocess, 16.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 20.8ms\n",
      "Speed: 3.3ms preprocess, 20.8ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 17.1ms\n",
      "Speed: 2.0ms preprocess, 17.1ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 18.4ms\n",
      "Speed: 0.0ms preprocess, 18.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 dining table, 6.7ms\n",
      "Speed: 2.0ms preprocess, 6.7ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 12.5ms\n",
      "Speed: 1.9ms preprocess, 12.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 tie, 2 chairs, 1 dining table, 18.1ms\n",
      "Speed: 2.6ms preprocess, 18.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 tie, 2 chairs, 1 dining table, 21.6ms\n",
      "Speed: 2.0ms preprocess, 21.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 12.6ms\n",
      "Speed: 2.0ms preprocess, 12.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Feature extraction complete!\n",
      "Features saved to: C:\\Users\\harme\\Desktop\\video-detect-gpu\\violence_features.yaml\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from tkinter import Tk, filedialog\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class ViolenceFeatureExtractor:\n",
    "    def __init__(self, detection_model_path, segmentation_model_path, pose_model_path):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.detection_model = YOLO(detection_model_path)\n",
    "        self.segmentation_model = YOLO(segmentation_model_path)\n",
    "        self.pose_model = YOLO(pose_model_path)\n",
    "        self.violence_objects = ['knife', 'gun', 'baseball bat', 'stick']\n",
    "\n",
    "    def calculate_motion_features(self, prev_poses, current_poses):\n",
    "        if not isinstance(prev_poses, list) or not isinstance(current_poses, list) or not prev_poses or not current_poses:\n",
    "            return {\n",
    "                'average_speed': 0,\n",
    "                'motion_intensity': 0,\n",
    "                'sudden_movements': 0\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            speeds = []\n",
    "            for prev_pose, curr_pose in zip(prev_poses, current_poses):\n",
    "                if prev_pose is not None and curr_pose is not None:\n",
    "                    displacement = np.linalg.norm(curr_pose - prev_pose, axis=1)\n",
    "                    speed = np.mean(displacement)\n",
    "                    speeds.append(speed)\n",
    "\n",
    "            if not speeds:\n",
    "                return {\n",
    "                    'average_speed': 0,\n",
    "                    'motion_intensity': 0,\n",
    "                    'sudden_movements': 0\n",
    "                }\n",
    "\n",
    "            average_speed = np.mean(speeds)\n",
    "            motion_intensity = np.std(speeds)\n",
    "            sudden_movements = np.sum(np.array(speeds) > np.mean(speeds) + 2 * np.std(speeds))\n",
    "\n",
    "            return {\n",
    "                'average_speed': float(average_speed),\n",
    "                'motion_intensity': float(motion_intensity),\n",
    "                'sudden_movements': int(sudden_movements)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in calculate_motion_features: {e}\")\n",
    "            return {\n",
    "                'average_speed': 0,\n",
    "                'motion_intensity': 0,\n",
    "                'sudden_movements': 0\n",
    "            }\n",
    "\n",
    "    def calculate_interaction_features(self, poses, boxes):\n",
    "        try:\n",
    "            if len(poses) == 0 or len(boxes) == 0:\n",
    "                return {\n",
    "                    'proximity': 0,\n",
    "                    'interaction_intensity': 0,\n",
    "                    'group_density': 0\n",
    "                }\n",
    "\n",
    "            # Convert to numpy arrays if they aren't already\n",
    "            poses_array = np.array(poses)\n",
    "            boxes_array = np.array(boxes)\n",
    "\n",
    "            # Calculate distances between all pairs of people\n",
    "            distances = cdist(boxes_array[:, :2], boxes_array[:, :2])\n",
    "            \n",
    "            # Calculate proximity (inverse of average distance)\n",
    "            proximity = 1 / (np.mean(distances) + 1e-6)\n",
    "            \n",
    "            # Calculate interaction intensity\n",
    "            poses_flat = poses_array.reshape(poses_array.shape[0], -1)\n",
    "            pose_distances = cdist(poses_flat, poses_flat)\n",
    "            interaction_intensity = np.mean(1 / (pose_distances + 1e-6))\n",
    "            \n",
    "            # Calculate group density\n",
    "            if len(boxes_array) > 1:\n",
    "                hull = cv2.convexHull(boxes_array[:, :2].astype(np.float32))\n",
    "                area = cv2.contourArea(hull)\n",
    "                group_density = len(boxes_array) / (area + 1e-6)\n",
    "            else:\n",
    "                group_density = 0\n",
    "\n",
    "            return {\n",
    "                'proximity': float(proximity),\n",
    "                'interaction_intensity': float(interaction_intensity),\n",
    "                'group_density': float(group_density)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in calculate_interaction_features: {e}\")\n",
    "            return {\n",
    "                'proximity': 0,\n",
    "                'interaction_intensity': 0,\n",
    "                'group_density': 0\n",
    "            }\n",
    "\n",
    "    def extract_violence_features(self, frame, prev_frame_data=None):\n",
    "        try:\n",
    "            # Run models\n",
    "            det_results = self.detection_model(frame)\n",
    "            seg_results = self.segmentation_model(frame)\n",
    "            pose_results = self.pose_model(frame)\n",
    "\n",
    "            # Initialize feature dictionary\n",
    "            features = {\n",
    "                'object_features': {},\n",
    "                'pose_features': {},\n",
    "                'motion_features': {},\n",
    "                'interaction_features': {},\n",
    "                'context_features': {}\n",
    "            }\n",
    "\n",
    "            # Extract object detection features\n",
    "            objects_detected = []\n",
    "            violence_objects_count = 0\n",
    "            person_boxes = []\n",
    "            \n",
    "            for result in det_results:\n",
    "                for box in result.boxes:\n",
    "                    cls = result.names[int(box.cls[0])]\n",
    "                    conf = float(box.conf[0])\n",
    "                    \n",
    "                    if cls in self.violence_objects:\n",
    "                        violence_objects_count += 1\n",
    "                    \n",
    "                    if cls == 'person':\n",
    "                        person_boxes.append(box.xyxy[0].cpu().numpy())\n",
    "                    \n",
    "                    objects_detected.append({\n",
    "                        'class': cls,\n",
    "                        'confidence': conf,\n",
    "                        'box': box.xyxy[0].cpu().numpy().tolist()\n",
    "                    })\n",
    "\n",
    "            features['object_features'] = {\n",
    "                'total_objects': len(objects_detected),\n",
    "                'violence_objects': violence_objects_count,\n",
    "                'person_count': len(person_boxes)\n",
    "            }\n",
    "\n",
    "            # Extract pose features\n",
    "            poses_list = []\n",
    "            pose_confidences = []\n",
    "            \n",
    "            for result in pose_results:\n",
    "                if result.keypoints is not None:\n",
    "                    for keypoints in result.keypoints:\n",
    "                        poses_list.append(keypoints.data[0].cpu().numpy())\n",
    "                        pose_confidences.extend(keypoints.data[0, :, 2].cpu().numpy())\n",
    "\n",
    "            if poses_list:\n",
    "                poses_array = np.array(poses_list)\n",
    "                features['pose_features'] = {\n",
    "                    'pose_count': len(poses_list),\n",
    "                    'average_confidence': float(np.mean(pose_confidences)),\n",
    "                    'pose_variance': float(np.var(poses_array.reshape(poses_array.shape[0], -1), axis=1).mean())\n",
    "                }\n",
    "            else:\n",
    "                features['pose_features'] = {\n",
    "                    'pose_count': 0,\n",
    "                    'average_confidence': 0,\n",
    "                    'pose_variance': 0\n",
    "                }\n",
    "\n",
    "            # Calculate motion features\n",
    "            if prev_frame_data and 'poses' in prev_frame_data and prev_frame_data['poses'] is not None:\n",
    "                features['motion_features'] = self.calculate_motion_features(\n",
    "                    prev_frame_data['poses'], poses_list)\n",
    "            else:\n",
    "                features['motion_features'] = {\n",
    "                    'average_speed': 0,\n",
    "                    'motion_intensity': 0,\n",
    "                    'sudden_movements': 0\n",
    "                }\n",
    "\n",
    "            # Calculate interaction features\n",
    "            if len(poses_list) > 0 and len(person_boxes) > 0:\n",
    "                features['interaction_features'] = self.calculate_interaction_features(\n",
    "                    poses_list, person_boxes)\n",
    "            else:\n",
    "                features['interaction_features'] = {\n",
    "                    'proximity': 0,\n",
    "                    'interaction_intensity': 0,\n",
    "                    'group_density': 0\n",
    "                }\n",
    "\n",
    "            return features, poses_list\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in extract_violence_features: {e}\")\n",
    "            return {\n",
    "                'object_features': {'total_objects': 0, 'violence_objects': 0, 'person_count': 0},\n",
    "                'pose_features': {'pose_count': 0, 'average_confidence': 0, 'pose_variance': 0},\n",
    "                'motion_features': {'average_speed': 0, 'motion_intensity': 0, 'sudden_movements': 0},\n",
    "                'interaction_features': {'proximity': 0, 'interaction_intensity': 0, 'group_density': 0},\n",
    "                'context_features': {}\n",
    "            }, []\n",
    "\n",
    "def process_video_for_violence_detection(video_path, extractor, output_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    video_data = {\n",
    "        'metadata': {\n",
    "            'path': video_path,\n",
    "            'fps': fps,\n",
    "            'frame_count': frame_count\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    prev_frame_data = None\n",
    "    frame_idx = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Extract features\n",
    "            features, poses = extractor.extract_violence_features(frame, prev_frame_data)\n",
    "            \n",
    "            # Store frame data\n",
    "            frame_data = {\n",
    "                'frame_index': frame_idx,\n",
    "                'timestamp': frame_idx / fps,\n",
    "                'features': features,\n",
    "                'poses': poses\n",
    "            }\n",
    "            \n",
    "            video_data['frames'].append(frame_data)\n",
    "            prev_frame_data = frame_data\n",
    "            \n",
    "            # Show progress\n",
    "            if frame_idx % 30 == 0:\n",
    "                print(f\"Processed {frame_idx}/{frame_count} frames\")\n",
    "            \n",
    "            frame_idx += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video: {e}\")\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "        # Save features to file\n",
    "        try:\n",
    "            with open(output_path, 'w') as f:\n",
    "                yaml.dump(video_data, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving features: {e}\")\n",
    "\n",
    "    return video_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Get video file\n",
    "    video_path = filedialog.askopenfilename(\n",
    "        title=\"Select Video File\",\n",
    "        filetypes=[(\"Video Files\", \"*.mp4;*.avi\")]\n",
    "    )\n",
    "\n",
    "    if not video_path:\n",
    "        print(\"No video file selected\")\n",
    "        exit()\n",
    "\n",
    "    # Define model paths\n",
    "    detection_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m.pt'\n",
    "    segmentation_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-seg.pt'\n",
    "    pose_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-pose.pt'\n",
    "    \n",
    "    # Initialize feature extractor\n",
    "    extractor = ViolenceFeatureExtractor(\n",
    "        detection_model_path,\n",
    "        segmentation_model_path,\n",
    "        pose_model_path\n",
    "    )\n",
    "\n",
    "    # Process video\n",
    "    output_path = r'C:\\Users\\harme\\Desktop\\video-detect-gpu\\violence_features.yaml'\n",
    "    video_data = process_video_for_violence_detection(video_path, extractor, output_path)\n",
    "\n",
    "    print(\"Feature extraction complete!\")\n",
    "    print(f\"Features saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Processing: 0.0% complete\n",
      "Processing: 26.3% complete\n",
      "Processing: 52.6% complete\n",
      "Paused - Press 'p' to resume\n",
      "Processing: 78.9% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000002515BB15DC0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\harme\\Desktop\\video-detect-gpu\\myenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to: C:\\Users\\harme\\Desktop\\video-detect-gpu\\violence_features.yaml\n",
      "Analyzed video saved to: C:/Users/harme/Desktop/video-detect-gpu/V_5_analyzed.mp4\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from tkinter import Tk, filedialog\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class ViolenceFeatureExtractor:\n",
    "    def __init__(self, detection_model_path, segmentation_model_path, pose_model_path):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load models\n",
    "        self.detection_model = YOLO(detection_model_path)\n",
    "        self.segmentation_model = YOLO(segmentation_model_path)\n",
    "        self.pose_model = YOLO(pose_model_path)\n",
    "        \n",
    "        # Define violence-related objects\n",
    "        self.violence_objects = ['knife', 'gun', 'baseball bat', 'stick']\n",
    "        \n",
    "        # Define colors for visualization\n",
    "        self.colors = {\n",
    "            'violence': (0, 0, 255),    # Red\n",
    "            'person': (0, 255, 0),      # Green\n",
    "            'other': (255, 0, 0),       # Blue\n",
    "            'keypoint': (255, 255, 0),  # Yellow\n",
    "            'connection': (0, 255, 255)  # Cyan\n",
    "        }\n",
    "\n",
    "    def calculate_motion_features(self, prev_poses, current_poses):\n",
    "        \"\"\"Calculate motion features between consecutive frames\"\"\"\n",
    "        try:\n",
    "            if not prev_poses or not current_poses:\n",
    "                return {\n",
    "                    'average_speed': 0,\n",
    "                    'motion_intensity': 0,\n",
    "                    'sudden_movements': 0\n",
    "                }\n",
    "\n",
    "            # Convert poses to numpy arrays if they aren't already\n",
    "            prev_poses = np.array(prev_poses)\n",
    "            current_poses = np.array(current_poses)\n",
    "\n",
    "            # Calculate displacement between frames\n",
    "            if prev_poses.shape == current_poses.shape:\n",
    "                displacement = np.linalg.norm(current_poses - prev_poses, axis=2)\n",
    "                average_speed = np.mean(displacement)\n",
    "                motion_intensity = np.std(displacement)\n",
    "                sudden_movements = np.sum(displacement > np.mean(displacement) + 2 * np.std(displacement))\n",
    "\n",
    "                return {\n",
    "                    'average_speed': float(average_speed),\n",
    "                    'motion_intensity': float(motion_intensity),\n",
    "                    'sudden_movements': int(sudden_movements)\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'average_speed': 0,\n",
    "                    'motion_intensity': 0,\n",
    "                    'sudden_movements': 0\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in calculate_motion_features: {e}\")\n",
    "            return {\n",
    "                'average_speed': 0,\n",
    "                'motion_intensity': 0,\n",
    "                'sudden_movements': 0\n",
    "            }\n",
    "\n",
    "    def draw_detections(self, frame, det_results, pose_results, seg_results):\n",
    "        \"\"\"Draw all detections on the frame\"\"\"\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        # Draw segmentation masks first (if any)\n",
    "        if seg_results:\n",
    "            for result in seg_results:\n",
    "                if result.masks is not None:\n",
    "                    for mask in result.masks.data:\n",
    "                        try:\n",
    "                            # Get mask as numpy array\n",
    "                            mask_np = mask.cpu().numpy()\n",
    "                            \n",
    "                            # Resize mask to match frame size\n",
    "                            mask_np = cv2.resize(mask_np, (frame.shape[1], frame.shape[0]))\n",
    "                            \n",
    "                            # Create binary mask\n",
    "                            mask_binary = (mask_np > 0.5).astype(np.uint8) * 255\n",
    "                            \n",
    "                            # Create colored mask\n",
    "                            colored_mask = np.zeros_like(frame)\n",
    "                            colored_mask[mask_binary > 0] = [0, 0, 255]  # Red color for mask\n",
    "                            \n",
    "                            # Apply mask\n",
    "                            display_frame = cv2.addWeighted(display_frame, 1, colored_mask, 0.3, 0)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing mask: {e}\")\n",
    "                            continue\n",
    "\n",
    "        # Draw object detections\n",
    "        for result in det_results:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                try:\n",
    "                    # Get box coordinates\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "                    cls = result.names[int(box.cls[0])]\n",
    "                    conf = float(box.conf[0])\n",
    "\n",
    "                    # Choose color based on class\n",
    "                    if cls in self.violence_objects:\n",
    "                        color = self.colors['violence']\n",
    "                    elif cls == 'person':\n",
    "                        color = self.colors['person']\n",
    "                    else:\n",
    "                        color = self.colors['other']\n",
    "\n",
    "                    # Draw box and label\n",
    "                    cv2.rectangle(display_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    label = f'{cls} {conf:.2f}'\n",
    "                    \n",
    "                    # Add background to text for better visibility\n",
    "                    (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "                    cv2.rectangle(display_frame, (x1, y1-text_h-5), (x1+text_w, y1), color, -1)\n",
    "                    cv2.putText(display_frame, label, (x1, y1-5), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error drawing detection box: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Draw pose keypoints and connections\n",
    "        if pose_results:\n",
    "            for result in pose_results:\n",
    "                if result.keypoints is not None:\n",
    "                    for kpts in result.keypoints:\n",
    "                        try:\n",
    "                            # Get keypoints data\n",
    "                            keypoints_data = kpts.data[0].cpu().numpy()\n",
    "                            \n",
    "                            # Draw keypoints\n",
    "                            for keypoint in keypoints_data:\n",
    "                                x, y, conf = keypoint\n",
    "                                if conf > 0.5:\n",
    "                                    cv2.circle(display_frame, \n",
    "                                             (int(float(x)), int(float(y))), \n",
    "                                             4, self.colors['keypoint'], -1)\n",
    "\n",
    "                            # Draw connections\n",
    "                            connections = [(5,7), (7,9), (6,8), (8,10), (5,6), \n",
    "                                         (11,13), (13,15), (12,14), (14,16), (11,12)]\n",
    "                            for connection in connections:\n",
    "                                pt1 = keypoints_data[connection[0]]\n",
    "                                pt2 = keypoints_data[connection[1]]\n",
    "                                \n",
    "                                if pt1[2] > 0.5 and pt2[2] > 0.5:\n",
    "                                    cv2.line(display_frame, \n",
    "                                           (int(float(pt1[0])), int(float(pt1[1]))),\n",
    "                                           (int(float(pt2[0])), int(float(pt2[1]))),\n",
    "                                           self.colors['connection'], 2)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error drawing pose: {e}\")\n",
    "                            continue\n",
    "\n",
    "        # Add frame information\n",
    "        cv2.putText(display_frame, \"Press 'q' to quit, 'p' to pause/resume\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        return display_frame\n",
    "    def extract_features(self, frame, prev_frame_data=None):\n",
    "        \"\"\"Extract all features from a frame\"\"\"\n",
    "        try:\n",
    "            # Run models\n",
    "            det_results = self.detection_model(frame, verbose=False)\n",
    "            seg_results = self.segmentation_model(frame, verbose=False)\n",
    "            pose_results = self.pose_model(frame, verbose=False)\n",
    "\n",
    "            # Draw detections\n",
    "            annotated_frame = self.draw_detections(frame, det_results, pose_results, seg_results)\n",
    "\n",
    "            # Initialize features dictionary\n",
    "            features = {\n",
    "                'objects': [],\n",
    "                'poses': [],\n",
    "                'segmentation': [],\n",
    "                'motion': {},\n",
    "                'violence_indicators': {\n",
    "                    'weapon_present': False,\n",
    "                    'rapid_motion': False,\n",
    "                    'close_interaction': False,\n",
    "                    'aggressive_pose': False\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Process detections\n",
    "            for result in det_results:\n",
    "                for box in result.boxes:\n",
    "                    try:\n",
    "                        cls = result.names[int(box.cls[0])]\n",
    "                        conf = float(box.conf[0])\n",
    "                        box_coords = box.xyxy[0].cpu().numpy().tolist()\n",
    "                        \n",
    "                        features['objects'].append({\n",
    "                            'class': cls,\n",
    "                            'confidence': conf,\n",
    "                            'box': box_coords\n",
    "                        })\n",
    "                        \n",
    "                        if cls in self.violence_objects:\n",
    "                            features['violence_indicators']['weapon_present'] = True\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing detection: {e}\")\n",
    "                        continue\n",
    "\n",
    "            # Process poses\n",
    "            if pose_results:\n",
    "                for result in pose_results:\n",
    "                    if result.keypoints is not None:\n",
    "                        for kpts in result.keypoints:\n",
    "                            try:\n",
    "                                pose_data = kpts.data[0].cpu().numpy().tolist()\n",
    "                                features['poses'].append(pose_data)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing pose: {e}\")\n",
    "                                continue\n",
    "\n",
    "            # Process segmentation\n",
    "            if seg_results:\n",
    "                for result in seg_results:\n",
    "                    if result.masks is not None:\n",
    "                        for mask in result.masks.data:\n",
    "                            try:\n",
    "                                mask_np = mask.cpu().numpy()\n",
    "                                features['segmentation'].append(np.sum(mask_np > 0.5))\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing segmentation: {e}\")\n",
    "                                continue\n",
    "\n",
    "            # Calculate motion features if previous frame exists\n",
    "            if prev_frame_data and 'poses' in prev_frame_data:\n",
    "                motion_features = self.calculate_motion_features(\n",
    "                    prev_frame_data['poses'], features['poses'])\n",
    "                features['motion'] = motion_features\n",
    "                \n",
    "                # Check for rapid motion\n",
    "                if motion_features.get('average_speed', 0) > 10:  # Threshold can be adjusted\n",
    "                    features['violence_indicators']['rapid_motion'] = True\n",
    "\n",
    "            return features, annotated_frame\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {e}\")\n",
    "            return None, frame\n",
    "\n",
    "def process_video(video_path, extractor, output_path):\n",
    "    \"\"\"Process video and extract features\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create video writer\n",
    "    output_video_path = video_path.rsplit('.', 1)[0] + '_analyzed.mp4'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize data storage\n",
    "    video_data = {\n",
    "        'metadata': {\n",
    "            'path': video_path,\n",
    "            'fps': fps,\n",
    "            'frame_count': frame_count,\n",
    "            'width': frame_width,\n",
    "            'height': frame_height\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    frame_idx = 0\n",
    "    prev_frame_data = None\n",
    "    paused = False\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            if not paused:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # Extract features and get annotated frame\n",
    "                features, annotated_frame = extractor.extract_features(frame, prev_frame_data)\n",
    "                \n",
    "                if features is not None:\n",
    "                    # Store frame data\n",
    "                    frame_data = {\n",
    "                        'frame_index': frame_idx,\n",
    "                        'timestamp': frame_idx / fps,\n",
    "                        'features': features\n",
    "                    }\n",
    "                    \n",
    "                    video_data['frames'].append(frame_data)\n",
    "                    prev_frame_data = features\n",
    "\n",
    "                    # Write frame to output video\n",
    "                    out.write(annotated_frame)\n",
    "\n",
    "                    # Show progress\n",
    "                    if frame_idx % 30 == 0:\n",
    "                        progress = (frame_idx / frame_count) * 100\n",
    "                        print(f\"Processing: {progress:.1f}% complete\")\n",
    "\n",
    "                    frame_idx += 1\n",
    "\n",
    "                # Display frame\n",
    "                cv2.imshow('Violence Detection Analysis', annotated_frame)\n",
    "\n",
    "            # Handle key events\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('p'):\n",
    "                paused = not paused\n",
    "                if paused:\n",
    "                    print(\"Paused - Press 'p' to resume\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Save features to YAML\n",
    "        try:\n",
    "            with open(output_path, 'w') as f:\n",
    "                yaml.dump(video_data, f, default_flow_style=False)\n",
    "            print(f\"Features saved to: {output_path}\")\n",
    "            print(f\"Analyzed video saved to: {output_video_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {e}\")\n",
    "\n",
    "    return video_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Get video file\n",
    "    video_path = filedialog.askopenfilename(\n",
    "        title=\"Select Video File\",\n",
    "        filetypes=[(\"Video Files\", \"*.mp4;*.avi\")]\n",
    "    )\n",
    "\n",
    "    if not video_path:\n",
    "        print(\"No video file selected\")\n",
    "        exit()\n",
    "\n",
    "    # Define model paths\n",
    "    detection_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m.pt'\n",
    "    segmentation_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-seg.pt'\n",
    "    pose_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-pose.pt'\n",
    "    \n",
    "    # Initialize feature extractor\n",
    "    extractor = ViolenceFeatureExtractor(\n",
    "        detection_model_path,\n",
    "        segmentation_model_path,\n",
    "        pose_model_path\n",
    "    )\n",
    "\n",
    "    # Process video\n",
    "    output_path = r'C:\\Users\\harme\\Desktop\\video-detect-gpu\\violence_features.yaml'\n",
    "    video_data = process_video(video_path, extractor, output_path)\n",
    "\n",
    "    print(\"Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU Information:\n",
      "GPU Device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "CUDA Version: 11.8\n",
      "Total GPU Memory: 4.29 GB\n",
      "Available Memory: 0.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harme\\AppData\\Local\\Temp\\ipykernel_5124\\3578002510.py:225: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Error in feature extraction: WARNING  torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 810, 1440) is incompatible.\n",
      "Features saved to: C:\\Users\\harme\\Desktop\\video-detect-gpu\\violence_features.yaml\n",
      "Analyzed video saved to: C:/Users/harme/Desktop/video-detect-gpu/NV_1_analyzed.mp4\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "import torch.cuda\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tkinter import Tk, filedialog\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class ViolenceFeatureExtractor:\n",
    "    def __init__(self, detection_model_path, segmentation_model_path, pose_model_path):\n",
    "        # Initialize device and GPU settings\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.print_gpu_info()\n",
    "        \n",
    "        # GPU optimization settings\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "        \n",
    "        # Load models and move to GPU\n",
    "        self.detection_model = YOLO(detection_model_path).to(self.device)\n",
    "        self.segmentation_model = YOLO(segmentation_model_path).to(self.device)\n",
    "        self.pose_model = YOLO(pose_model_path).to(self.device)\n",
    "        \n",
    "        # Define violence-related objects\n",
    "        self.violence_objects = ['knife', 'gun', 'baseball bat', 'stick']\n",
    "        \n",
    "        # Define colors for visualization\n",
    "        self.colors = {\n",
    "            'violence': (0, 0, 255),    # Red\n",
    "            'person': (0, 255, 0),      # Green\n",
    "            'other': (255, 0, 0),       # Blue\n",
    "            'keypoint': (255, 255, 0),  # Yellow\n",
    "            'connection': (0, 255, 255)  # Cyan\n",
    "        }\n",
    "        \n",
    "        # Performance settings\n",
    "        self.frame_skip = 2\n",
    "        self.batch_size = 4\n",
    "        self.scale_factor = 0.75\n",
    "\n",
    "    def print_gpu_info(self):\n",
    "        \"\"\"Print GPU information\"\"\"\n",
    "        print(\"\\nGPU Information:\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "            print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "            print(f\"Available Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "        else:\n",
    "            print(\"No GPU available. Using CPU.\")\n",
    "\n",
    "    def preprocess_frame(self, frame):\n",
    "        \"\"\"Preprocess frame for model input\"\"\"\n",
    "        # Resize frame for faster processing\n",
    "        if self.scale_factor != 1.0:\n",
    "            width = int(frame.shape[1] * self.scale_factor)\n",
    "            height = int(frame.shape[0] * self.scale_factor)\n",
    "            frame = cv2.resize(frame, (width, height))\n",
    "        \n",
    "        # Convert to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Normalize\n",
    "        frame_normalized = frame_rgb.astype(np.float32) / 255.0\n",
    "        \n",
    "        return frame_normalized\n",
    "\n",
    "    def calculate_motion_features(self, prev_poses, current_poses):\n",
    "        \"\"\"Calculate motion features between consecutive frames\"\"\"\n",
    "        try:\n",
    "            if not prev_poses or not current_poses:\n",
    "                return {\n",
    "                    'average_speed': 0,\n",
    "                    'motion_intensity': 0,\n",
    "                    'sudden_movements': 0\n",
    "                }\n",
    "\n",
    "            # Convert poses to numpy arrays\n",
    "            prev_poses = np.array(prev_poses)\n",
    "            current_poses = np.array(current_poses)\n",
    "\n",
    "            if prev_poses.shape == current_poses.shape:\n",
    "                # Calculate displacement\n",
    "                displacement = np.linalg.norm(current_poses - prev_poses, axis=2)\n",
    "                average_speed = np.mean(displacement)\n",
    "                motion_intensity = np.std(displacement)\n",
    "                sudden_movements = np.sum(displacement > np.mean(displacement) + 2 * np.std(displacement))\n",
    "\n",
    "                return {\n",
    "                    'average_speed': float(average_speed),\n",
    "                    'motion_intensity': float(motion_intensity),\n",
    "                    'sudden_movements': int(sudden_movements)\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                'average_speed': 0,\n",
    "                'motion_intensity': 0,\n",
    "                'sudden_movements': 0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in motion calculation: {e}\")\n",
    "            return {\n",
    "                'average_speed': 0,\n",
    "                'motion_intensity': 0,\n",
    "                'sudden_movements': 0\n",
    "            }\n",
    "\n",
    "    def draw_detections(self, frame, det_results, pose_results, seg_results):\n",
    "        \"\"\"Draw all detections on the frame\"\"\"\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        # Draw segmentation masks\n",
    "        if seg_results:\n",
    "            for result in seg_results:\n",
    "                if result.masks is not None:\n",
    "                    for mask in result.masks.data:\n",
    "                        try:\n",
    "                            mask_np = mask.cpu().numpy()\n",
    "                            mask_np = cv2.resize(mask_np, (frame.shape[1], frame.shape[0]))\n",
    "                            mask_binary = (mask_np > 0.5).astype(np.uint8) * 255\n",
    "                            colored_mask = np.zeros_like(frame)\n",
    "                            colored_mask[mask_binary > 0] = [0, 0, 255]\n",
    "                            display_frame = cv2.addWeighted(display_frame, 1, colored_mask, 0.3, 0)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in mask drawing: {e}\")\n",
    "                            continue\n",
    "\n",
    "        # Draw object detections\n",
    "        for result in det_results:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                try:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "                    cls = result.names[int(box.cls[0])]\n",
    "                    conf = float(box.conf[0])\n",
    "\n",
    "                    color = (self.colors['violence'] if cls in self.violence_objects \n",
    "                            else self.colors['person'] if cls == 'person' \n",
    "                            else self.colors['other'])\n",
    "\n",
    "                    cv2.rectangle(display_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    label = f'{cls} {conf:.2f}'\n",
    "                    \n",
    "                    (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "                    cv2.rectangle(display_frame, (x1, y1-text_h-5), (x1+text_w, y1), color, -1)\n",
    "                    cv2.putText(display_frame, label, (x1, y1-5), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in detection drawing: {e}\")\n",
    "                    continue\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU Information:\n",
      "GPU Device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "CUDA Version: 11.8\n",
      "Total GPU Memory: 4.29 GB\n",
      "Available Memory: 0.29 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harme\\AppData\\Local\\Temp\\ipykernel_5124\\1366430731.py:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0.0% complete\n",
      "GPU Memory: 0.30 GB\n",
      "Processing: 90.9% complete\n",
      "GPU Memory: 0.30 GB\n",
      "Features saved to: C:\\Users\\harme\\Desktop\\video-detect-gpu\\violence_features.yaml\n",
      "Analyzed video saved to: C:/Users/harme/Desktop/video-detect-gpu/NV_1_analyzed.mp4\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "import torch.cuda\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tkinter import Tk, filedialog\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class ViolenceFeatureExtractor:\n",
    "    def __init__(self, detection_model_path, segmentation_model_path, pose_model_path):\n",
    "        # Initialize device and GPU settings\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.print_gpu_info()\n",
    "        \n",
    "        # GPU optimization settings\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "        \n",
    "        # Load models and move to GPU\n",
    "        self.detection_model = YOLO(detection_model_path).to(self.device)\n",
    "        self.segmentation_model = YOLO(segmentation_model_path).to(self.device)\n",
    "        self.pose_model = YOLO(pose_model_path).to(self.device)\n",
    "        \n",
    "        # Define violence-related objects\n",
    "        self.violence_objects = ['knife', 'gun', 'baseball bat', 'stick']\n",
    "        \n",
    "        # Define colors for visualization\n",
    "        self.colors = {\n",
    "            'violence': (0, 0, 255),    # Red\n",
    "            'person': (0, 255, 0),      # Green\n",
    "            'other': (255, 0, 0),       # Blue\n",
    "            'keypoint': (255, 255, 0),  # Yellow\n",
    "            'connection': (0, 255, 255)  # Cyan\n",
    "        }\n",
    "        \n",
    "        # Performance settings\n",
    "        self.frame_skip = 2\n",
    "        self.batch_size = 4\n",
    "        self.input_size = 640  # YOLO input size\n",
    "\n",
    "    def print_gpu_info(self):\n",
    "        \"\"\"Print GPU information\"\"\"\n",
    "        print(\"\\nGPU Information:\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "            print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "            print(f\"Available Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "        else:\n",
    "            print(\"No GPU available. Using CPU.\")\n",
    "\n",
    "    def preprocess_frame(self, frame):\n",
    "        \"\"\"Preprocess frame for model input\"\"\"\n",
    "        try:\n",
    "            # Convert to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Calculate size to maintain aspect ratio\n",
    "            h, w = frame_rgb.shape[:2]\n",
    "            r = self.input_size / max(h, w)  # Ratio\n",
    "            new_h, new_w = int(h * r), int(w * r)\n",
    "            \n",
    "            # Resize\n",
    "            resized = cv2.resize(frame_rgb, (new_w, new_h))\n",
    "            \n",
    "            # Create canvas of input_size x input_size\n",
    "            canvas = np.zeros((self.input_size, self.input_size, 3), dtype=np.uint8)\n",
    "            \n",
    "            # Calculate padding\n",
    "            pad_h = (self.input_size - new_h) // 2\n",
    "            pad_w = (self.input_size - new_w) // 2\n",
    "            \n",
    "            # Place resized image on canvas\n",
    "            canvas[pad_h:pad_h+new_h, pad_w:pad_w+new_w] = resized\n",
    "            \n",
    "            # Normalize\n",
    "            normalized = canvas.astype(np.float32) / 255.0\n",
    "            \n",
    "            return normalized, (r, pad_w, pad_h)  # Return scale and padding info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in preprocessing: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def calculate_motion_features(self, prev_poses, current_poses):\n",
    "        \"\"\"Calculate motion features between consecutive frames\"\"\"\n",
    "        try:\n",
    "            if not prev_poses or not current_poses:\n",
    "                return {\n",
    "                    'average_speed': 0,\n",
    "                    'motion_intensity': 0,\n",
    "                    'sudden_movements': 0\n",
    "                }\n",
    "\n",
    "            # Convert poses to numpy arrays\n",
    "            prev_poses = np.array(prev_poses)\n",
    "            current_poses = np.array(current_poses)\n",
    "\n",
    "            if prev_poses.shape == current_poses.shape:\n",
    "                # Calculate displacement\n",
    "                displacement = np.linalg.norm(current_poses - prev_poses, axis=2)\n",
    "                average_speed = np.mean(displacement)\n",
    "                motion_intensity = np.std(displacement)\n",
    "                sudden_movements = np.sum(displacement > np.mean(displacement) + 2 * np.std(displacement))\n",
    "\n",
    "                return {\n",
    "                    'average_speed': float(average_speed),\n",
    "                    'motion_intensity': float(motion_intensity),\n",
    "                    'sudden_movements': int(sudden_movements)\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                'average_speed': 0,\n",
    "                'motion_intensity': 0,\n",
    "                'sudden_movements': 0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in motion calculation: {e}\")\n",
    "            return {\n",
    "                'average_speed': 0,\n",
    "                'motion_intensity': 0,\n",
    "                'sudden_movements': 0\n",
    "            }\n",
    "\n",
    "    def rescale_coords(self, x, y, scale_info):\n",
    "        \"\"\"Rescale coordinates back to original image size\"\"\"\n",
    "        scale, pad_w, pad_h = scale_info\n",
    "        x_orig = (x - pad_w) / scale\n",
    "        y_orig = (y - pad_h) / scale\n",
    "        return int(x_orig), int(y_orig)\n",
    "\n",
    "    def draw_detections(self, frame, det_results, pose_results, seg_results, scale_info):\n",
    "        \"\"\"Draw all detections on the frame\"\"\"\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        # Draw segmentation masks\n",
    "        if seg_results:\n",
    "            for result in seg_results:\n",
    "                if result.masks is not None:\n",
    "                    for mask in result.masks.data:\n",
    "                        try:\n",
    "                            mask_np = mask.cpu().numpy()\n",
    "                            mask_np = cv2.resize(mask_np, (frame.shape[1], frame.shape[0]))\n",
    "                            mask_binary = (mask_np > 0.5).astype(np.uint8) * 255\n",
    "                            colored_mask = np.zeros_like(frame)\n",
    "                            colored_mask[mask_binary > 0] = [0, 0, 255]\n",
    "                            display_frame = cv2.addWeighted(display_frame, 1, colored_mask, 0.3, 0)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in mask drawing: {e}\")\n",
    "                            continue\n",
    "                        \n",
    "                # Draw object detections\n",
    "        for result in det_results:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                try:\n",
    "                    # Get box coordinates and rescale them\n",
    "                    x1, y1, x2, y2 = map(float, box.xyxy[0].cpu().numpy())\n",
    "                    x1, y1 = self.rescale_coords(x1, y1, scale_info)\n",
    "                    x2, y2 = self.rescale_coords(x2, y2, scale_info)\n",
    "                    \n",
    "                    cls = result.names[int(box.cls[0])]\n",
    "                    conf = float(box.conf[0])\n",
    "\n",
    "                    color = (self.colors['violence'] if cls in self.violence_objects \n",
    "                            else self.colors['person'] if cls == 'person' \n",
    "                            else self.colors['other'])\n",
    "\n",
    "                    cv2.rectangle(display_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    label = f'{cls} {conf:.2f}'\n",
    "                    \n",
    "                    (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "                    cv2.rectangle(display_frame, (x1, y1-text_h-5), (x1+text_w, y1), color, -1)\n",
    "                    cv2.putText(display_frame, label, (x1, y1-5), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in detection drawing: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Draw pose keypoints and connections\n",
    "        if pose_results:\n",
    "            for result in pose_results:\n",
    "                if result.keypoints is not None:\n",
    "                    for kpts in result.keypoints:\n",
    "                        try:\n",
    "                            keypoints_data = kpts.data[0].cpu().numpy()\n",
    "                            \n",
    "                            # Draw keypoints\n",
    "                            for keypoint in keypoints_data:\n",
    "                                x, y, conf = keypoint\n",
    "                                if conf > 0.5:\n",
    "                                    x, y = self.rescale_coords(x, y, scale_info)\n",
    "                                    cv2.circle(display_frame, (x, y), 4, self.colors['keypoint'], -1)\n",
    "\n",
    "                            # Draw connections\n",
    "                            connections = [(5,7), (7,9), (6,8), (8,10), (5,6), \n",
    "                                         (11,13), (13,15), (12,14), (14,16), (11,12)]\n",
    "                            for connection in connections:\n",
    "                                pt1 = keypoints_data[connection[0]]\n",
    "                                pt2 = keypoints_data[connection[1]]\n",
    "                                \n",
    "                                if pt1[2] > 0.5 and pt2[2] > 0.5:\n",
    "                                    x1, y1 = self.rescale_coords(pt1[0], pt1[1], scale_info)\n",
    "                                    x2, y2 = self.rescale_coords(pt2[0], pt2[1], scale_info)\n",
    "                                    cv2.line(display_frame, (x1, y1), (x2, y2),\n",
    "                                           self.colors['connection'], 2)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in pose drawing: {e}\")\n",
    "                            continue\n",
    "\n",
    "        # Add frame information\n",
    "        cv2.putText(display_frame, \"Press 'q' to quit, 'p' to pause/resume\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        return display_frame\n",
    "\n",
    "    def extract_features(self, frame, prev_frame_data=None):\n",
    "        \"\"\"Extract features from a single frame\"\"\"\n",
    "        try:\n",
    "            # Preprocess frame\n",
    "            processed_frame, scale_info = self.preprocess_frame(frame)\n",
    "            if processed_frame is None:\n",
    "                return None, frame\n",
    "\n",
    "            # Convert to tensor and add batch dimension\n",
    "            frame_tensor = torch.from_numpy(processed_frame).permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
    "\n",
    "            # Run models with GPU acceleration\n",
    "            with torch.cuda.amp.autocast():\n",
    "                det_results = self.detection_model(frame_tensor, verbose=False)\n",
    "                seg_results = self.segmentation_model(frame_tensor, verbose=False)\n",
    "                pose_results = self.pose_model(frame_tensor, verbose=False)\n",
    "\n",
    "            # Draw detections\n",
    "            annotated_frame = self.draw_detections(frame, det_results, pose_results, seg_results, scale_info)\n",
    "\n",
    "            # Initialize features\n",
    "            features = {\n",
    "                'objects': [],\n",
    "                'poses': [],\n",
    "                'segmentation': [],\n",
    "                'motion': {},\n",
    "                'violence_indicators': {\n",
    "                    'weapon_present': False,\n",
    "                    'rapid_motion': False,\n",
    "                    'close_interaction': False,\n",
    "                    'aggressive_pose': False\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Process detections\n",
    "            for result in det_results:\n",
    "                for box in result.boxes:\n",
    "                    try:\n",
    "                        cls = result.names[int(box.cls[0])]\n",
    "                        conf = float(box.conf[0])\n",
    "                        box_coords = box.xyxy[0].cpu().numpy().tolist()\n",
    "                        \n",
    "                        features['objects'].append({\n",
    "                            'class': cls,\n",
    "                            'confidence': conf,\n",
    "                            'box': box_coords\n",
    "                        })\n",
    "                        \n",
    "                        if cls in self.violence_objects:\n",
    "                            features['violence_indicators']['weapon_present'] = True\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing detection: {e}\")\n",
    "                        continue\n",
    "\n",
    "            # Process poses\n",
    "            if pose_results:\n",
    "                for result in pose_results:\n",
    "                    if result.keypoints is not None:\n",
    "                        for kpts in result.keypoints:\n",
    "                            try:\n",
    "                                pose_data = kpts.data[0].cpu().numpy().tolist()\n",
    "                                features['poses'].append(pose_data)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing pose: {e}\")\n",
    "                                continue\n",
    "\n",
    "            # Process segmentation\n",
    "            if seg_results:\n",
    "                for result in seg_results:\n",
    "                    if result.masks is not None:\n",
    "                        for mask in result.masks.data:\n",
    "                            try:\n",
    "                                mask_np = mask.cpu().numpy()\n",
    "                                features['segmentation'].append(np.sum(mask_np > 0.5))\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing segmentation: {e}\")\n",
    "                                continue\n",
    "\n",
    "            # Calculate motion features\n",
    "            if prev_frame_data and 'poses' in prev_frame_data:\n",
    "                motion_features = self.calculate_motion_features(\n",
    "                    prev_frame_data['poses'], features['poses'])\n",
    "                features['motion'] = motion_features\n",
    "                \n",
    "                if motion_features.get('average_speed', 0) > 10:\n",
    "                    features['violence_indicators']['rapid_motion'] = True\n",
    "\n",
    "            return features, annotated_frame\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {e}\")\n",
    "            return None, frame\n",
    "\n",
    "def process_video(video_path, extractor, output_path):\n",
    "    \"\"\"Process video with GPU acceleration\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create video writer\n",
    "    output_video_path = video_path.rsplit('.', 1)[0] + '_analyzed.mp4'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize data storage\n",
    "    video_data = {\n",
    "        'metadata': {\n",
    "            'path': video_path,\n",
    "            'fps': fps,\n",
    "            'frame_count': frame_count,\n",
    "            'width': frame_width,\n",
    "            'height': frame_height\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    frame_idx = 0\n",
    "    prev_frame_data = None\n",
    "    paused = False\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            if not paused:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # Skip frames if needed\n",
    "                if frame_idx % extractor.frame_skip != 0:\n",
    "                    frame_idx += 1\n",
    "                    continue\n",
    "\n",
    "                # Extract features and get annotated frame\n",
    "                features, annotated_frame = extractor.extract_features(frame, prev_frame_data)\n",
    "                \n",
    "                if features is not None:\n",
    "                    frame_data = {\n",
    "                        'frame_index': frame_idx,\n",
    "                        'timestamp': frame_idx / fps,\n",
    "                        'features': features\n",
    "                    }\n",
    "                    \n",
    "                    video_data['frames'].append(frame_data)\n",
    "                    prev_frame_data = features\n",
    "                    out.write(annotated_frame)\n",
    "\n",
    "                    # Show progress\n",
    "                    if frame_idx % (30 * extractor.frame_skip) == 0:\n",
    "                        progress = (frame_idx / frame_count) * 100\n",
    "                        print(f\"Processing: {progress:.1f}% complete\")\n",
    "                        if torch.cuda.is_available():\n",
    "                            print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "                    # Display frame\n",
    "                    cv2.imshow('Violence Detection Analysis', annotated_frame)\n",
    "\n",
    "                frame_idx += 1\n",
    "\n",
    "                # Handle key events\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "                elif key == ord('p'):\n",
    "                    paused = not paused\n",
    "                    print(\"Paused - Press 'p' to resume\" if paused else \"Resumed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Final GPU cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save features\n",
    "        try:\n",
    "            with open(output_path, 'w') as f:\n",
    "                yaml.dump(video_data, f, default_flow_style=False)\n",
    "            print(f\"Features saved to: {output_path}\")\n",
    "            print(f\"Analyzed video saved to: {output_video_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {e}\")\n",
    "\n",
    "    return video_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Get video file\n",
    "    video_path = filedialog.askopenfilename(\n",
    "        title=\"Select Video File\",\n",
    "        filetypes=[(\"Video Files\", \"*.mp4;*.avi\")]\n",
    "    )\n",
    "\n",
    "    if not video_path:\n",
    "        print(\"No video file selected\")\n",
    "        exit()\n",
    "\n",
    "    # Define model paths\n",
    "    detection_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m.pt'\n",
    "    segmentation_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-seg.pt'\n",
    "    pose_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-pose.pt'\n",
    "    \n",
    "    # Initialize feature extractor\n",
    "    extractor = ViolenceFeatureExtractor(\n",
    "        detection_model_path,\n",
    "        segmentation_model_path,\n",
    "        pose_model_path\n",
    "    )\n",
    "\n",
    "    # Process video\n",
    "    output_path = r'C:\\Users\\harme\\Desktop\\video-detect-gpu\\violence_features.yaml'\n",
    "    video_data = process_video(video_path, extractor, output_path)\n",
    "\n",
    "    print(\"Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU Information:\n",
      "GPU Device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "CUDA Version: 11.8\n",
      "Total GPU Memory: 4.29 GB\n",
      "Available Memory: 0.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harme\\AppData\\Local\\Temp\\ipykernel_13920\\1464784378.py:308: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0.0% complete\n",
      "GPU Memory: 0.30 GB\n",
      "Processing: 52.6% complete\n",
      "GPU Memory: 0.30 GB\n",
      "Features saved to: C:\\Users\\harme\\Desktop\\video-detect-gpu\\violence_features.yaml\n",
      "Analyzed video saved to: C:/Users/harme/Desktop/video-detect-gpu/V_5_analyzed.mp4\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "import torch.cuda\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tkinter import Tk, filedialog\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class ViolenceFeatureExtractor:\n",
    "    def __init__(self, detection_model_path, segmentation_model_path, pose_model_path):\n",
    "        # Initialize device and GPU settings\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.print_gpu_info()\n",
    "        \n",
    "        # GPU optimization settings\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "        \n",
    "        # Load models and move to GPU\n",
    "        self.detection_model = YOLO(detection_model_path).to(self.device)\n",
    "        self.segmentation_model = YOLO(segmentation_model_path).to(self.device)\n",
    "        self.pose_model = YOLO(pose_model_path).to(self.device)\n",
    "        \n",
    "        # Define violence-related objects and relevant classes\n",
    "        self.violence_objects = ['knife', 'gun', 'baseball bat', 'stick', 'bottle']\n",
    "        self.relevant_classes = ['person'] + self.violence_objects\n",
    "        \n",
    "        # Define colors for visualization\n",
    "        self.colors = {\n",
    "            'violence': (0, 0, 255),    # Red\n",
    "            'person': (0, 255, 0),      # Green\n",
    "            'interaction': (255, 0, 0),  # Blue\n",
    "            'keypoint': (255, 255, 0),  # Yellow\n",
    "            'connection': (0, 255, 255)  # Cyan\n",
    "        }\n",
    "        \n",
    "        # Performance and detection settings\n",
    "        self.frame_skip = 2\n",
    "        self.input_size = 640\n",
    "        self.conf_threshold = 0.5\n",
    "        self.interaction_threshold = 0.5  # For person-to-person interaction detection\n",
    "\n",
    "    def print_gpu_info(self):\n",
    "        \"\"\"Print GPU information\"\"\"\n",
    "        print(\"\\nGPU Information:\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "            print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "            print(f\"Available Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "        else:\n",
    "            print(\"No GPU available. Using CPU.\")\n",
    "\n",
    "    def preprocess_frame(self, frame):\n",
    "        \"\"\"Preprocess frame for model input\"\"\"\n",
    "        try:\n",
    "            # Convert to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Calculate size to maintain aspect ratio\n",
    "            h, w = frame_rgb.shape[:2]\n",
    "            r = self.input_size / max(h, w)\n",
    "            new_h, new_w = int(h * r), int(w * r)\n",
    "            \n",
    "            # Resize\n",
    "            resized = cv2.resize(frame_rgb, (new_w, new_h))\n",
    "            \n",
    "            # Create canvas of input_size x input_size\n",
    "            canvas = np.zeros((self.input_size, self.input_size, 3), dtype=np.uint8)\n",
    "            \n",
    "            # Calculate padding\n",
    "            pad_h = (self.input_size - new_h) // 2\n",
    "            pad_w = (self.input_size - new_w) // 2\n",
    "            \n",
    "            # Place resized image on canvas\n",
    "            canvas[pad_h:pad_h+new_h, pad_w:pad_w+new_w] = resized\n",
    "            \n",
    "            # Normalize\n",
    "            normalized = canvas.astype(np.float32) / 255.0\n",
    "            \n",
    "            return normalized, (r, pad_w, pad_h)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in preprocessing: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def analyze_person_interactions(self, person_boxes):\n",
    "        \"\"\"Analyze interactions between detected people\"\"\"\n",
    "        interactions = []\n",
    "        if len(person_boxes) < 2:\n",
    "            return interactions\n",
    "\n",
    "        for i in range(len(person_boxes)):\n",
    "            for j in range(i + 1, len(person_boxes)):\n",
    "                box1 = person_boxes[i]\n",
    "                box2 = person_boxes[j]\n",
    "                \n",
    "                # Calculate centers\n",
    "                center1 = [(box1[0] + box1[2])/2, (box1[1] + box1[3])/2]\n",
    "                center2 = [(box2[0] + box2[2])/2, (box2[1] + box2[3])/2]\n",
    "                \n",
    "                # Calculate distance and box sizes\n",
    "                distance = np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)\n",
    "                box1_size = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "                box2_size = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "                avg_size = (box1_size + box2_size) / 2\n",
    "                \n",
    "                # Check for close interaction\n",
    "                if distance < avg_size * self.interaction_threshold:\n",
    "                   interactions.append({\n",
    "                         'person1_idx': i,\n",
    "                         'person2_idx': j,\n",
    "                         'distance': distance,\n",
    "                         'relative_distance': distance / avg_size,\n",
    "                         'center1': center1,\n",
    "                         'center2': center2,\n",
    "                         'box1': box1,\n",
    "                         'box2': box2\n",
    "                     })\n",
    "        \n",
    "        return interactions\n",
    "\n",
    "    def calculate_motion_features(self, prev_poses, current_poses):\n",
    "        \"\"\"Calculate motion features between consecutive frames\"\"\"\n",
    "        try:\n",
    "            if not prev_poses or not current_poses:\n",
    "                return {\n",
    "                    'average_speed': 0,\n",
    "                    'motion_intensity': 0,\n",
    "                    'sudden_movements': 0\n",
    "                }\n",
    "\n",
    "            # Convert poses to numpy arrays\n",
    "            prev_poses = np.array(prev_poses)\n",
    "            current_poses = np.array(current_poses)\n",
    "\n",
    "            if prev_poses.shape == current_poses.shape:\n",
    "                # Calculate displacement\n",
    "                displacement = np.linalg.norm(current_poses - prev_poses, axis=2)\n",
    "                average_speed = np.mean(displacement)\n",
    "                motion_intensity = np.std(displacement)\n",
    "                sudden_movements = np.sum(displacement > np.mean(displacement) + 2 * np.std(displacement))\n",
    "\n",
    "                return {\n",
    "                    'average_speed': float(average_speed),\n",
    "                    'motion_intensity': float(motion_intensity),\n",
    "                    'sudden_movements': int(sudden_movements)\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                'average_speed': 0,\n",
    "                'motion_intensity': 0,\n",
    "                'sudden_movements': 0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in motion calculation: {e}\")\n",
    "            return {\n",
    "                'average_speed': 0,\n",
    "                'motion_intensity': 0,\n",
    "                'sudden_movements': 0\n",
    "            }\n",
    "    def analyze_poses_for_violence(self, poses):\n",
    "        \"\"\"Analyze poses for potential aggressive/violent behavior\"\"\"\n",
    "        try:\n",
    "            if not poses:\n",
    "                return False\n",
    "\n",
    "            for pose in poses:\n",
    "                # Convert pose to numpy array for calculations\n",
    "                pose_array = np.array(pose)\n",
    "                \n",
    "                # Check for rapid arm movements (high confidence keypoints only)\n",
    "                arm_keypoints = [5, 7, 9, 6, 8, 10]  # Shoulders, elbows, wrists\n",
    "                arm_positions = pose_array[arm_keypoints]\n",
    "                arm_confidences = arm_positions[:, 2]\n",
    "                \n",
    "                if np.mean(arm_confidences) > 0.5:\n",
    "                    # Calculate arm angles and velocities\n",
    "                    # Add your specific pose analysis logic here\n",
    "                    return True\n",
    "                    \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in pose analysis: {e}\")\n",
    "            return False\n",
    "\n",
    "    def rescale_coords(self, x, y, scale_info):\n",
    "        \"\"\"Rescale coordinates back to original image size\"\"\"\n",
    "        scale, pad_w, pad_h = scale_info\n",
    "        x_orig = (x - pad_w) / scale\n",
    "        y_orig = (y - pad_h) / scale\n",
    "        return int(x_orig), int(y_orig)\n",
    "\n",
    "    def draw_detections(self, frame, det_results, pose_results, interactions, scale_info):\n",
    "        \"\"\"Draw detections, poses, and interactions\"\"\"\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        # Draw object detections\n",
    "        for result in det_results:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                try:\n",
    "                    # Get box coordinates and rescale them\n",
    "                    x1, y1, x2, y2 = map(float, box.xyxy[0].cpu().numpy())\n",
    "                    x1, y1 = self.rescale_coords(x1, y1, scale_info)\n",
    "                    x2, y2 = self.rescale_coords(x2, y2, scale_info)\n",
    "                    \n",
    "                    cls = result.names[int(box.cls[0])]\n",
    "                    conf = float(box.conf[0])\n",
    "\n",
    "                    # Only draw relevant classes\n",
    "                    if cls in self.relevant_classes:\n",
    "                        color = (self.colors['violence'] if cls in self.violence_objects \n",
    "                                else self.colors['person'])\n",
    "\n",
    "                        cv2.rectangle(display_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                        label = f'{cls} {conf:.2f}'\n",
    "                        \n",
    "                        (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "                        cv2.rectangle(display_frame, (x1, y1-text_h-5), (x1+text_w, y1), color, -1)\n",
    "                        cv2.putText(display_frame, label, (x1, y1-5), \n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in detection drawing: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Draw interactions\n",
    "        for interaction in interactions:\n",
    "            try:\n",
    "            # Get centers from interaction data\n",
    "                 x1, y1 = self.rescale_coords(interaction['center1'][0], interaction['center1'][1], scale_info)\n",
    "                 x2, y2 = self.rescale_coords(interaction['center2'][0], interaction['center2'][1], scale_info)\n",
    "              \n",
    "            # Draw line between interacting people\n",
    "                 cv2.line(display_frame, (x1, y1), (x2, y2), self.colors['interaction'], 2)\n",
    "            \n",
    "            # Optional: Draw interaction distance\n",
    "                 mid_point = ((x1 + x2)//2, (y1 + y2)//2)\n",
    "                 distance_label = f\"D: {interaction['relative_distance']:.2f}\"\n",
    "                 cv2.putText(display_frame, distance_label, mid_point, \n",
    "                         cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['interaction'], 2)\n",
    "            \n",
    "            except Exception as e:\n",
    "                 print(f\"Error drawing interaction: {e}\")\n",
    "                 continue\n",
    "\n",
    "        # Draw pose keypoints and connections\n",
    "        if pose_results:\n",
    "            for result in pose_results:\n",
    "                if result.keypoints is not None:\n",
    "                    for kpts in result.keypoints:\n",
    "                        try:\n",
    "                            keypoints_data = kpts.data[0].cpu().numpy()\n",
    "                            \n",
    "                            # Draw keypoints\n",
    "                            for keypoint in keypoints_data:\n",
    "                                x, y, conf = keypoint\n",
    "                                if conf > 0.5:\n",
    "                                    x, y = self.rescale_coords(x, y, scale_info)\n",
    "                                    cv2.circle(display_frame, (x, y), 4, self.colors['keypoint'], -1)\n",
    "\n",
    "                            # Draw connections\n",
    "                            connections = [(5,7), (7,9), (6,8), (8,10), (5,6), \n",
    "                                         (11,13), (13,15), (12,14), (14,16), (11,12)]\n",
    "                            for connection in connections:\n",
    "                                pt1 = keypoints_data[connection[0]]\n",
    "                                pt2 = keypoints_data[connection[1]]\n",
    "                                \n",
    "                                if pt1[2] > 0.5 and pt2[2] > 0.5:\n",
    "                                    x1, y1 = self.rescale_coords(pt1[0], pt1[1], scale_info)\n",
    "                                    x2, y2 = self.rescale_coords(pt2[0], pt2[1], scale_info)\n",
    "                                    cv2.line(display_frame, (x1, y1), (x2, y2),\n",
    "                                           self.colors['connection'], 2)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in pose drawing: {e}\")\n",
    "                            continue\n",
    "\n",
    "        # Add violence indicators\n",
    "        if self.current_risk_level > 0.7:  # High risk threshold\n",
    "            cv2.putText(display_frame, \"HIGH RISK\", (10, 60), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Add frame information\n",
    "        cv2.putText(display_frame, \"Press 'q' to quit, 'p' to pause/resume\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        return display_frame\n",
    "\n",
    "    def extract_features(self, frame, prev_frame_data=None):\n",
    "        \"\"\"Extract violence-relevant features from frame\"\"\"\n",
    "        try:\n",
    "            # Preprocess frame\n",
    "            processed_frame, scale_info = self.preprocess_frame(frame)\n",
    "            if processed_frame is None:\n",
    "                return None, frame\n",
    "\n",
    "            # Convert to tensor and add batch dimension\n",
    "            frame_tensor = torch.from_numpy(processed_frame).permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
    "\n",
    "            # Run models with GPU acceleration\n",
    "            with torch.cuda.amp.autocast():\n",
    "                det_results = self.detection_model(frame_tensor, verbose=False)\n",
    "                pose_results = self.pose_model(frame_tensor, verbose=False)\n",
    "\n",
    "            # Initialize features\n",
    "            features = {\n",
    "                'objects': [],\n",
    "                'poses': [],\n",
    "                'interactions': [],\n",
    "                'motion': {},\n",
    "                'violence_indicators': {\n",
    "                    'weapon_present': False,\n",
    "                    'close_interaction': False,\n",
    "                    'rapid_motion': False,\n",
    "                    'aggressive_pose': False\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Process relevant detections\n",
    "            person_boxes = []\n",
    "            for result in det_results:\n",
    "                for box in result.boxes:\n",
    "                    try:\n",
    "                        cls = result.names[int(box.cls[0])]\n",
    "                        if cls in self.relevant_classes:\n",
    "                            conf = float(box.conf[0])\n",
    "                            box_coords = box.xyxy[0].cpu().numpy().tolist()\n",
    "                            \n",
    "                            features['objects'].append({\n",
    "                                'class': cls,\n",
    "                                'confidence': conf,\n",
    "                                'box': box_coords\n",
    "                            })\n",
    "                            \n",
    "                            if cls == 'person':\n",
    "                                person_boxes.append(box_coords)\n",
    "                            elif cls in self.violence_objects:\n",
    "                                features['violence_indicators']['weapon_present'] = True\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing detection: {e}\")\n",
    "                        continue\n",
    "\n",
    "            # Analyze person interactions\n",
    "            if len(person_boxes) >= 2:\n",
    "                interactions = self.analyze_person_interactions(person_boxes)\n",
    "                features['interactions'] = interactions\n",
    "                features['violence_indicators']['close_interaction'] = len(interactions) > 0\n",
    "\n",
    "            # Process poses and analyze for violence\n",
    "            if pose_results:\n",
    "                for result in pose_results:\n",
    "                    if result.keypoints is not None:\n",
    "                        for kpts in result.keypoints:\n",
    "                            try:\n",
    "                                pose_data = kpts.data[0].cpu().numpy().tolist()\n",
    "                                features['poses'].append(pose_data)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing pose: {e}\")\n",
    "                                continue\n",
    "\n",
    "                features['violence_indicators']['aggressive_pose'] = self.analyze_poses_for_violence(features['poses'])\n",
    "\n",
    "            # Calculate motion features\n",
    "            if prev_frame_data and 'poses' in prev_frame_data:\n",
    "                motion_features = self.calculate_motion_features(\n",
    "                    prev_frame_data['poses'], features['poses'])\n",
    "                features['motion'] = motion_features\n",
    "                \n",
    "                features['violence_indicators']['rapid_motion'] = motion_features.get('average_speed', 0) > 10\n",
    "\n",
    "            # Calculate overall risk level\n",
    "            risk_weights = {\n",
    "                'weapon_present': 0.4,\n",
    "                'close_interaction': 0.3,\n",
    "                'rapid_motion': 0.2,\n",
    "                'aggressive_pose': 0.1\n",
    "            }\n",
    "            \n",
    "            self.current_risk_level = sum(\n",
    "                risk_weights[indicator] * int(value)\n",
    "                for indicator, value in features['violence_indicators'].items()\n",
    "            )\n",
    "\n",
    "            # Draw detections\n",
    "            annotated_frame = self.draw_detections(\n",
    "                frame, det_results, pose_results, \n",
    "                features['interactions'], scale_info\n",
    "            )\n",
    "\n",
    "            return features, annotated_frame\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {e}\")\n",
    "            return None, frame\n",
    "def process_video(video_path, extractor, output_path):\n",
    "    \"\"\"Process video with GPU acceleration\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create video writer\n",
    "    output_video_path = video_path.rsplit('.', 1)[0] + '_analyzed.mp4'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize data storage\n",
    "    video_data = {\n",
    "        'metadata': {\n",
    "            'path': video_path,\n",
    "            'fps': fps,\n",
    "            'frame_count': frame_count,\n",
    "            'width': frame_width,\n",
    "            'height': frame_height\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    frame_idx = 0\n",
    "    prev_frame_data = None\n",
    "    paused = False\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            if not paused:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # Skip frames if needed\n",
    "                if frame_idx % extractor.frame_skip != 0:\n",
    "                    frame_idx += 1\n",
    "                    continue\n",
    "\n",
    "                # Extract features and get annotated frame\n",
    "                features, annotated_frame = extractor.extract_features(frame, prev_frame_data)\n",
    "                \n",
    "                if features is not None:\n",
    "                    frame_data = {\n",
    "                        'frame_index': frame_idx,\n",
    "                        'timestamp': frame_idx / fps,\n",
    "                        'features': features\n",
    "                    }\n",
    "                    \n",
    "                    video_data['frames'].append(frame_data)\n",
    "                    prev_frame_data = features\n",
    "                    out.write(annotated_frame)\n",
    "\n",
    "                    # Show progress\n",
    "                    if frame_idx % (30 * extractor.frame_skip) == 0:\n",
    "                        progress = (frame_idx / frame_count) * 100\n",
    "                        print(f\"Processing: {progress:.1f}% complete\")\n",
    "                        if torch.cuda.is_available():\n",
    "                            print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "                    # Display frame\n",
    "                    cv2.imshow('Violence Detection Analysis', annotated_frame)\n",
    "\n",
    "                frame_idx += 1\n",
    "\n",
    "                # Handle key events\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "                elif key == ord('p'):\n",
    "                    paused = not paused\n",
    "                    print(\"Paused - Press 'p' to resume\" if paused else \"Resumed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Final GPU cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save features\n",
    "        try:\n",
    "            with open(output_path, 'w') as f:\n",
    "                yaml.dump(video_data, f, default_flow_style=False)\n",
    "            print(f\"Features saved to: {output_path}\")\n",
    "            print(f\"Analyzed video saved to: {output_video_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {e}\")\n",
    "\n",
    "    return video_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Get video file\n",
    "    video_path = filedialog.askopenfilename(\n",
    "        title=\"Select Video File\",\n",
    "        filetypes=[(\"Video Files\", \"*.mp4;*.avi\")]\n",
    "    )\n",
    "\n",
    "    if not video_path:\n",
    "        print(\"No video file selected\")\n",
    "        exit()\n",
    "\n",
    "    # Define model paths\n",
    "    detection_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m.pt'\n",
    "    segmentation_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-seg.pt'\n",
    "    pose_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-pose.pt'\n",
    "    \n",
    "    # Initialize feature extractor\n",
    "    extractor = ViolenceFeatureExtractor(\n",
    "        detection_model_path,\n",
    "        segmentation_model_path,\n",
    "        pose_model_path\n",
    "    )\n",
    "\n",
    "    # Process video\n",
    "    output_path = r'C:\\Users\\harme\\Desktop\\video-detect-gpu\\violence_features.yaml'\n",
    "    video_data = process_video(video_path, extractor, output_path)\n",
    "\n",
    "    print(\"Analysis complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "import torch.cuda\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tkinter import Tk, filedialog\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, model_state_path = 'model_state.pth'):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "            \n",
    "        self.print_gpu_info()\n",
    "        self.load_models(model_state_path)\n",
    "        \n",
    "    def print_gpu_info(self):\n",
    "        \"\"\"Print GPU information\"\"\"\n",
    "        print(\"\\nGPU Information:\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "            print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "            print(f\"Available Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "        else:\n",
    "            print(\"No GPU available. Using CPU.\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    def load_models(self, model_state_path):\n",
    "        \"\"\"Load models from the saved state\"\"\"\n",
    "        print(\"Loading models...\")\n",
    "        model_state = torch.load(model_state_path, map_location=self.device)\n",
    "        \n",
    "        # Initialize models\n",
    "        self.detection_model = YOLO().to(self.device)\n",
    "        self.segmentation_model = YOLO().to(self.device)\n",
    "        self.pose_model = YOLO().to(self.device)\n",
    "        \n",
    "        # Load state dicts\n",
    "        self.detection_model.load_state_dict(model_state['detection_model'])\n",
    "        self.segmentation_model.load_state_dict(model_state['segmentation_model'])\n",
    "        self.pose_model.load_state_dict(model_state['pose_model'])\n",
    "        \n",
    "        print(\"Models loaded successfully!\")\n",
    "        \n",
    "    def define_violence_detection_parameters(self):\n",
    "        self.violence_objects = [\"knife\",\"gun\",\"baseball bat\", \"stick\",\"bottle\"]\n",
    "        self.relevant_classes = [\"person\"] + self.violence_objects\n",
    "        \n",
    "        self.colors = {\n",
    "            'violence': (0, 0, 255),    # Red\n",
    "            'person': (0, 255, 0),      # Green\n",
    "            'interaction': (255, 0, 0),  # Blue\n",
    "            'keypoint': (255, 255, 0),  # Yellow\n",
    "            'connection': (0, 255, 255)  # Cyan\n",
    "        }\n",
    "        \n",
    "        self.frame_skip = 2\n",
    "        self.input_size = 640\n",
    "        self.conf_threshold = 0.5\n",
    "        self.interaction_threshold = 0.5\n",
    "        \n",
    "    def preprocess_frame(self,frame):\n",
    "        \n",
    "        try:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            h, w = frame_rgb.shape[:2]\n",
    "            r = self.input_size / max(h, w)\n",
    "            new_h, new_w = int(h * r), int(w * r)\n",
    "            \n",
    "            resized = cv2.resize(frame_rgb, (new_h, new_w))\n",
    "            \n",
    "            canvas = np.zeros((self.input_size, self.input_size, 3), dtype=np.uint8)\n",
    "            \n",
    "            pad_h = (self.input_size, - new_h) // 2\n",
    "            pad_w = (self.input_size - new_w) // 2\n",
    "            \n",
    "            canvas[pad_h : pad_h + new_h, pad_w : pad_w + new_w] = resized\n",
    "            \n",
    "            normalized = canvas.astype(np.float32) / 255.0\n",
    "            \n",
    "            return normalized, (r, pad_w, pad_h)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in preprocessing: {e}\")\n",
    "            return None, None\n",
    "        \n",
    "    def analyze_person_interactions(self, person_boxes):\n",
    "        \n",
    "        interactions = []\n",
    "        if len(person_boxes) < 2:\n",
    "            return interactions\n",
    "        \n",
    "        for i in range(len(person_boxes)):\n",
    "            for j in range(i+1, len(person_boxes)):\n",
    "                box1 = person_boxes[i]\n",
    "                box2 = person_boxes[j]\n",
    "                \n",
    "                center1 = [(box1[0] + box1[2])/2, (box1[1] + box1[3])/2]\n",
    "                center2 = [(box2[0] + box2[2])/2, (box2[1] + box2[3])/2]\n",
    "                \n",
    "                distance = np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)\n",
    "                box1_size = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "                box2_size = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "                avg_size = (box1_size + box2_size) / 2\n",
    "                \n",
    "                # Check for close interaction\n",
    "                if distance < avg_size * self.interaction_threshold:\n",
    "                   interactions.append({\n",
    "                         'person1_idx': i,\n",
    "                         'person2_idx': j,\n",
    "                         'distance': distance,\n",
    "                         'relative_distance': distance / avg_size,\n",
    "                         'center1': center1,\n",
    "                         'center2': center2,\n",
    "                         'box1': box1,\n",
    "                         'box2': box2\n",
    "                     })\n",
    "        \n",
    "        return interactions\n",
    "\n",
    "    def calculate_motion_features(self, prev_poses, current_poses):\n",
    "        \"\"\"Calculate motion features between consecutive frames\"\"\"\n",
    "        try:\n",
    "            if not prev_poses or not current_poses:\n",
    "                return {\n",
    "                    'average_speed': 0,\n",
    "                    'motion_intensity': 0,\n",
    "                    'sudden_movements': 0\n",
    "                }\n",
    "\n",
    "            # Convert poses to numpy arrays\n",
    "            prev_poses = np.array(prev_poses)\n",
    "            current_poses = np.array(current_poses)\n",
    "\n",
    "            if prev_poses.shape == current_poses.shape:\n",
    "                # Calculate displacement\n",
    "                displacement = np.linalg.norm(current_poses - prev_poses, axis=2)\n",
    "                average_speed = np.mean(displacement)\n",
    "                motion_intensity = np.std(displacement)\n",
    "                sudden_movements = np.sum(displacement > np.mean(displacement) + 2 * np.std(displacement))\n",
    "\n",
    "                return {\n",
    "                    'average_speed': float(average_speed),\n",
    "                    'motion_intensity': float(motion_intensity),\n",
    "                    'sudden_movements': int(sudden_movements)\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                'average_speed': 0,\n",
    "                'motion_intensity': 0,\n",
    "                'sudden_movements': 0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in motion calculation: {e}\")\n",
    "            return {\n",
    "                'average_speed': 0,\n",
    "                'motion_intensity': 0,\n",
    "                'sudden_movements': 0\n",
    "            }\n",
    "    def analyze_poses_for_violence(self, poses):\n",
    "        \"\"\"Analyze poses for potential aggressive/violent behavior\"\"\"\n",
    "        try:\n",
    "            if not poses:\n",
    "                return False\n",
    "\n",
    "            for pose in poses:\n",
    "                # Convert pose to numpy array for calculations\n",
    "                pose_array = np.array(pose)\n",
    "                \n",
    "                # Check for rapid arm movements (high confidence keypoints only)\n",
    "                arm_keypoints = [5, 7, 9, 6, 8, 10]  # Shoulders, elbows, wrists\n",
    "                arm_positions = pose_array[arm_keypoints]\n",
    "                arm_confidences = arm_positions[:, 2]\n",
    "                \n",
    "                if np.mean(arm_confidences) > 0.5:\n",
    "                    # Calculate arm angles and velocities\n",
    "                    # Add your specific pose analysis logic here\n",
    "                    return True\n",
    "                    \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in pose analysis: {e}\")\n",
    "            return False\n",
    "\n",
    "    def rescale_coords(self, x, y, scale_info):\n",
    "        \"\"\"Rescale coordinates back to original image size\"\"\"\n",
    "        scale, pad_w, pad_h = scale_info\n",
    "        x_orig = (x - pad_w) / scale\n",
    "        y_orig = (y - pad_h) / scale\n",
    "        return int(x_orig), int(y_orig)\n",
    "\n",
    "    def draw_detections(self, frame, det_results, pose_results, interactions, scale_info):\n",
    "        \"\"\"Draw detections, poses, and interactions\"\"\"\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        # Draw object detections\n",
    "        for result in det_results:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                try:\n",
    "                    # Get box coordinates and rescale them\n",
    "                    x1, y1, x2, y2 = map(float, box.xyxy[0].cpu().numpy())\n",
    "                    x1, y1 = self.rescale_coords(x1, y1, scale_info)\n",
    "                    x2, y2 = self.rescale_coords(x2, y2, scale_info)\n",
    "                    \n",
    "                    cls = result.names[int(box.cls[0])]\n",
    "                    conf = float(box.conf[0])\n",
    "\n",
    "                    # Only draw relevant classes\n",
    "                    if cls in self.relevant_classes:\n",
    "                        color = (self.colors['violence'] if cls in self.violence_objects \n",
    "                                else self.colors['person'])\n",
    "\n",
    "                        cv2.rectangle(display_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                        label = f'{cls} {conf:.2f}'\n",
    "                        \n",
    "                        (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "                        cv2.rectangle(display_frame, (x1, y1-text_h-5), (x1+text_w, y1), color, -1)\n",
    "                        cv2.putText(display_frame, label, (x1, y1-5), \n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in detection drawing: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Draw interactions\n",
    "        for interaction in interactions:\n",
    "            try:\n",
    "            # Get centers from interaction data\n",
    "                 x1, y1 = self.rescale_coords(interaction['center1'][0], interaction['center1'][1], scale_info)\n",
    "                 x2, y2 = self.rescale_coords(interaction['center2'][0], interaction['center2'][1], scale_info)\n",
    "              \n",
    "            # Draw line between interacting people\n",
    "                 cv2.line(display_frame, (x1, y1), (x2, y2), self.colors['interaction'], 2)\n",
    "            \n",
    "            # Optional: Draw interaction distance\n",
    "                 mid_point = ((x1 + x2)//2, (y1 + y2)//2)\n",
    "                 distance_label = f\"D: {interaction['relative_distance']:.2f}\"\n",
    "                 cv2.putText(display_frame, distance_label, mid_point, \n",
    "                         cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['interaction'], 2)\n",
    "            \n",
    "            except Exception as e:\n",
    "                 print(f\"Error drawing interaction: {e}\")\n",
    "                 continue\n",
    "\n",
    "        # Draw pose keypoints and connections\n",
    "        if pose_results:\n",
    "            for result in pose_results:\n",
    "                if result.keypoints is not None:\n",
    "                    for kpts in result.keypoints:\n",
    "                        try:\n",
    "                            keypoints_data = kpts.data[0].cpu().numpy()\n",
    "                            \n",
    "                            # Draw keypoints\n",
    "                            for keypoint in keypoints_data:\n",
    "                                x, y, conf = keypoint\n",
    "                                if conf > 0.5:\n",
    "                                    x, y = self.rescale_coords(x, y, scale_info)\n",
    "                                    cv2.circle(display_frame, (x, y), 4, self.colors['keypoint'], -1)\n",
    "\n",
    "                            # Draw connections\n",
    "                            connections = [(5,7), (7,9), (6,8), (8,10), (5,6), \n",
    "                                         (11,13), (13,15), (12,14), (14,16), (11,12)]\n",
    "                            for connection in connections:\n",
    "                                pt1 = keypoints_data[connection[0]]\n",
    "                                pt2 = keypoints_data[connection[1]]\n",
    "                                \n",
    "                                if pt1[2] > 0.5 and pt2[2] > 0.5:\n",
    "                                    x1, y1 = self.rescale_coords(pt1[0], pt1[1], scale_info)\n",
    "                                    x2, y2 = self.rescale_coords(pt2[0], pt2[1], scale_info)\n",
    "                                    cv2.line(display_frame, (x1, y1), (x2, y2),\n",
    "                                           self.colors['connection'], 2)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in pose drawing: {e}\")\n",
    "                            continue\n",
    "\n",
    "        # Add violence indicators\n",
    "        if self.current_risk_level > 0.7:  # High risk threshold\n",
    "            cv2.putText(display_frame, \"HIGH RISK\", (10, 60), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Add frame information\n",
    "        cv2.putText(display_frame, \"Press 'q' to quit, 'p' to pause/resume\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        return display_frame\n",
    "\n",
    "    def extract_features(self, frame, prev_frame_data=None):\n",
    "        \"\"\"Extract violence-relevant features from frame\"\"\"\n",
    "        try:\n",
    "            # Preprocess frame\n",
    "            processed_frame, scale_info = self.preprocess_frame(frame)\n",
    "            if processed_frame is None:\n",
    "                return None, frame\n",
    "\n",
    "            # Convert to tensor and add batch dimension\n",
    "            frame_tensor = torch.from_numpy(processed_frame).permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
    "\n",
    "            # Run models with GPU acceleration\n",
    "            with torch.cuda.amp.autocast():\n",
    "                det_results = self.detection_model(frame_tensor, verbose=False)\n",
    "                pose_results = self.pose_model(frame_tensor, verbose=False)\n",
    "\n",
    "            # Initialize features\n",
    "            features = {\n",
    "                'objects': [],\n",
    "                'poses': [],\n",
    "                'interactions': [],\n",
    "                'motion': {},\n",
    "                'violence_indicators': {\n",
    "                    'weapon_present': False,\n",
    "                    'close_interaction': False,\n",
    "                    'rapid_motion': False,\n",
    "                    'aggressive_pose': False\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Process relevant detections\n",
    "            person_boxes = []\n",
    "            for result in det_results:\n",
    "                for box in result.boxes:\n",
    "                    try:\n",
    "                        cls = result.names[int(box.cls[0])]\n",
    "                        if cls in self.relevant_classes:\n",
    "                            conf = float(box.conf[0])\n",
    "                            box_coords = box.xyxy[0].cpu().numpy().tolist()\n",
    "                            \n",
    "                            features['objects'].append({\n",
    "                                'class': cls,\n",
    "                                'confidence': conf,\n",
    "                                'box': box_coords\n",
    "                            })\n",
    "                            \n",
    "                            if cls == 'person':\n",
    "                                person_boxes.append(box_coords)\n",
    "                            elif cls in self.violence_objects:\n",
    "                                features['violence_indicators']['weapon_present'] = True\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing detection: {e}\")\n",
    "                        continue\n",
    "\n",
    "            # Analyze person interactions\n",
    "            if len(person_boxes) >= 2:\n",
    "                interactions = self.analyze_person_interactions(person_boxes)\n",
    "                features['interactions'] = interactions\n",
    "                features['violence_indicators']['close_interaction'] = len(interactions) > 0\n",
    "\n",
    "            # Process poses and analyze for violence\n",
    "            if pose_results:\n",
    "                for result in pose_results:\n",
    "                    if result.keypoints is not None:\n",
    "                        for kpts in result.keypoints:\n",
    "                            try:\n",
    "                                pose_data = kpts.data[0].cpu().numpy().tolist()\n",
    "                                features['poses'].append(pose_data)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing pose: {e}\")\n",
    "                                continue\n",
    "\n",
    "                features['violence_indicators']['aggressive_pose'] = self.analyze_poses_for_violence(features['poses'])\n",
    "\n",
    "            # Calculate motion features\n",
    "            if prev_frame_data and 'poses' in prev_frame_data:\n",
    "                motion_features = self.calculate_motion_features(\n",
    "                    prev_frame_data['poses'], features['poses'])\n",
    "                features['motion'] = motion_features\n",
    "                \n",
    "                features['violence_indicators']['rapid_motion'] = motion_features.get('average_speed', 0) > 10\n",
    "\n",
    "            # Calculate overall risk level\n",
    "            risk_weights = {\n",
    "                'weapon_present': 0.4,\n",
    "                'close_interaction': 0.3,\n",
    "                'rapid_motion': 0.2,\n",
    "                'aggressive_pose': 0.1\n",
    "            }\n",
    "            \n",
    "            self.current_risk_level = sum(\n",
    "                risk_weights[indicator] * int(value)\n",
    "                for indicator, value in features['violence_indicators'].items()\n",
    "            )\n",
    "\n",
    "            # Draw detections\n",
    "            annotated_frame = self.draw_detections(\n",
    "                frame, det_results, pose_results, \n",
    "                features['interactions'], scale_info\n",
    "            )\n",
    "\n",
    "            return features, annotated_frame\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {e}\")\n",
    "            return None, frame\n",
    "def process_video(video_path, extractor, output_path):\n",
    "    \"\"\"Process video with GPU acceleration\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create video writer\n",
    "    output_video_path = video_path.rsplit('.', 1)[0] + '_analyzed.mp4'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize data storage\n",
    "    video_data = {\n",
    "        'metadata': {\n",
    "            'path': video_path,\n",
    "            'fps': fps,\n",
    "            'frame_count': frame_count,\n",
    "            'width': frame_width,\n",
    "            'height': frame_height\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    frame_idx = 0\n",
    "    prev_frame_data = None\n",
    "    paused = False\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            if not paused:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # Skip frames if needed\n",
    "                if frame_idx % extractor.frame_skip != 0:\n",
    "                    frame_idx += 1\n",
    "                    continue\n",
    "\n",
    "                # Extract features and get annotated frame\n",
    "                features, annotated_frame = extractor.extract_features(frame, prev_frame_data)\n",
    "                \n",
    "                if features is not None:\n",
    "                    frame_data = {\n",
    "                        'frame_index': frame_idx,\n",
    "                        'timestamp': frame_idx / fps,\n",
    "                        'features': features\n",
    "                    }\n",
    "                    \n",
    "                    video_data['frames'].append(frame_data)\n",
    "                    prev_frame_data = features\n",
    "                    out.write(annotated_frame)\n",
    "\n",
    "                    # Show progress\n",
    "                    if frame_idx % (30 * extractor.frame_skip) == 0:\n",
    "                        progress = (frame_idx / frame_count) * 100\n",
    "                        print(f\"Processing: {progress:.1f}% complete\")\n",
    "                        if torch.cuda.is_available():\n",
    "                            print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "                    # Display frame\n",
    "                    cv2.imshow('Violence Detection Analysis', annotated_frame)\n",
    "\n",
    "                frame_idx += 1\n",
    "\n",
    "                # Handle key events\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "                elif key == ord('p'):\n",
    "                    paused = not paused\n",
    "                    print(\"Paused - Press 'p' to resume\" if paused else \"Resumed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Final GPU cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save features\n",
    "        try:\n",
    "            with open(output_path, 'w') as f:\n",
    "                yaml.dump(video_data, f, default_flow_style=False)\n",
    "            print(f\"Features saved to: {output_path}\")\n",
    "            print(f\"Analyzed video saved to: {output_video_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {e}\")\n",
    "\n",
    "    return video_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Get video file\n",
    "    video_path = filedialog.askopenfilename(\n",
    "        title=\"Select Video File\",\n",
    "        filetypes=[(\"Video Files\", \"*.mp4;*.avi\")]\n",
    "    )\n",
    "\n",
    "    if not video_path:\n",
    "        print(\"No video file selected\")\n",
    "        exit()                \n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import argparse\n",
    "\n",
    "class ModelSetup:\n",
    "    def __init__(self, detection_model_path, segmentation_model_path, pose_model_path):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.print_gpu_info()\n",
    "        \n",
    "        # Load models\n",
    "        self.detection_model = YOLO(detection_model_path).to(self.device)\n",
    "        self.segmentation_model = YOLO(segmentation_model_path).to(self.device)\n",
    "        self.pose_model = YOLO(pose_model_path).to(self.device)\n",
    "\n",
    "    def print_gpu_info(self):\n",
    "        \"\"\"Print GPU information\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "            print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        else:\n",
    "            print(\"Using CPU.\")\n",
    "\n",
    "    def save_models(self):\n",
    "        \"\"\"Save the model setup state for future use\"\"\"\n",
    "        torch.save({\n",
    "            'detection_model': self.detection_model.state_dict(),\n",
    "            'segmentation_model': self.segmentation_model.state_dict(),\n",
    "            'pose_model': self.pose_model.state_dict()\n",
    "        }, 'model_state.pth')\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Set up models for feature extraction\")\n",
    "    parser.add_argument('detection_model', type=str, help='Path to detection model')\n",
    "    parser.add_argument('segmentation_model', type=str, help='Path to segmentation model')\n",
    "    parser.add_argument('pose_model', type=str, help='Path to pose model')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    setup = ModelSetup(args.detection_model, args.segmentation_model, args.pose_model)\n",
    "    setup.save_models()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
