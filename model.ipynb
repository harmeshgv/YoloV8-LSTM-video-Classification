{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "import torch.cuda\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tkinter import Tk, filedialog\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViolenceFeatureExtractor:\n",
    "    def __init__(self, detection_model_path, segmentation_model_path, pose_model_path):\n",
    "        # Initialize device and GPU settings\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.print_gpu_info()\n",
    "        \n",
    "        # GPU optimization settings\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "        \n",
    "        # Load models and move to GPU\n",
    "        self.detection_model = YOLO(detection_model_path).to(self.device)\n",
    "        self.segmentation_model = YOLO(segmentation_model_path).to(self.device)\n",
    "        self.pose_model = YOLO(pose_model_path).to(self.device)\n",
    "        \n",
    "        # Define violence-related objects and relevant classes\n",
    "        self.violence_objects = ['knife', 'gun', 'baseball bat', 'stick', 'bottle']\n",
    "        self.relevant_classes = ['person'] + self.violence_objects\n",
    "        \n",
    "        # Define colors for visualization\n",
    "        self.colors = {\n",
    "            'violence': (0, 0, 255),    # Red\n",
    "            'person': (0, 255, 0),      # Green\n",
    "            'interaction': (255, 0, 0),  # Blue\n",
    "            'keypoint': (255, 255, 0),  # Yellow\n",
    "            'connection': (0, 255, 255)  # Cyan\n",
    "        }\n",
    "        \n",
    "        # Performance and detection settings\n",
    "        self.frame_skip = 2\n",
    "        self.input_size = 640\n",
    "        self.conf_threshold = 0.5\n",
    "        self.interaction_threshold = 0.5  # For person-to-person interaction detection\n",
    "\n",
    "    def print_gpu_info(self):\n",
    "        \"\"\"Print GPU information\"\"\"\n",
    "        print(\"\\nGPU Information:\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "            print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "            print(f\"Available Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "        else:\n",
    "            print(\"No GPU available. Using CPU.\")\n",
    "\n",
    "    def preprocess_frame(self, frame):\n",
    "        \"\"\"Preprocess frame for model input\"\"\"\n",
    "        try:\n",
    "            # Convert to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Calculate size to maintain aspect ratio\n",
    "            h, w = frame_rgb.shape[:2]\n",
    "            r = self.input_size / max(h, w)\n",
    "            new_h, new_w = int(h * r), int(w * r)\n",
    "            \n",
    "            # Resize\n",
    "            resized = cv2.resize(frame_rgb, (new_w, new_h))\n",
    "            \n",
    "            # Create canvas of input_size x input_size\n",
    "            canvas = np.zeros((self.input_size, self.input_size, 3), dtype=np.uint8)\n",
    "            \n",
    "            # Calculate padding\n",
    "            pad_h = (self.input_size - new_h) // 2\n",
    "            pad_w = (self.input_size - new_w) // 2\n",
    "            \n",
    "            # Place resized image on canvas\n",
    "            canvas[pad_h:pad_h+new_h, pad_w:pad_w+new_w] = resized\n",
    "            \n",
    "            # Normalize\n",
    "            normalized = canvas.astype(np.float32) / 255.0\n",
    "            \n",
    "            return normalized, (r, pad_w, pad_h)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in preprocessing: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def analyze_person_interactions(self, person_boxes):\n",
    "        \"\"\"Analyze interactions between detected people\"\"\"\n",
    "        interactions = []\n",
    "        if len(person_boxes) < 2:\n",
    "            return interactions\n",
    "\n",
    "        for i in range(len(person_boxes)):\n",
    "            for j in range(i + 1, len(person_boxes)):\n",
    "                box1 = person_boxes[i]\n",
    "                box2 = person_boxes[j]\n",
    "                \n",
    "                # Calculate centers\n",
    "                center1 = [(box1[0] + box1[2])/2, (box1[1] + box1[3])/2]\n",
    "                center2 = [(box2[0] + box2[2])/2, (box2[1] + box2[3])/2]\n",
    "                \n",
    "                # Calculate distance and box sizes\n",
    "                distance = np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)\n",
    "                box1_size = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "                box2_size = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "                avg_size = (box1_size + box2_size) / 2\n",
    "                \n",
    "                # Check for close interaction\n",
    "                if distance < avg_size * self.interaction_threshold:\n",
    "                   interactions.append({\n",
    "                         'person1_idx': i,\n",
    "                         'person2_idx': j,\n",
    "                         'distance': distance,\n",
    "                         'relative_distance': distance / avg_size,\n",
    "                         'center1': center1,\n",
    "                         'center2': center2,\n",
    "                         'box1': box1,\n",
    "                         'box2': box2\n",
    "                     })\n",
    "        \n",
    "        return interactions\n",
    "\n",
    "    def calculate_motion_features(self, prev_poses, current_poses):\n",
    "        \"\"\"Calculate motion features between consecutive frames\"\"\"\n",
    "        try:\n",
    "            if not prev_poses or not current_poses:\n",
    "                return {\n",
    "                    'average_speed': 0,\n",
    "                    'motion_intensity': 0,\n",
    "                    'sudden_movements': 0\n",
    "                }\n",
    "\n",
    "            # Convert poses to numpy arrays\n",
    "            prev_poses = np.array(prev_poses)\n",
    "            current_poses = np.array(current_poses)\n",
    "\n",
    "            if prev_poses.shape == current_poses.shape:\n",
    "                # Calculate displacement\n",
    "                displacement = np.linalg.norm(current_poses - prev_poses, axis=2)\n",
    "                average_speed = np.mean(displacement)\n",
    "                motion_intensity = np.std(displacement)\n",
    "                sudden_movements = np.sum(displacement > np.mean(displacement) + 2 * np.std(displacement))\n",
    "\n",
    "                return {\n",
    "                    'average_speed': float(average_speed),\n",
    "                    'motion_intensity': float(motion_intensity),\n",
    "                    'sudden_movements': int(sudden_movements)\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                'average_speed': 0,\n",
    "                'motion_intensity': 0,\n",
    "                'sudden_movements': 0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in motion calculation: {e}\")\n",
    "            return {\n",
    "                'average_speed': 0,\n",
    "                'motion_intensity': 0,\n",
    "                'sudden_movements': 0\n",
    "            }\n",
    "    def analyze_poses_for_violence(self, poses):\n",
    "        \"\"\"Analyze poses for potential aggressive/violent behavior\"\"\"\n",
    "        try:\n",
    "            if not poses:\n",
    "                return False\n",
    "\n",
    "            for pose in poses:\n",
    "                # Convert pose to numpy array for calculations\n",
    "                pose_array = np.array(pose)\n",
    "                \n",
    "                # Check for rapid arm movements (high confidence keypoints only)\n",
    "                arm_keypoints = [5, 7, 9, 6, 8, 10]  # Shoulders, elbows, wrists\n",
    "                arm_positions = pose_array[arm_keypoints]\n",
    "                arm_confidences = arm_positions[:, 2]\n",
    "                \n",
    "                if np.mean(arm_confidences) > 0.5:\n",
    "                    # Calculate arm angles and velocities\n",
    "                    # Add your specific pose analysis logic here\n",
    "                    return True\n",
    "                    \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in pose analysis: {e}\")\n",
    "            return False\n",
    "\n",
    "    def rescale_coords(self, x, y, scale_info):\n",
    "        \"\"\"Rescale coordinates back to original image size\"\"\"\n",
    "        scale, pad_w, pad_h = scale_info\n",
    "        x_orig = (x - pad_w) / scale\n",
    "        y_orig = (y - pad_h) / scale\n",
    "        return int(x_orig), int(y_orig)\n",
    "\n",
    "    def draw_detections(self, frame, det_results, pose_results, interactions, scale_info):\n",
    "        \"\"\"Draw detections, poses, and interactions\"\"\"\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        # Draw object detections\n",
    "        for result in det_results:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                try:\n",
    "                    # Get box coordinates and rescale them\n",
    "                    x1, y1, x2, y2 = map(float, box.xyxy[0].cpu().numpy())\n",
    "                    x1, y1 = self.rescale_coords(x1, y1, scale_info)\n",
    "                    x2, y2 = self.rescale_coords(x2, y2, scale_info)\n",
    "                    \n",
    "                    cls = result.names[int(box.cls[0])]\n",
    "                    conf = float(box.conf[0])\n",
    "\n",
    "                    # Only draw relevant classes\n",
    "                    if cls in self.relevant_classes:\n",
    "                        color = (self.colors['violence'] if cls in self.violence_objects \n",
    "                                else self.colors['person'])\n",
    "\n",
    "                        cv2.rectangle(display_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                        label = f'{cls} {conf:.2f}'\n",
    "                        \n",
    "                        (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "                        cv2.rectangle(display_frame, (x1, y1-text_h-5), (x1+text_w, y1), color, -1)\n",
    "                        cv2.putText(display_frame, label, (x1, y1-5), \n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in detection drawing: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Draw interactions\n",
    "        for interaction in interactions:\n",
    "            try:\n",
    "            # Get centers from interaction data\n",
    "                 x1, y1 = self.rescale_coords(interaction['center1'][0], interaction['center1'][1], scale_info)\n",
    "                 x2, y2 = self.rescale_coords(interaction['center2'][0], interaction['center2'][1], scale_info)\n",
    "              \n",
    "            # Draw line between interacting people\n",
    "                 cv2.line(display_frame, (x1, y1), (x2, y2), self.colors['interaction'], 2)\n",
    "            \n",
    "            # Optional: Draw interaction distance\n",
    "                 mid_point = ((x1 + x2)//2, (y1 + y2)//2)\n",
    "                 distance_label = f\"D: {interaction['relative_distance']:.2f}\"\n",
    "                 cv2.putText(display_frame, distance_label, mid_point, \n",
    "                         cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['interaction'], 2)\n",
    "            \n",
    "            except Exception as e:\n",
    "                 print(f\"Error drawing interaction: {e}\")\n",
    "                 continue\n",
    "\n",
    "        # Draw pose keypoints and connections\n",
    "        if pose_results:\n",
    "            for result in pose_results:\n",
    "                if result.keypoints is not None:\n",
    "                    for kpts in result.keypoints:\n",
    "                        try:\n",
    "                            keypoints_data = kpts.data[0].cpu().numpy()\n",
    "                            \n",
    "                            # Draw keypoints\n",
    "                            for keypoint in keypoints_data:\n",
    "                                x, y, conf = keypoint\n",
    "                                if conf > 0.5:\n",
    "                                    x, y = self.rescale_coords(x, y, scale_info)\n",
    "                                    cv2.circle(display_frame, (x, y), 4, self.colors['keypoint'], -1)\n",
    "\n",
    "                            # Draw connections\n",
    "                            connections = [(5,7), (7,9), (6,8), (8,10), (5,6), \n",
    "                                         (11,13), (13,15), (12,14), (14,16), (11,12)]\n",
    "                            for connection in connections:\n",
    "                                pt1 = keypoints_data[connection[0]]\n",
    "                                pt2 = keypoints_data[connection[1]]\n",
    "                                \n",
    "                                if pt1[2] > 0.5 and pt2[2] > 0.5:\n",
    "                                    x1, y1 = self.rescale_coords(pt1[0], pt1[1], scale_info)\n",
    "                                    x2, y2 = self.rescale_coords(pt2[0], pt2[1], scale_info)\n",
    "                                    cv2.line(display_frame, (x1, y1), (x2, y2),\n",
    "                                           self.colors['connection'], 2)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in pose drawing: {e}\")\n",
    "                            continue\n",
    "\n",
    "        # Add violence indicators\n",
    "        if self.current_risk_level > 0.7:  # High risk threshold\n",
    "            cv2.putText(display_frame, \"HIGH RISK\", (10, 60), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Add frame information\n",
    "        cv2.putText(display_frame, \"Press 'q' to quit, 'p' to pause/resume\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        return display_frame\n",
    "\n",
    "    def extract_features(self, frame, prev_frame_data=None):\n",
    "        \"\"\"Extract violence-relevant features from frame\"\"\"\n",
    "        try:\n",
    "            # Preprocess frame\n",
    "            processed_frame, scale_info = self.preprocess_frame(frame)\n",
    "            if processed_frame is None:\n",
    "                return None, frame\n",
    "\n",
    "            # Convert to tensor and add batch dimension\n",
    "            frame_tensor = torch.from_numpy(processed_frame).permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
    "\n",
    "            # Run models with GPU acceleration\n",
    "            with torch.cuda.amp.autocast():\n",
    "                det_results = self.detection_model(frame_tensor, verbose=False)\n",
    "                pose_results = self.pose_model(frame_tensor, verbose=False)\n",
    "\n",
    "            # Initialize features\n",
    "            features = {\n",
    "                'objects': [],\n",
    "                'poses': [],\n",
    "                'interactions': [],\n",
    "                'motion': {},\n",
    "                'violence_indicators': {\n",
    "                    'weapon_present': False,\n",
    "                    'close_interaction': False,\n",
    "                    'rapid_motion': False,\n",
    "                    'aggressive_pose': False\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Process relevant detections\n",
    "            person_boxes = []\n",
    "            for result in det_results:\n",
    "                for box in result.boxes:\n",
    "                    try:\n",
    "                        cls = result.names[int(box.cls[0])]\n",
    "                        if cls in self.relevant_classes:\n",
    "                            conf = float(box.conf[0])\n",
    "                            box_coords = box.xyxy[0].cpu().numpy().tolist()\n",
    "                            \n",
    "                            features['objects'].append({\n",
    "                                'class': cls,\n",
    "                                'confidence': conf,\n",
    "                                'box': box_coords\n",
    "                            })\n",
    "                            \n",
    "                            if cls == 'person':\n",
    "                                person_boxes.append(box_coords)\n",
    "                            elif cls in self.violence_objects:\n",
    "                                features['violence_indicators']['weapon_present'] = True\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing detection: {e}\")\n",
    "                        continue\n",
    "\n",
    "            # Analyze person interactions\n",
    "            if len(person_boxes) >= 2:\n",
    "                interactions = self.analyze_person_interactions(person_boxes)\n",
    "                features['interactions'] = interactions\n",
    "                features['violence_indicators']['close_interaction'] = len(interactions) > 0\n",
    "\n",
    "            # Process poses and analyze for violence\n",
    "            if pose_results:\n",
    "                for result in pose_results:\n",
    "                    if result.keypoints is not None:\n",
    "                        for kpts in result.keypoints:\n",
    "                            try:\n",
    "                                pose_data = kpts.data[0].cpu().numpy().tolist()\n",
    "                                features['poses'].append(pose_data)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing pose: {e}\")\n",
    "                                continue\n",
    "\n",
    "                features['violence_indicators']['aggressive_pose'] = self.analyze_poses_for_violence(features['poses'])\n",
    "\n",
    "            # Calculate motion features\n",
    "            if prev_frame_data and 'poses' in prev_frame_data:\n",
    "                motion_features = self.calculate_motion_features(\n",
    "                    prev_frame_data['poses'], features['poses'])\n",
    "                features['motion'] = motion_features\n",
    "                \n",
    "                features['violence_indicators']['rapid_motion'] = motion_features.get('average_speed', 0) > 10\n",
    "\n",
    "            # Calculate overall risk level\n",
    "            risk_weights = {\n",
    "                'weapon_present': 0.4,\n",
    "                'close_interaction': 0.3,\n",
    "                'rapid_motion': 0.2,\n",
    "                'aggressive_pose': 0.1\n",
    "            }\n",
    "            \n",
    "            self.current_risk_level = sum(\n",
    "                risk_weights[indicator] * int(value)\n",
    "                for indicator, value in features['violence_indicators'].items()\n",
    "            )\n",
    "\n",
    "            # Draw detections\n",
    "            annotated_frame = self.draw_detections(\n",
    "                frame, det_results, pose_results, \n",
    "                features['interactions'], scale_info\n",
    "            )\n",
    "\n",
    "            return features, annotated_frame\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {e}\")\n",
    "            return None, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_video(video_path, extractor, output_path):\n",
    "    \"\"\"Process video with GPU acceleration\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create video writer\n",
    "    output_video_path = video_path.rsplit('.', 1)[0] + '_analyzed.mp4'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize data storage\n",
    "    video_data = {\n",
    "        'metadata': {\n",
    "            'path': video_path,\n",
    "            'fps': fps,\n",
    "            'frame_count': frame_count,\n",
    "            'width': frame_width,\n",
    "            'height': frame_height\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    frame_idx = 0\n",
    "    prev_frame_data = None\n",
    "    paused = False\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            if not paused:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # Skip frames if needed\n",
    "                if frame_idx % extractor.frame_skip != 0:\n",
    "                    frame_idx += 1\n",
    "                    continue\n",
    "\n",
    "                # Extract features and get annotated frame\n",
    "                features, annotated_frame = extractor.extract_features(frame, prev_frame_data)\n",
    "                \n",
    "                if features is not None:\n",
    "                    frame_data = {\n",
    "                        'frame_index': frame_idx,\n",
    "                        'timestamp': frame_idx / fps,\n",
    "                        'features': features\n",
    "                    }\n",
    "                    \n",
    "                    video_data['frames'].append(frame_data)\n",
    "                    prev_frame_data = features\n",
    "                    out.write(annotated_frame)\n",
    "\n",
    "                    # Show progress\n",
    "                    if frame_idx % (30 * extractor.frame_skip) == 0:\n",
    "                        progress = (frame_idx / frame_count) * 100\n",
    "                        print(f\"Processing: {progress:.1f}% complete\")\n",
    "                        if torch.cuda.is_available():\n",
    "                            print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "                    # Display frame\n",
    "                    cv2.imshow('Violence Detection Analysis', annotated_frame)\n",
    "\n",
    "                frame_idx += 1\n",
    "\n",
    "                # Handle key events\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "                elif key == ord('p'):\n",
    "                    paused = not paused\n",
    "                    print(\"Paused - Press 'p' to resume\" if paused else \"Resumed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Final GPU cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save features\n",
    "        try:\n",
    "            with open(output_path, 'w') as f:\n",
    "                yaml.dump(video_data, f, default_flow_style=False)\n",
    "            print(f\"Features saved to: {output_path}\")\n",
    "            print(f\"Analyzed video saved to: {output_video_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {e}\")\n",
    "\n",
    "    return video_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Get video file\n",
    "    video_path = filedialog.askopenfilename(\n",
    "        title=\"Select Video File\",\n",
    "        filetypes=[(\"Video Files\", \"*.mp4;*.avi\")]\n",
    "    )\n",
    "\n",
    "    if not video_path:\n",
    "        print(\"No video file selected\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU Information:\n",
      "GPU Device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "CUDA Version: 11.8\n",
      "Total GPU Memory: 4.29 GB\n",
      "Available Memory: 0.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harme\\AppData\\Local\\Temp\\ipykernel_18352\\2763640433.py:299: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0.0% complete\n",
      "GPU Memory: 0.30 GB\n",
      "Processing: 90.9% complete\n",
      "GPU Memory: 0.30 GB\n",
      "Features saved to: C:\\Users\\harme\\Desktop\\video-detect-gpu\\violence_features.yaml\n",
      "Analyzed video saved to: C:/Users/harme/Desktop/video-detect-gpu/NV_1_analyzed.mp4\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "detection_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m.pt'\n",
    "segmentation_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-seg.pt'\n",
    "pose_model_path = r'C:\\Users\\harme\\Desktop\\violence detection\\yolo11m-pose.pt'\n",
    "    \n",
    "extractor = ViolenceFeatureExtractor(\n",
    "        detection_model_path,\n",
    "        segmentation_model_path,\n",
    "        pose_model_path\n",
    "    )\n",
    "\n",
    "    # Process video\n",
    "output_path = r'C:\\Users\\harme\\Desktop\\video-detect-gpu\\violence_features.yaml'\n",
    "video_data = process_video(video_path, extractor, output_path)\n",
    "print(\"Analysis complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
