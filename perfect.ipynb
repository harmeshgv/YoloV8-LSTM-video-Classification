{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Add the root directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\")))\n",
    "\n",
    "# Now import the module\n",
    "from utils.feature_extraction import ViolenceFeatureExtractor\n",
    "\n",
    "\n",
    "# Initialize the ViolenceFeatureExtractor\n",
    "extractor = ViolenceFeatureExtractor(\n",
    "    detection_model_path=r\"C:\\Users\\harme\\Desktop\\video-detect-gpu\\app\\models\\detection_model.pt\",\n",
    "    segmentation_model_path=r\"C:\\Users\\harme\\Desktop\\video-detect-gpu\\app\\models\\segmentation_model.pt\",\n",
    "    pose_model_path=r\"C:\\Users\\harme\\Desktop\\video-detect-gpu\\app\\models\\pose_model.pt\",\n",
    ")\n",
    "\n",
    "# Hardcoded file paths\n",
    "video_file = r\"C:\\Users\\harme\\Desktop\\video-detect-gpu\\NV_1.mp4\"\n",
    "yaml_file = r\"C:\\Users\\harme\\Desktop\\video-detect-gpu\\violence_features.yaml\"\n",
    "\n",
    "# Button to start processing\n",
    "if st.button(\"Process Video\"):\n",
    "    if os.path.exists(video_file):\n",
    "        # Process the video and get the in-memory video buffer\n",
    "        video_buffer = extractor.process_video(video_file, yaml_file)\n",
    "\n",
    "        if video_buffer:\n",
    "            st.subheader(\"Processed Video\")\n",
    "\n",
    "            # Display the in-memory video in the Streamlit app\n",
    "            st.video(video_buffer)\n",
    "\n",
    "            # Add a download button for the in-memory video\n",
    "            st.download_button(\n",
    "                label=\"Download Processed Video\",\n",
    "                data=video_buffer,\n",
    "                file_name=\"nv_1_analyzed.mp4\",\n",
    "                mime=\"video/mp4\",\n",
    "            )\n",
    "        else:\n",
    "            st.error(\"Video processing failed. Please check the input video.\")\n",
    "    else:\n",
    "        st.error(\"Input video file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import io\n",
    "import imageio\n",
    "from ultralytics import YOLO\n",
    "from scipy.spatial.distance import cdist\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class ViolenceFeatureExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract violence-related features from video frames using YOLO models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, detection_model_path, segmentation_model_path, pose_model_path):\n",
    "        \"\"\"\n",
    "        Initialize the ViolenceFeatureExtractor with YOLO models.\n",
    "\n",
    "        Args:\n",
    "            detection_model_path (str): Path to the detection model.\n",
    "            segmentation_model_path (str): Path to the segmentation model.\n",
    "            pose_model_path (str): Path to the pose estimation model.\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._setup_gpu()\n",
    "\n",
    "        self.detection_model = YOLO(detection_model_path).to(self.device)\n",
    "        self.segmentation_model = YOLO(segmentation_model_path).to(self.device)\n",
    "        self.pose_model = YOLO(pose_model_path).to(self.device)\n",
    "\n",
    "        self.violence_objects = [\"knife\", \"gun\", \"baseball bat\", \"stick\", \"bottle\"]\n",
    "        self.relevant_classes = [\"person\"] + self.violence_objects\n",
    "\n",
    "        self.colors = {\n",
    "            \"violence\": (0, 0, 255),  # Red\n",
    "            \"person\": (0, 255, 0),  # Green\n",
    "            \"interaction\": (255, 0, 0),  # Blue\n",
    "            \"keypoint\": (255, 255, 0),  # Yellow\n",
    "            \"connection\": (0, 255, 255),  # Cyan\n",
    "        }\n",
    "\n",
    "        self.frame_skip = 2\n",
    "        self.input_size = 640\n",
    "        self.conf_threshold = 0.5\n",
    "        self.interaction_threshold = 0.5\n",
    "        self.current_risk_level = 0.0\n",
    "\n",
    "    def _setup_gpu(self):\n",
    "        \"\"\"Configure GPU settings if available.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            print(\"No GPU available. Using CPU.\")\n",
    "\n",
    "    def preprocess_frame(self, frame):\n",
    "        \"\"\"Preprocess a frame for model input.\"\"\"\n",
    "        try:\n",
    "            # Convert BGR to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Get original dimensions\n",
    "            h, w = frame_rgb.shape[:2]\n",
    "\n",
    "            # Calculate scaling factor\n",
    "            r = self.input_size / max(h, w)\n",
    "\n",
    "            # Resize the frame\n",
    "            new_h, new_w = int(h * r), int(w * r)\n",
    "            resized = cv2.resize(frame_rgb, (new_w, new_h))\n",
    "\n",
    "            # Pad the frame to match input_size\n",
    "            canvas = np.zeros((self.input_size, self.input_size, 3), dtype=np.uint8)\n",
    "            pad_h = (self.input_size - new_h) // 2\n",
    "            pad_w = (self.input_size - new_w) // 2\n",
    "            canvas[pad_h : pad_h + new_h, pad_w : pad_w + new_w] = resized\n",
    "\n",
    "            # Normalize the frame\n",
    "            normalized = canvas.astype(np.float32) / 255.0\n",
    "\n",
    "            # Return the normalized frame and scaling/padding info\n",
    "            return normalized, (r, pad_w, pad_h)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in preprocessing: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def analyze_person_interactions(self, person_boxes):\n",
    "        \"\"\"Analyze interactions between detected people.\"\"\"\n",
    "        interactions = []\n",
    "        if len(person_boxes) < 2:\n",
    "            return interactions\n",
    "\n",
    "        for i in range(len(person_boxes)):\n",
    "            for j in range(i + 1, len(person_boxes)):\n",
    "                box1 = person_boxes[i]\n",
    "                box2 = person_boxes[j]\n",
    "                center1 = [(box1[0] + box1[2]) / 2, (box1[1] + box1[3]) / 2]\n",
    "                center2 = [(box2[0] + box2[2]) / 2, (box2[1] + box2[3]) / 2]\n",
    "                distance = np.sqrt(\n",
    "                    (center1[0] - center2[0]) ** 2 + (center1[1] - center2[1]) ** 2\n",
    "                )\n",
    "                box1_size = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "                box2_size = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "                avg_size = (box1_size + box2_size) / 2\n",
    "\n",
    "                if distance < avg_size * self.interaction_threshold:\n",
    "                    interactions.append(\n",
    "                        {\n",
    "                            \"person1_idx\": i,\n",
    "                            \"person2_idx\": j,\n",
    "                            \"distance\": distance,\n",
    "                            \"relative_distance\": distance / avg_size,\n",
    "                            \"center1\": center1,\n",
    "                            \"center2\": center2,\n",
    "                            \"box1\": box1,\n",
    "                            \"box2\": box2,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        return interactions\n",
    "\n",
    "    def calculate_motion_features(self, prev_poses, current_poses):\n",
    "        \"\"\"Calculate motion features between consecutive frames.\"\"\"\n",
    "        try:\n",
    "            if not prev_poses or not current_poses:\n",
    "                return {\n",
    "                    \"average_speed\": 0,\n",
    "                    \"motion_intensity\": 0,\n",
    "                    \"sudden_movements\": 0,\n",
    "                }\n",
    "\n",
    "            prev_poses = np.array(prev_poses)\n",
    "            current_poses = np.array(current_poses)\n",
    "\n",
    "            if prev_poses.shape == current_poses.shape:\n",
    "                displacement = np.linalg.norm(current_poses - prev_poses, axis=2)\n",
    "                average_speed = np.mean(displacement)\n",
    "                motion_intensity = np.std(displacement)\n",
    "                sudden_movements = np.sum(\n",
    "                    displacement > np.mean(displacement) + 2 * np.std(displacement)\n",
    "                )\n",
    "\n",
    "                return {\n",
    "                    \"average_speed\": float(average_speed),\n",
    "                    \"motion_intensity\": float(motion_intensity),\n",
    "                    \"sudden_movements\": int(sudden_movements),\n",
    "                }\n",
    "\n",
    "            return {\"average_speed\": 0, \"motion_intensity\": 0, \"sudden_movements\": 0}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in motion calculation: {e}\")\n",
    "            return {\"average_speed\": 0, \"motion_intensity\": 0, \"sudden_movements\": 0}\n",
    "\n",
    "    def analyze_poses_for_violence(self, poses):\n",
    "        \"\"\"Analyze poses for potential aggressive/violent behavior.\"\"\"\n",
    "        try:\n",
    "            if not poses:\n",
    "                return False\n",
    "\n",
    "            for pose in poses:\n",
    "                pose_array = np.array(pose)\n",
    "                arm_keypoints = [5, 7, 9, 6, 8, 10]\n",
    "                arm_positions = pose_array[arm_keypoints]\n",
    "                arm_confidences = arm_positions[:, 2]\n",
    "\n",
    "                if np.mean(arm_confidences) > 0.5:\n",
    "                    return True\n",
    "\n",
    "            return False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in pose analysis: {e}\")\n",
    "            return False\n",
    "\n",
    "    def rescale_coords(self, x, y, scale_info):\n",
    "        \"\"\"Rescale coordinates back to original image size.\"\"\"\n",
    "        scale, pad_w, pad_h = scale_info\n",
    "        x_orig = (x - pad_w) / scale\n",
    "        y_orig = (y - pad_h) / scale\n",
    "        return int(x_orig), int(y_orig)\n",
    "\n",
    "    def draw_detections(self, frame, det_results, pose_results, interactions, scale_info):\n",
    "        \"\"\"Draw detections, poses, and interactions on the frame.\"\"\"\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        for result in det_results:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                try:\n",
    "                    x1, y1, x2, y2 = map(float, box.xyxy[0].cpu().numpy())\n",
    "                    x1, y1 = self.rescale_coords(x1, y1, scale_info)\n",
    "                    x2, y2 = self.rescale_coords(x2, y2, scale_info)\n",
    "                    cls = result.names[int(box.cls[0])]\n",
    "                    conf = float(box.conf[0])\n",
    "\n",
    "                    if cls in self.relevant_classes:\n",
    "                        color = (\n",
    "                            self.colors[\"violence\"]\n",
    "                            if cls in self.violence_objects\n",
    "                            else self.colors[\"person\"]\n",
    "                        )\n",
    "\n",
    "                        cv2.rectangle(display_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                        label = f\"{cls} {conf:.2f}\"\n",
    "                        (text_w, text_h), _ = cv2.getTextSize(\n",
    "                            label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2\n",
    "                        )\n",
    "                        cv2.rectangle(\n",
    "                            display_frame,\n",
    "                            (x1, y1 - text_h - 5),\n",
    "                            (x1 + text_w, y1),\n",
    "                            color,\n",
    "                            -1,\n",
    "                        )\n",
    "                        cv2.putText(\n",
    "                            display_frame,\n",
    "                            label,\n",
    "                            (x1, y1 - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.5,\n",
    "                            (255, 255, 255),\n",
    "                            2,\n",
    "                        )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in detection drawing: {e}\")\n",
    "                    continue\n",
    "\n",
    "        for interaction in interactions:\n",
    "            try:\n",
    "                x1, y1 = self.rescale_coords(\n",
    "                    interaction[\"center1\"][0], interaction[\"center1\"][1], scale_info\n",
    "                )\n",
    "                x2, y2 = self.rescale_coords(\n",
    "                    interaction[\"center2\"][0], interaction[\"center2\"][1], scale_info\n",
    "                )\n",
    "                cv2.line(\n",
    "                    display_frame, (x1, y1), (x2, y2), self.colors[\"interaction\"], 2\n",
    "                )\n",
    "                mid_point = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "                distance_label = f\"D: {interaction['relative_distance']:.2f}\"\n",
    "                cv2.putText(\n",
    "                    display_frame,\n",
    "                    distance_label,\n",
    "                    mid_point,\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    self.colors[\"interaction\"],\n",
    "                    2,\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error drawing interaction: {e}\")\n",
    "                continue\n",
    "\n",
    "        if pose_results:\n",
    "            for result in pose_results:\n",
    "                if result.keypoints is not None:\n",
    "                    for kpts in result.keypoints:\n",
    "                        try:\n",
    "                            keypoints_data = kpts.data[0].cpu().numpy()\n",
    "                            for keypoint in keypoints_data:\n",
    "                                x, y, conf = keypoint\n",
    "                                if conf > 0.5:\n",
    "                                    x, y = self.rescale_coords(x, y, scale_info)\n",
    "                                    cv2.circle(\n",
    "                                        display_frame,\n",
    "                                        (x, y),\n",
    "                                        4,\n",
    "                                        self.colors[\"keypoint\"],\n",
    "                                        -1,\n",
    "                                    )\n",
    "\n",
    "                            connections = [\n",
    "                                (5, 7),\n",
    "                                (7, 9),\n",
    "                                (6, 8),\n",
    "                                (8, 10),\n",
    "                                (5, 6),\n",
    "                                (11, 13),\n",
    "                                (13, 15),\n",
    "                                (12, 14),\n",
    "                                (14, 16),\n",
    "                                (11, 12),\n",
    "                            ]\n",
    "                            for connection in connections:\n",
    "                                pt1 = keypoints_data[connection[0]]\n",
    "                                pt2 = keypoints_data[connection[1]]\n",
    "\n",
    "                                if pt1[2] > 0.5 and pt2[2] > 0.5:\n",
    "                                    x1, y1 = self.rescale_coords(\n",
    "                                        pt1[0], pt1[1], scale_info\n",
    "                                    )\n",
    "                                    x2, y2 = self.rescale_coords(\n",
    "                                        pt2[0], pt2[1], scale_info\n",
    "                                    )\n",
    "                                    cv2.line(\n",
    "                                        display_frame,\n",
    "                                        (x1, y1),\n",
    "                                        (x2, y2),\n",
    "                                        self.colors[\"connection\"],\n",
    "                                        2,\n",
    "                                    )\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in pose drawing: {e}\")\n",
    "                            continue\n",
    "\n",
    "        if self.current_risk_level > 0.7:\n",
    "            cv2.putText(\n",
    "                display_frame,\n",
    "                \"HIGH RISK\",\n",
    "                (10, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1,\n",
    "                (0, 0, 255),\n",
    "                2,\n",
    "            )\n",
    "\n",
    "        cv2.putText(\n",
    "            display_frame,\n",
    "            \"Press 'q' to quit, 'p' to pause/resume\",\n",
    "            (10, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            (255, 255, 255),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "        return display_frame\n",
    "\n",
    "    def extract_features(self, frame, prev_frame_data=None):\n",
    "        \"\"\"Extract violence-relevant features from a frame.\"\"\"\n",
    "        try:\n",
    "            processed_frame, scale_info = self.preprocess_frame(frame)\n",
    "            if processed_frame is None:\n",
    "                return None, frame\n",
    "\n",
    "            frame_tensor = (\n",
    "                torch.from_numpy(processed_frame)\n",
    "                .permute(2, 0, 1)\n",
    "                .unsqueeze(0)\n",
    "                .to(self.device)\n",
    "            )\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                det_results = self.detection_model(frame_tensor, verbose=False)\n",
    "                pose_results = self.pose_model(frame_tensor, verbose=False)\n",
    "\n",
    "            features = {\n",
    "                \"objects\": [],\n",
    "                \"poses\": [],\n",
    "                \"interactions\": [],\n",
    "                \"motion\": {},\n",
    "                \"violence_indicators\": {\n",
    "                    \"weapon_present\": False,\n",
    "                    \"close_interaction\": False,\n",
    "                    \"rapid_motion\": False,\n",
    "                    \"aggressive_pose\": False,\n",
    "                },\n",
    "            }\n",
    "\n",
    "            person_boxes = []\n",
    "            for result in det_results:\n",
    "                for box in result.boxes:\n",
    "                    try:\n",
    "                        cls = result.names[int(box.cls[0])]\n",
    "                        if cls in self.relevant_classes:\n",
    "                            conf = float(box.conf[0])\n",
    "                            box_coords = box.xyxy[0].cpu().numpy().tolist()\n",
    "\n",
    "                            features[\"objects\"].append(\n",
    "                                {\"class\": cls, \"confidence\": conf, \"box\": box_coords}\n",
    "                            )\n",
    "\n",
    "                            if cls == \"person\":\n",
    "                                person_boxes.append(box_coords)\n",
    "                            elif cls in self.violence_objects:\n",
    "                                features[\"violence_indicators\"][\"weapon_present\"] = True\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing detection: {e}\")\n",
    "                        continue\n",
    "\n",
    "            if len(person_boxes) >= 2:\n",
    "                interactions = self.analyze_person_interactions(person_boxes)\n",
    "                features[\"interactions\"] = interactions\n",
    "                features[\"violence_indicators\"][\"close_interaction\"] = len(interactions) > 0\n",
    "\n",
    "            if pose_results:\n",
    "                for result in pose_results:\n",
    "                    if result.keypoints is not None:\n",
    "                        for kpts in result.keypoints:\n",
    "                            try:\n",
    "                                pose_data = kpts.data[0].cpu().numpy().tolist()\n",
    "                                features[\"poses\"].append(pose_data)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing pose: {e}\")\n",
    "                                continue\n",
    "\n",
    "                features[\"violence_indicators\"][\"aggressive_pose\"] = (\n",
    "                    self.analyze_poses_for_violence(features[\"poses\"])\n",
    "                )\n",
    "\n",
    "            if prev_frame_data and \"poses\" in prev_frame_data:\n",
    "                motion_features = self.calculate_motion_features(\n",
    "                    prev_frame_data[\"poses\"], features[\"poses\"]\n",
    "                )\n",
    "                features[\"motion\"] = motion_features\n",
    "                features[\"violence_indicators\"][\"rapid_motion\"] = (\n",
    "                    motion_features.get(\"average_speed\", 0) > 10\n",
    "                )\n",
    "\n",
    "            risk_weights = {\n",
    "                \"weapon_present\": 0.4,\n",
    "                \"close_interaction\": 0.3,\n",
    "                \"rapid_motion\": 0.2,\n",
    "                \"aggressive_pose\": 0.1,\n",
    "            }\n",
    "\n",
    "            self.current_risk_level = sum(\n",
    "                risk_weights[indicator] * int(value)\n",
    "                for indicator, value in features[\"violence_indicators\"].items()\n",
    "            )\n",
    "\n",
    "            annotated_frame = self.draw_detections(\n",
    "                frame, det_results, pose_results, features[\"interactions\"], scale_info\n",
    "            )\n",
    "\n",
    "            return features, annotated_frame\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {e}\")\n",
    "            return None, frame\n",
    "\n",
    "    def convert_numpy_to_python(self, obj):\n",
    "        \"\"\"Recursively convert NumPy objects to native Python types.\"\"\"\n",
    "        if isinstance(obj, np.generic):\n",
    "            return obj.item()  # Convert NumPy scalar to Python scalar\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: self.convert_numpy_to_python(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.convert_numpy_to_python(item) for item in obj]\n",
    "        elif isinstance(obj, tuple):\n",
    "            return tuple(self.convert_numpy_to_python(item) for item in obj)\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    def process_video(self, video_path, yaml_path):\n",
    "        \"\"\"Process a video file to extract violence-related features and return the video in memory.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open video file\")\n",
    "            return None\n",
    "\n",
    "        # Extract video metadata\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Initialize data storage\n",
    "        video_data = {\n",
    "            \"metadata\": {\n",
    "                \"path\": video_path,\n",
    "                \"fps\": fps,\n",
    "                \"frame_count\": frame_count,\n",
    "                \"width\": frame_width,\n",
    "                \"height\": frame_height,\n",
    "            },\n",
    "            \"frames\": [],\n",
    "        }\n",
    "\n",
    "        # Load existing data if the YAML file exists\n",
    "        if os.path.exists(yaml_path):\n",
    "            try:\n",
    "                with open(yaml_path, \"r\") as yaml_file:\n",
    "                    existing_data = yaml.safe_load(yaml_file) or {}\n",
    "                    if \"frames\" in existing_data:\n",
    "                        video_data[\"frames\"].extend(existing_data[\"frames\"])\n",
    "            except yaml.YAMLError as e:\n",
    "                print(f\"Error loading YAML file: {e}\")\n",
    "                # If the YAML file is corrupted, start with an empty data structure\n",
    "                video_data[\"frames\"] = []\n",
    "\n",
    "        # Create an in-memory byte stream to store the video\n",
    "        video_buffer = io.BytesIO()\n",
    "\n",
    "        # Use imageio to write the video to the in-memory buffer\n",
    "        with imageio.get_writer(video_buffer, format=\"mp4\", fps=fps, macro_block_size = 1) as writer:\n",
    "            frame_idx = 0\n",
    "            prev_frame_data = None\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break  # Exit loop if no more frames\n",
    "\n",
    "                # Skip frames based on frame_skip\n",
    "                if frame_idx % self.frame_skip != 0:\n",
    "                    frame_idx += 1\n",
    "                    continue\n",
    "\n",
    "                # Extract features and draw detections\n",
    "                features, annotated_frame = self.extract_features(frame, prev_frame_data)\n",
    "\n",
    "                if features is not None:\n",
    "                    # Append detailed frame data to the frames list\n",
    "                    frame_data = {\n",
    "                        \"frame_index\": frame_idx,\n",
    "                        \"timestamp\": frame_idx / fps,  # Calculate timestamp\n",
    "                        \"features\": features,\n",
    "                    }\n",
    "                    video_data[\"frames\"].append(frame_data)\n",
    "\n",
    "                    # Convert the annotated frame to RGB (required by imageio)\n",
    "                    annotated_frame_rgb = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    # Append the frame to the video buffer\n",
    "                    writer.append_data(annotated_frame_rgb)\n",
    "\n",
    "                    prev_frame_data = features\n",
    "\n",
    "                frame_idx += 1\n",
    "\n",
    "        # Release the video capture object\n",
    "        cap.release()\n",
    "\n",
    "        # Convert NumPy objects to native Python types before saving\n",
    "        video_data_converted = self.convert_numpy_to_python(video_data)\n",
    "\n",
    "        # Save all data (metadata + frame features) to a YAML file\n",
    "        with open(yaml_path, \"w\") as yaml_file:\n",
    "            yaml.dump(video_data_converted, yaml_file, default_flow_style=False)\n",
    "\n",
    "        # Return the in-memory video buffer\n",
    "        video_buffer.seek(0)  # Reset the buffer position to the beginning\n",
    "        return video_buffer"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
